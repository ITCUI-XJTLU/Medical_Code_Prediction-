{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzoQF_hHhtqT","executionInfo":{"status":"ok","timestamp":1661180333723,"user_tz":-480,"elapsed":27923,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"}},"outputId":"3b7090b8-c1de-45fe-e09c-19f784dab3c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]},{"output_type":"execute_result","data":{"text/plain":["['Multi-Filter-Residual-Convolutional-Neural-Network',\n"," 'LAAT',\n"," 'LAAT_Run.ipynb',\n"," 'Process_Data',\n"," 'MResCNN_RUN.ipynb',\n"," 'BERT_FineTune',\n"," 'Transformer_MCP_1.ipynb',\n"," 'Bert-Chinese-Text-Classification-Pytorch',\n"," 'Transformer_MCP_3.ipynb',\n"," 'Find_MN.ipynb',\n"," 'Transformer_MCP_5_Bert-based-model + Heir.ipynb',\n"," 'Transformer_MCP_6.ipynb',\n"," 'caml-mimic',\n"," 'MResCNN_RUN_2.ipynb',\n"," 'Transformer_MCP_2.ipynb',\n"," 'Transformer_MCP_4.ipynb',\n"," 'Transformer_MCP_7.ipynb',\n"," 'FineTune_Visualization.ipynb',\n"," 'CALM_RUN.ipynb']"]},"metadata":{},"execution_count":2}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","import os\n","path=\"/content/drive/My Drive/MIMIC\"\n","\n","os.chdir(path)\n","os.listdir(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15431,"status":"ok","timestamp":1660962233739,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"kW_NR_xZhtnr","outputId":"d7d99951-4646-448e-c1f7-a8bf0ea1739a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'caml-mimic'...\n","remote: Enumerating objects: 526, done.\u001b[K\n","remote: Total 526 (delta 0), reused 0 (delta 0), pack-reused 526\u001b[K\n","Receiving objects: 100% (526/526), 150.40 MiB | 14.54 MiB/s, done.\n","Resolving deltas: 100% (258/258), done.\n","Checking out files: 100% (75/75), done.\n"]}],"source":["! git clone https://github.com/jamesmullenbach/caml-mimic.git"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1661180336387,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"nJtvI2XEhtlL"},"outputs":[],"source":["os.chdir(\"/content/drive/My Drive/MIMIC/caml-mimic\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ndkMnELbhtit","outputId":"72cd9f75-dc85-40b1-e98a-d3adea8f68bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 483.32it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4009, 0.6252, 0.4897, 0.5492, 0.8826\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4441, 0.7039, 0.5462, 0.6151, 0.9112\n","rec_at_5: 0.5913\n","prec_at_5: 0.5972\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 70\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 70 [batch #0, batch_size 16, seq length 212]\tLoss: 0.156765\n","16it [00:00, 155.84it/s]Train epoch: 70 [batch #25, batch_size 16, seq length 571]\tLoss: 0.080969\n","46it [00:00, 127.89it/s]Train epoch: 70 [batch #50, batch_size 16, seq length 709]\tLoss: 0.090883\n","72it [00:00, 117.73it/s]Train epoch: 70 [batch #75, batch_size 16, seq length 806]\tLoss: 0.104482\n","96it [00:00, 115.09it/s]Train epoch: 70 [batch #100, batch_size 16, seq length 892]\tLoss: 0.086901\n","119it [00:01, 106.92it/s]Train epoch: 70 [batch #125, batch_size 16, seq length 978]\tLoss: 0.094043\n","141it [00:01, 97.40it/s] Train epoch: 70 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.109744\n","171it [00:01, 88.50it/s]Train epoch: 70 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.097881\n","199it [00:01, 85.77it/s]Train epoch: 70 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.102746\n","217it [00:02, 84.23it/s]Train epoch: 70 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.103117\n","243it [00:02, 75.00it/s]Train epoch: 70 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.111686\n","268it [00:02, 73.55it/s]Train epoch: 70 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.114747\n","300it [00:03, 68.60it/s]Train epoch: 70 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.123634\n","322it [00:03, 63.95it/s]Train epoch: 70 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.113812\n","350it [00:04, 63.56it/s]Train epoch: 70 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.118751\n","375it [00:04, 55.83it/s]Train epoch: 70 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.131353\n","400it [00:05, 55.29it/s]Train epoch: 70 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.131589\n","424it [00:05, 51.15it/s]Train epoch: 70 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.125345\n","446it [00:06, 47.34it/s]Train epoch: 70 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.126413\n","471it [00:06, 42.45it/s]Train epoch: 70 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.150146\n","498it [00:07, 34.74it/s]Train epoch: 70 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.162300\n","505it [00:07, 66.69it/s]\n","epoch loss: 0.1129288878660686\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 475.98it/s]\n","Finish save rediction by checkpoint  70\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4006, 0.6256, 0.4888, 0.5488, 0.8823\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4444, 0.7039, 0.5466, 0.6154, 0.9107\n","rec_at_5: 0.5908\n","prec_at_5: 0.5972\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 71\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 71 [batch #0, batch_size 16, seq length 212]\tLoss: 0.152899\n","16it [00:00, 148.85it/s]Train epoch: 71 [batch #25, batch_size 16, seq length 571]\tLoss: 0.079727\n","45it [00:00, 126.69it/s]Train epoch: 71 [batch #50, batch_size 16, seq length 709]\tLoss: 0.091068\n","71it [00:00, 124.23it/s]Train epoch: 71 [batch #75, batch_size 16, seq length 806]\tLoss: 0.104213\n","96it [00:00, 113.43it/s]Train epoch: 71 [batch #100, batch_size 16, seq length 892]\tLoss: 0.086775\n","120it [00:01, 103.48it/s]Train epoch: 71 [batch #125, batch_size 16, seq length 978]\tLoss: 0.094254\n","142it [00:01, 95.44it/s] Train epoch: 71 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.106934\n","172it [00:01, 89.26it/s]Train epoch: 71 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.097158\n","199it [00:01, 85.45it/s]Train epoch: 71 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.101977\n","217it [00:02, 77.14it/s]Train epoch: 71 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.100408\n","243it [00:02, 77.32it/s]Train epoch: 71 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.110442\n","275it [00:02, 71.41it/s]Train epoch: 71 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.113323\n","299it [00:03, 67.64it/s]Train epoch: 71 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.120419\n","321it [00:03, 64.70it/s]Train epoch: 71 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.112027\n","349it [00:04, 62.06it/s]Train epoch: 71 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.118238\n","370it [00:04, 60.02it/s]Train epoch: 71 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.128593\n","396it [00:04, 57.13it/s]Train epoch: 71 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.127694\n","420it [00:05, 51.71it/s]Train epoch: 71 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.124480\n","448it [00:06, 48.32it/s]Train epoch: 71 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.125230\n","473it [00:06, 42.87it/s]Train epoch: 71 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.146707\n","500it [00:07, 35.16it/s]Train epoch: 71 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.158076\n","505it [00:07, 66.76it/s]\n","epoch loss: 0.11073522259398262\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 483.53it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3996, 0.6244, 0.4881, 0.5479, 0.8822\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4424, 0.7018, 0.5449, 0.6135, 0.9103\n","rec_at_5: 0.5906\n","prec_at_5: 0.5972\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 72\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 72 [batch #0, batch_size 16, seq length 212]\tLoss: 0.154103\n","16it [00:00, 151.65it/s]Train epoch: 72 [batch #25, batch_size 16, seq length 571]\tLoss: 0.079741\n","46it [00:00, 129.56it/s]Train epoch: 72 [batch #50, batch_size 16, seq length 709]\tLoss: 0.088431\n","73it [00:00, 120.69it/s]Train epoch: 72 [batch #75, batch_size 16, seq length 806]\tLoss: 0.101387\n","98it [00:00, 107.33it/s]Train epoch: 72 [batch #100, batch_size 16, seq length 892]\tLoss: 0.083979\n","120it [00:01, 104.83it/s]Train epoch: 72 [batch #125, batch_size 16, seq length 978]\tLoss: 0.093363\n","141it [00:01, 97.06it/s]Train epoch: 72 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.106887\n","171it [00:01, 90.50it/s]Train epoch: 72 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.094910\n","199it [00:01, 83.37it/s]Train epoch: 72 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.102234\n","217it [00:02, 77.69it/s]Train epoch: 72 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.100680\n","244it [00:02, 75.84it/s]Train epoch: 72 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.109393\n","268it [00:02, 72.71it/s]Train epoch: 72 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.112070\n","300it [00:03, 70.22it/s]Train epoch: 72 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.120986\n","323it [00:03, 65.26it/s]Train epoch: 72 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.112199\n","344it [00:04, 63.61it/s]Train epoch: 72 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.115521\n","372it [00:04, 61.34it/s]Train epoch: 72 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.127037\n","398it [00:05, 56.02it/s]Train epoch: 72 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.127368\n","422it [00:05, 53.84it/s]Train epoch: 72 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.123481\n","446it [00:05, 48.46it/s]Train epoch: 72 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.125261\n","471it [00:06, 44.53it/s]Train epoch: 72 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.145212\n","500it [00:07, 35.39it/s]Train epoch: 72 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.156190\n","505it [00:07, 67.09it/s]\n","epoch loss: 0.1098944899506203\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 483.15it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3999, 0.6244, 0.4882, 0.5480, 0.8821\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4413, 0.7024, 0.5428, 0.6124, 0.9098\n","rec_at_5: 0.5909\n","prec_at_5: 0.5968\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 73\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 73 [batch #0, batch_size 16, seq length 212]\tLoss: 0.145632\n","16it [00:00, 155.74it/s]Train epoch: 73 [batch #25, batch_size 16, seq length 571]\tLoss: 0.076319\n","46it [00:00, 131.66it/s]Train epoch: 73 [batch #50, batch_size 16, seq length 709]\tLoss: 0.090428\n","73it [00:00, 116.84it/s]Train epoch: 73 [batch #75, batch_size 16, seq length 806]\tLoss: 0.101144\n","97it [00:00, 113.53it/s]Train epoch: 73 [batch #100, batch_size 16, seq length 892]\tLoss: 0.084306\n","120it [00:01, 104.33it/s]Train epoch: 73 [batch #125, batch_size 16, seq length 978]\tLoss: 0.092094\n","141it [00:01, 97.19it/s]Train epoch: 73 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.104429\n","171it [00:01, 91.66it/s]Train epoch: 73 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.092395\n","200it [00:01, 85.29it/s]Train epoch: 73 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.098652\n","218it [00:02, 82.07it/s]Train epoch: 73 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.098320\n","244it [00:02, 76.96it/s]Train epoch: 73 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.108071\n","268it [00:02, 72.68it/s]Train epoch: 73 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.107872\n","300it [00:03, 68.10it/s]Train epoch: 73 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.118808\n","322it [00:03, 63.85it/s]Train epoch: 73 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.108757\n","350it [00:04, 62.28it/s]Train epoch: 73 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.114485\n","371it [00:04, 60.41it/s]Train epoch: 73 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.125609\n","397it [00:04, 56.79it/s]Train epoch: 73 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.125275\n","421it [00:05, 53.21it/s]Train epoch: 73 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.118955\n","449it [00:06, 47.70it/s]Train epoch: 73 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.118303\n","474it [00:06, 42.53it/s]Train epoch: 73 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.145202\n","497it [00:07, 36.13it/s]Train epoch: 73 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.154177\n","505it [00:07, 66.93it/s]\n","epoch loss: 0.10799052915213132\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 477.63it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3979, 0.6234, 0.4852, 0.5457, 0.8819\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4404, 0.7020, 0.5416, 0.6115, 0.9095\n","rec_at_5: 0.5913\n","prec_at_5: 0.5972\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 74\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 74 [batch #0, batch_size 16, seq length 212]\tLoss: 0.139116\n","17it [00:00, 160.40it/s]Train epoch: 74 [batch #25, batch_size 16, seq length 571]\tLoss: 0.077350\n","49it [00:00, 128.22it/s]Train epoch: 74 [batch #50, batch_size 16, seq length 709]\tLoss: 0.084417\n","75it [00:00, 122.48it/s]Train epoch: 74 [batch #75, batch_size 16, seq length 806]\tLoss: 0.100826\n","100it [00:00, 112.50it/s]Train epoch: 74 [batch #100, batch_size 16, seq length 892]\tLoss: 0.084369\n","123it [00:01, 102.52it/s]Train epoch: 74 [batch #125, batch_size 16, seq length 978]\tLoss: 0.088262\n","145it [00:01, 98.25it/s] Train epoch: 74 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.103018\n","175it [00:01, 91.29it/s]Train epoch: 74 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.091281\n","194it [00:01, 86.41it/s]Train epoch: 74 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.098732\n","221it [00:02, 79.37it/s]Train epoch: 74 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.096508\n","248it [00:02, 76.68it/s]Train epoch: 74 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.106062\n","272it [00:02, 72.92it/s]Train epoch: 74 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.110719\n","296it [00:03, 68.33it/s]Train epoch: 74 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.117411\n","325it [00:03, 66.11it/s]Train epoch: 74 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.107258\n","346it [00:04, 63.64it/s]Train epoch: 74 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.111216\n","374it [00:04, 60.34it/s]Train epoch: 74 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.124294\n","400it [00:05, 54.71it/s]Train epoch: 74 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.123969\n","424it [00:05, 52.85it/s]Train epoch: 74 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.118392\n","446it [00:05, 47.82it/s]Train epoch: 74 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.118673\n","471it [00:06, 42.96it/s]Train epoch: 74 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.139673\n","499it [00:07, 36.52it/s]Train epoch: 74 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.151306\n","505it [00:07, 67.24it/s]\n","epoch loss: 0.10623470038396887\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 485.28it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3961, 0.6211, 0.4841, 0.5441, 0.8817\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4388, 0.7002, 0.5403, 0.6100, 0.9089\n","rec_at_5: 0.5898\n","prec_at_5: 0.5961\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 75\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 75 [batch #0, batch_size 16, seq length 212]\tLoss: 0.138612\n","16it [00:00, 155.15it/s]Train epoch: 75 [batch #25, batch_size 16, seq length 571]\tLoss: 0.075927\n","47it [00:00, 128.36it/s]Train epoch: 75 [batch #50, batch_size 16, seq length 709]\tLoss: 0.086098\n","72it [00:00, 117.29it/s]Train epoch: 75 [batch #75, batch_size 16, seq length 806]\tLoss: 0.099021\n","96it [00:00, 107.98it/s]Train epoch: 75 [batch #100, batch_size 16, seq length 892]\tLoss: 0.079363\n","118it [00:01, 106.52it/s]Train epoch: 75 [batch #125, batch_size 16, seq length 978]\tLoss: 0.088800\n","150it [00:01, 90.47it/s]Train epoch: 75 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.103499\n","170it [00:01, 86.92it/s]Train epoch: 75 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.089082\n","198it [00:01, 82.93it/s]Train epoch: 75 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.095906\n","225it [00:02, 78.23it/s]Train epoch: 75 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.096692\n","249it [00:02, 77.62it/s]Train epoch: 75 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.108409\n","273it [00:02, 71.75it/s]Train epoch: 75 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.108686\n","297it [00:03, 68.62it/s]Train epoch: 75 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.116123\n","325it [00:03, 64.84it/s]Train epoch: 75 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.107198\n","346it [00:04, 63.63it/s]Train epoch: 75 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.110632\n","374it [00:04, 60.17it/s]Train epoch: 75 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.119835\n","399it [00:05, 55.06it/s]Train epoch: 75 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.122294\n","423it [00:05, 50.35it/s]Train epoch: 75 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.115889\n","449it [00:06, 46.43it/s]Train epoch: 75 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.117605\n","474it [00:06, 42.62it/s]Train epoch: 75 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.139691\n","497it [00:07, 35.48it/s]Train epoch: 75 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.148696\n","505it [00:07, 66.36it/s]\n","epoch loss: 0.10485944038644285\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 488.98it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3980, 0.6247, 0.4846, 0.5458, 0.8815\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4396, 0.7022, 0.5403, 0.6107, 0.9085\n","rec_at_5: 0.5898\n","prec_at_5: 0.5958\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 76\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 76 [batch #0, batch_size 16, seq length 212]\tLoss: 0.144621\n","16it [00:00, 159.30it/s]Train epoch: 76 [batch #25, batch_size 16, seq length 571]\tLoss: 0.073965\n","48it [00:00, 129.52it/s]Train epoch: 76 [batch #50, batch_size 16, seq length 709]\tLoss: 0.085316\n","75it [00:00, 116.69it/s]Train epoch: 76 [batch #75, batch_size 16, seq length 806]\tLoss: 0.097583\n","99it [00:00, 107.78it/s]Train epoch: 76 [batch #100, batch_size 16, seq length 892]\tLoss: 0.079476\n","121it [00:01, 105.21it/s]Train epoch: 76 [batch #125, batch_size 16, seq length 978]\tLoss: 0.086422\n","142it [00:01, 99.06it/s]Train epoch: 76 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.100567\n","172it [00:01, 91.46it/s]Train epoch: 76 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.085965\n","200it [00:01, 86.82it/s]Train epoch: 76 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.094258\n","218it [00:02, 80.65it/s]Train epoch: 76 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.092719\n","244it [00:02, 77.24it/s]Train epoch: 76 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.104093\n","268it [00:02, 74.48it/s]Train epoch: 76 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.105303\n","300it [00:03, 67.23it/s]Train epoch: 76 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.112804\n","322it [00:03, 63.88it/s]Train epoch: 76 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.104520\n","350it [00:04, 63.96it/s]Train epoch: 76 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.106898\n","371it [00:04, 58.95it/s]Train epoch: 76 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.117930\n","395it [00:04, 55.69it/s]Train epoch: 76 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.120468\n","425it [00:05, 50.30it/s]Train epoch: 76 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.115617\n","447it [00:06, 46.84it/s]Train epoch: 76 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.116469\n","472it [00:06, 42.69it/s]Train epoch: 76 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.136530\n","500it [00:07, 36.31it/s]Train epoch: 76 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.146984\n","505it [00:07, 66.82it/s]\n","epoch loss: 0.1029906738623239\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 478.87it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3965, 0.6211, 0.4838, 0.5439, 0.8813\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4378, 0.6991, 0.5394, 0.6089, 0.9082\n","rec_at_5: 0.5895\n","prec_at_5: 0.5963\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 77\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 77 [batch #0, batch_size 16, seq length 212]\tLoss: 0.152041\n","16it [00:00, 154.90it/s]Train epoch: 77 [batch #25, batch_size 16, seq length 571]\tLoss: 0.070537\n","47it [00:00, 125.82it/s]Train epoch: 77 [batch #50, batch_size 16, seq length 709]\tLoss: 0.082207\n","73it [00:00, 115.19it/s]Train epoch: 77 [batch #75, batch_size 16, seq length 806]\tLoss: 0.093947\n","97it [00:00, 111.29it/s]Train epoch: 77 [batch #100, batch_size 16, seq length 892]\tLoss: 0.080702\n","120it [00:01, 103.48it/s]Train epoch: 77 [batch #125, batch_size 16, seq length 978]\tLoss: 0.086642\n","141it [00:01, 96.78it/s]Train epoch: 77 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.099004\n","171it [00:01, 89.61it/s]Train epoch: 77 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.086556\n","199it [00:01, 82.66it/s]Train epoch: 77 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.092412\n","225it [00:02, 79.02it/s]Train epoch: 77 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.092407\n","249it [00:02, 74.03it/s]Train epoch: 77 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.103768\n","273it [00:03, 71.04it/s]Train epoch: 77 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.104111\n","297it [00:03, 68.70it/s]Train epoch: 77 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.108953\n","325it [00:03, 62.75it/s]Train epoch: 77 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.105590\n","346it [00:04, 61.08it/s]Train epoch: 77 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.108024\n","374it [00:04, 57.56it/s]Train epoch: 77 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.118342\n","398it [00:05, 53.08it/s]Train epoch: 77 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.118587\n","422it [00:05, 51.00it/s]Train epoch: 77 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.112281\n","449it [00:06, 46.04it/s]Train epoch: 77 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.115050\n","474it [00:06, 40.98it/s]Train epoch: 77 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.134700\n","497it [00:07, 36.95it/s]Train epoch: 77 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.145793\n","505it [00:07, 65.84it/s]\n","epoch loss: 0.1016007522840311\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 478.31it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3954, 0.6194, 0.4828, 0.5426, 0.8811\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4375, 0.6981, 0.5396, 0.6087, 0.9078\n","rec_at_5: 0.5883\n","prec_at_5: 0.5948\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 78\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 78 [batch #0, batch_size 16, seq length 212]\tLoss: 0.138748\n","15it [00:00, 149.90it/s]Train epoch: 78 [batch #25, batch_size 16, seq length 571]\tLoss: 0.071486\n","44it [00:00, 124.77it/s]Train epoch: 78 [batch #50, batch_size 16, seq length 709]\tLoss: 0.082369\n","70it [00:00, 119.27it/s]Train epoch: 78 [batch #75, batch_size 16, seq length 806]\tLoss: 0.095560\n","94it [00:00, 105.89it/s]Train epoch: 78 [batch #100, batch_size 16, seq length 892]\tLoss: 0.076319\n","116it [00:01, 104.42it/s]Train epoch: 78 [batch #125, batch_size 16, seq length 978]\tLoss: 0.085311\n","147it [00:01, 93.43it/s]Train epoch: 78 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.097628\n","167it [00:01, 88.97it/s]Train epoch: 78 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.084687\n","195it [00:01, 85.78it/s]Train epoch: 78 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.091297\n","222it [00:02, 79.77it/s]Train epoch: 78 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.090161\n","247it [00:02, 73.17it/s]Train epoch: 78 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.099560\n","271it [00:02, 70.53it/s]Train epoch: 78 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.101700\n","295it [00:03, 67.71it/s]Train epoch: 78 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.110025\n","325it [00:03, 66.71it/s]Train epoch: 78 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.100114\n","346it [00:04, 63.42it/s]Train epoch: 78 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.105548\n","373it [00:04, 57.91it/s]Train epoch: 78 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.115501\n","397it [00:05, 56.75it/s]Train epoch: 78 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.116971\n","421it [00:05, 51.53it/s]Train epoch: 78 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.110346\n","449it [00:06, 46.60it/s]Train epoch: 78 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.113151\n","474it [00:06, 41.98it/s]Train epoch: 78 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.132810\n","497it [00:07, 37.13it/s]Train epoch: 78 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.142852\n","505it [00:07, 66.36it/s]\n","epoch loss: 0.0998941361719724\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 482.54it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3952, 0.6180, 0.4829, 0.5422, 0.8807\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4369, 0.6975, 0.5390, 0.6081, 0.9072\n","rec_at_5: 0.5879\n","prec_at_5: 0.5944\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 79\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 79 [batch #0, batch_size 16, seq length 212]\tLoss: 0.135638\n","17it [00:00, 161.07it/s]Train epoch: 79 [batch #25, batch_size 16, seq length 571]\tLoss: 0.070559\n","49it [00:00, 129.15it/s]Train epoch: 79 [batch #50, batch_size 16, seq length 709]\tLoss: 0.080548\n","63it [00:00, 125.58it/s]Train epoch: 79 [batch #75, batch_size 16, seq length 806]\tLoss: 0.094476\n","100it [00:00, 108.93it/s]Train epoch: 79 [batch #100, batch_size 16, seq length 892]\tLoss: 0.078170\n","122it [00:01, 106.51it/s]Train epoch: 79 [batch #125, batch_size 16, seq length 978]\tLoss: 0.082802\n","144it [00:01, 95.58it/s]Train epoch: 79 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.097062\n","174it [00:01, 87.35it/s]Train epoch: 79 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.084694\n","192it [00:01, 84.95it/s]Train epoch: 79 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.091481\n","219it [00:02, 80.42it/s]Train epoch: 79 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.091507\n","245it [00:02, 78.04it/s]Train epoch: 79 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.100891\n","269it [00:02, 72.38it/s]Train epoch: 79 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.101387\n","299it [00:03, 65.77it/s]Train epoch: 79 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.107180\n","321it [00:03, 65.64it/s]Train epoch: 79 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.101523\n","349it [00:04, 62.99it/s]Train epoch: 79 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.103340\n","370it [00:04, 57.32it/s]Train epoch: 79 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.115101\n","395it [00:05, 54.62it/s]Train epoch: 79 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.115263\n","425it [00:05, 49.81it/s]Train epoch: 79 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.108978\n","447it [00:06, 47.23it/s]Train epoch: 79 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.110565\n","472it [00:06, 42.08it/s]Train epoch: 79 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.130334\n","498it [00:07, 33.93it/s]Train epoch: 79 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.137335\n","505it [00:07, 65.55it/s]\n","epoch loss: 0.0985099624657985\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 470.26it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3959, 0.6188, 0.4840, 0.5431, 0.8805\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4367, 0.6981, 0.5384, 0.6080, 0.9065\n","rec_at_5: 0.5879\n","prec_at_5: 0.5938\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 80\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 80 [batch #0, batch_size 16, seq length 212]\tLoss: 0.141544\n","16it [00:00, 153.23it/s]Train epoch: 80 [batch #25, batch_size 16, seq length 571]\tLoss: 0.070557\n","47it [00:00, 129.38it/s]Train epoch: 80 [batch #50, batch_size 16, seq length 709]\tLoss: 0.077566\n","74it [00:00, 118.07it/s]Train epoch: 80 [batch #75, batch_size 16, seq length 806]\tLoss: 0.090940\n","98it [00:00, 107.15it/s]Train epoch: 80 [batch #100, batch_size 16, seq length 892]\tLoss: 0.079284\n","120it [00:01, 97.01it/s] Train epoch: 80 [batch #125, batch_size 16, seq length 978]\tLoss: 0.081836\n","141it [00:01, 98.49it/s]Train epoch: 80 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.094710\n","171it [00:01, 92.67it/s]Train epoch: 80 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.083295\n","199it [00:01, 82.01it/s]Train epoch: 80 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.089638\n","217it [00:02, 81.69it/s]Train epoch: 80 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.090597\n","243it [00:02, 77.90it/s]Train epoch: 80 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.097610\n","275it [00:03, 71.33it/s]Train epoch: 80 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.098023\n","299it [00:03, 68.31it/s]Train epoch: 80 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.104586\n","320it [00:03, 65.77it/s]Train epoch: 80 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.098040\n","348it [00:04, 58.88it/s]Train epoch: 80 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.100694\n","375it [00:04, 59.08it/s]Train epoch: 80 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.112219\n","399it [00:05, 43.45it/s]Train epoch: 80 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.113601\n","425it [00:06, 29.83it/s]Train epoch: 80 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.107706\n","450it [00:07, 20.81it/s]Train epoch: 80 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.108495\n","474it [00:08, 19.29it/s]Train epoch: 80 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.126300\n","499it [00:09, 19.48it/s]Train epoch: 80 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.136449\n","505it [00:10, 50.19it/s]\n","epoch loss: 0.09670087357263754\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 391.29it/s]\n","Finish save rediction by checkpoint  80\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3952, 0.6188, 0.4824, 0.5422, 0.8803\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4355, 0.6975, 0.5369, 0.6067, 0.9061\n","rec_at_5: 0.5859\n","prec_at_5: 0.5928\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 81\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 81 [batch #0, batch_size 16, seq length 212]\tLoss: 0.147502\n","15it [00:00, 149.75it/s]Train epoch: 81 [batch #25, batch_size 16, seq length 571]\tLoss: 0.068794\n","45it [00:00, 131.84it/s]Train epoch: 81 [batch #50, batch_size 16, seq length 709]\tLoss: 0.079021\n","72it [00:00, 124.01it/s]Train epoch: 81 [batch #75, batch_size 16, seq length 806]\tLoss: 0.092717\n","97it [00:00, 111.89it/s]Train epoch: 81 [batch #100, batch_size 16, seq length 892]\tLoss: 0.076586\n","120it [00:01, 101.48it/s]Train epoch: 81 [batch #125, batch_size 16, seq length 978]\tLoss: 0.080528\n","141it [00:01, 95.25it/s]Train epoch: 81 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.096122\n","171it [00:01, 89.21it/s]Train epoch: 81 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.080479\n","200it [00:01, 83.28it/s]Train epoch: 81 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.088998\n","218it [00:02, 80.33it/s]Train epoch: 81 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.087572\n","243it [00:02, 72.02it/s]Train epoch: 81 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.094749\n","275it [00:03, 71.72it/s]Train epoch: 81 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.097084\n","298it [00:03, 68.55it/s]Train epoch: 81 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.104283\n","319it [00:03, 63.59it/s]Train epoch: 81 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.097262\n","347it [00:04, 61.82it/s]Train epoch: 81 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.100681\n","374it [00:04, 57.42it/s]Train epoch: 81 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.111192\n","398it [00:05, 56.91it/s]Train epoch: 81 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.111030\n","422it [00:05, 51.58it/s]Train epoch: 81 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.105086\n","448it [00:06, 44.39it/s]Train epoch: 81 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.105015\n","473it [00:06, 42.89it/s]Train epoch: 81 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.125875\n","500it [00:07, 33.71it/s]Train epoch: 81 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.133669\n","505it [00:07, 65.59it/s]\n","epoch loss: 0.09528246891528074\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 471.77it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3940, 0.6151, 0.4823, 0.5407, 0.8800\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4340, 0.6945, 0.5365, 0.6053, 0.9056\n","rec_at_5: 0.5859\n","prec_at_5: 0.5929\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 82\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 82 [batch #0, batch_size 16, seq length 212]\tLoss: 0.129545\n","16it [00:00, 151.51it/s]Train epoch: 82 [batch #25, batch_size 16, seq length 571]\tLoss: 0.066138\n","47it [00:00, 121.70it/s]Train epoch: 82 [batch #50, batch_size 16, seq length 709]\tLoss: 0.077555\n","73it [00:00, 112.05it/s]Train epoch: 82 [batch #75, batch_size 16, seq length 806]\tLoss: 0.089150\n","97it [00:00, 109.73it/s]Train epoch: 82 [batch #100, batch_size 16, seq length 892]\tLoss: 0.074393\n","120it [00:01, 94.32it/s]Train epoch: 82 [batch #125, batch_size 16, seq length 978]\tLoss: 0.079686\n","150it [00:01, 89.80it/s]Train epoch: 82 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.091820\n","170it [00:01, 90.43it/s]Train epoch: 82 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.081301\n","198it [00:02, 83.49it/s]Train epoch: 82 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.086966\n","225it [00:02, 76.30it/s]Train epoch: 82 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.086513\n","249it [00:02, 73.36it/s]Train epoch: 82 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.095620\n","273it [00:03, 67.21it/s]Train epoch: 82 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.099094\n","294it [00:03, 63.34it/s]Train epoch: 82 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.100283\n","322it [00:03, 61.99it/s]Train epoch: 82 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.096682\n","349it [00:04, 58.55it/s]Train epoch: 82 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.099927\n","374it [00:04, 56.42it/s]Train epoch: 82 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.107572\n","398it [00:05, 52.51it/s]Train epoch: 82 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.108298\n","421it [00:05, 47.00it/s]Train epoch: 82 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.104225\n","446it [00:06, 44.84it/s]Train epoch: 82 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.103815\n","471it [00:06, 41.83it/s]Train epoch: 82 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.124615\n","500it [00:07, 33.49it/s]Train epoch: 82 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.131128\n","505it [00:07, 63.28it/s]\n","epoch loss: 0.09352074886607652\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 463.12it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3941, 0.6167, 0.4810, 0.5405, 0.8799\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4338, 0.6958, 0.5354, 0.6051, 0.9052\n","rec_at_5: 0.5860\n","prec_at_5: 0.5930\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 83\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 83 [batch #0, batch_size 16, seq length 212]\tLoss: 0.132648\n","16it [00:00, 155.75it/s]Train epoch: 83 [batch #25, batch_size 16, seq length 571]\tLoss: 0.065699\n","47it [00:00, 122.44it/s]Train epoch: 83 [batch #50, batch_size 16, seq length 709]\tLoss: 0.075103\n","73it [00:00, 119.46it/s]Train epoch: 83 [batch #75, batch_size 16, seq length 806]\tLoss: 0.088519\n","98it [00:00, 110.86it/s]Train epoch: 83 [batch #100, batch_size 16, seq length 892]\tLoss: 0.071427\n","121it [00:01, 100.52it/s]Train epoch: 83 [batch #125, batch_size 16, seq length 978]\tLoss: 0.077404\n","143it [00:01, 98.08it/s]Train epoch: 83 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.091599\n","173it [00:01, 85.06it/s]Train epoch: 83 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.078966\n","200it [00:02, 80.16it/s]Train epoch: 83 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.083704\n","218it [00:02, 76.56it/s]Train epoch: 83 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.083189\n","250it [00:02, 74.08it/s]Train epoch: 83 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.093436\n","274it [00:03, 72.42it/s]Train epoch: 83 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.093931\n","297it [00:03, 67.92it/s]Train epoch: 83 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.100337\n","325it [00:03, 62.16it/s]Train epoch: 83 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.093199\n","346it [00:04, 59.01it/s]Train epoch: 83 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.096042\n","371it [00:04, 57.52it/s]Train epoch: 83 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.105518\n","396it [00:05, 55.62it/s]Train epoch: 83 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.107873\n","420it [00:05, 49.55it/s]Train epoch: 83 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.102298\n","447it [00:06, 45.76it/s]Train epoch: 83 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.102666\n","472it [00:06, 41.75it/s]Train epoch: 83 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.123029\n","499it [00:07, 32.85it/s]Train epoch: 83 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.128105\n","505it [00:07, 64.97it/s]\n","epoch loss: 0.09210552304688066\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.60it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3922, 0.6151, 0.4790, 0.5386, 0.8796\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4319, 0.6944, 0.5333, 0.6033, 0.9045\n","rec_at_5: 0.5829\n","prec_at_5: 0.5898\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 84\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 84 [batch #0, batch_size 16, seq length 212]\tLoss: 0.137110\n","15it [00:00, 146.50it/s]Train epoch: 84 [batch #25, batch_size 16, seq length 571]\tLoss: 0.062562\n","45it [00:00, 126.31it/s]Train epoch: 84 [batch #50, batch_size 16, seq length 709]\tLoss: 0.074559\n","71it [00:00, 122.18it/s]Train epoch: 84 [batch #75, batch_size 16, seq length 806]\tLoss: 0.086770\n","96it [00:00, 109.61it/s]Train epoch: 84 [batch #100, batch_size 16, seq length 892]\tLoss: 0.072470\n","119it [00:01, 102.25it/s]Train epoch: 84 [batch #125, batch_size 16, seq length 978]\tLoss: 0.077532\n","150it [00:01, 88.18it/s]Train epoch: 84 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.090961\n","168it [00:01, 85.81it/s]Train epoch: 84 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.077432\n","195it [00:01, 82.94it/s]Train epoch: 84 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.083326\n","220it [00:02, 76.30it/s]Train epoch: 84 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.084869\n","244it [00:02, 72.01it/s]Train epoch: 84 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.090674\n","268it [00:03, 70.36it/s]Train epoch: 84 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.092730\n","300it [00:03, 68.37it/s]Train epoch: 84 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.097197\n","322it [00:03, 65.54it/s]Train epoch: 84 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.094577\n","350it [00:04, 61.73it/s]Train epoch: 84 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.095153\n","370it [00:04, 55.87it/s]Train epoch: 84 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.105242\n","400it [00:05, 52.38it/s]Train epoch: 84 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.103934\n","424it [00:05, 51.76it/s]Train epoch: 84 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.099576\n","446it [00:06, 46.52it/s]Train epoch: 84 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.099947\n","471it [00:06, 41.58it/s]Train epoch: 84 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.117942\n","497it [00:07, 35.16it/s]Train epoch: 84 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.125883\n","505it [00:07, 64.97it/s]\n","epoch loss: 0.09051048368363097\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 480.77it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3914, 0.6135, 0.4790, 0.5379, 0.8794\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4310, 0.6928, 0.5328, 0.6024, 0.9041\n","rec_at_5: 0.5825\n","prec_at_5: 0.5897\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 85\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 85 [batch #0, batch_size 16, seq length 212]\tLoss: 0.129418\n","16it [00:00, 157.40it/s]Train epoch: 85 [batch #25, batch_size 16, seq length 571]\tLoss: 0.063021\n","47it [00:00, 128.21it/s]Train epoch: 85 [batch #50, batch_size 16, seq length 709]\tLoss: 0.074109\n","74it [00:00, 123.50it/s]Train epoch: 85 [batch #75, batch_size 16, seq length 806]\tLoss: 0.085799\n","99it [00:00, 111.62it/s]Train epoch: 85 [batch #100, batch_size 16, seq length 892]\tLoss: 0.071537\n","122it [00:01, 102.45it/s]Train epoch: 85 [batch #125, batch_size 16, seq length 978]\tLoss: 0.075592\n","143it [00:01, 97.49it/s]Train epoch: 85 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.088697\n","172it [00:01, 83.85it/s]Train epoch: 85 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.075107\n","199it [00:02, 78.08it/s]Train epoch: 85 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.083417\n","217it [00:02, 80.10it/s]Train epoch: 85 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.082837\n","243it [00:02, 76.70it/s]Train epoch: 85 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.092060\n","275it [00:03, 69.20it/s]Train epoch: 85 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.091904\n","296it [00:03, 68.41it/s]Train epoch: 85 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.096457\n","324it [00:03, 64.88it/s]Train epoch: 85 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.090995\n","345it [00:04, 62.82it/s]Train epoch: 85 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.096035\n","372it [00:04, 56.22it/s]Train epoch: 85 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.102115\n","396it [00:05, 54.28it/s]Train epoch: 85 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.104223\n","420it [00:05, 52.61it/s]Train epoch: 85 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.095855\n","448it [00:06, 45.73it/s]Train epoch: 85 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.098064\n","473it [00:06, 40.90it/s]Train epoch: 85 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.117553\n","499it [00:07, 34.02it/s]Train epoch: 85 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.123615\n","505it [00:07, 65.02it/s]\n","epoch loss: 0.08933548220239654\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 469.12it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3903, 0.6102, 0.4787, 0.5365, 0.8791\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4307, 0.6902, 0.5339, 0.6020, 0.9041\n","rec_at_5: 0.5809\n","prec_at_5: 0.5888\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 86\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 86 [batch #0, batch_size 16, seq length 212]\tLoss: 0.128486\n","15it [00:00, 149.55it/s]Train epoch: 86 [batch #25, batch_size 16, seq length 571]\tLoss: 0.060669\n","45it [00:00, 128.65it/s]Train epoch: 86 [batch #50, batch_size 16, seq length 709]\tLoss: 0.072770\n","72it [00:00, 118.85it/s]Train epoch: 86 [batch #75, batch_size 16, seq length 806]\tLoss: 0.086421\n","96it [00:00, 107.16it/s]Train epoch: 86 [batch #100, batch_size 16, seq length 892]\tLoss: 0.069715\n","118it [00:01, 104.78it/s]Train epoch: 86 [batch #125, batch_size 16, seq length 978]\tLoss: 0.073619\n","149it [00:01, 97.21it/s]Train epoch: 86 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.086647\n","169it [00:01, 89.91it/s]Train epoch: 86 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.076500\n","197it [00:01, 83.66it/s]Train epoch: 86 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.082557\n","224it [00:02, 80.87it/s]Train epoch: 86 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.083872\n","249it [00:02, 73.66it/s]Train epoch: 86 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.086997\n","273it [00:03, 69.09it/s]Train epoch: 86 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.091842\n","296it [00:03, 68.56it/s]Train epoch: 86 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.094762\n","325it [00:03, 65.81it/s]Train epoch: 86 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.089507\n","346it [00:04, 62.45it/s]Train epoch: 86 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.091676\n","374it [00:04, 59.33it/s]Train epoch: 86 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.099478\n","398it [00:05, 56.75it/s]Train epoch: 86 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.103753\n","422it [00:05, 50.79it/s]Train epoch: 86 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.096970\n","449it [00:06, 46.80it/s]Train epoch: 86 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.097219\n","474it [00:06, 43.27it/s]Train epoch: 86 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.115464\n","497it [00:07, 36.76it/s]Train epoch: 86 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.119507\n","505it [00:07, 66.28it/s]\n","epoch loss: 0.08760904639332306\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 483.18it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3882, 0.6086, 0.4751, 0.5336, 0.8790\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4283, 0.6903, 0.5302, 0.5998, 0.9033\n","rec_at_5: 0.5816\n","prec_at_5: 0.5893\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 87\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 87 [batch #0, batch_size 16, seq length 212]\tLoss: 0.126175\n","16it [00:00, 157.42it/s]Train epoch: 87 [batch #25, batch_size 16, seq length 571]\tLoss: 0.062989\n","47it [00:00, 127.93it/s]Train epoch: 87 [batch #50, batch_size 16, seq length 709]\tLoss: 0.071219\n","74it [00:00, 123.95it/s]Train epoch: 87 [batch #75, batch_size 16, seq length 806]\tLoss: 0.083099\n","99it [00:00, 110.37it/s]Train epoch: 87 [batch #100, batch_size 16, seq length 892]\tLoss: 0.068881\n","122it [00:01, 98.77it/s] Train epoch: 87 [batch #125, batch_size 16, seq length 978]\tLoss: 0.075315\n","142it [00:01, 87.77it/s]Train epoch: 87 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.085899\n","169it [00:01, 84.21it/s]Train epoch: 87 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.075996\n","196it [00:01, 82.17it/s]Train epoch: 87 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.082875\n","222it [00:02, 77.98it/s]Train epoch: 87 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.078854\n","246it [00:02, 72.28it/s]Train epoch: 87 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.088824\n","270it [00:03, 67.47it/s]Train epoch: 87 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.090008\n","300it [00:03, 65.98it/s]Train epoch: 87 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.094195\n","321it [00:03, 58.93it/s]Train epoch: 87 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.087088\n","349it [00:04, 60.55it/s]Train epoch: 87 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.092698\n","374it [00:04, 49.61it/s]Train epoch: 87 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.101529\n","398it [00:05, 49.43it/s]Train epoch: 87 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.097538\n","422it [00:05, 48.72it/s]Train epoch: 87 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.094027\n","449it [00:06, 45.53it/s]Train epoch: 87 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.098165\n","474it [00:07, 40.60it/s]Train epoch: 87 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.114663\n","500it [00:07, 32.63it/s]Train epoch: 87 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.117042\n","505it [00:08, 63.12it/s]\n","epoch loss: 0.08639412605305119\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.32it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3881, 0.6065, 0.4766, 0.5338, 0.8789\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4272, 0.6874, 0.5302, 0.5987, 0.9028\n","rec_at_5: 0.5815\n","prec_at_5: 0.5883\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 88\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 88 [batch #0, batch_size 16, seq length 212]\tLoss: 0.119347\n","15it [00:00, 148.58it/s]Train epoch: 88 [batch #25, batch_size 16, seq length 571]\tLoss: 0.060268\n","44it [00:00, 118.27it/s]Train epoch: 88 [batch #50, batch_size 16, seq length 709]\tLoss: 0.069445\n","68it [00:00, 113.43it/s]Train epoch: 88 [batch #75, batch_size 16, seq length 806]\tLoss: 0.084062\n","91it [00:00, 89.14it/s] Train epoch: 88 [batch #100, batch_size 16, seq length 892]\tLoss: 0.068495\n","118it [00:01, 70.34it/s]Train epoch: 88 [batch #125, batch_size 16, seq length 978]\tLoss: 0.072188\n","147it [00:01, 62.25it/s]Train epoch: 88 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.083232\n","173it [00:02, 55.25it/s]Train epoch: 88 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.072616\n","197it [00:02, 50.74it/s]Train epoch: 88 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.078733\n","223it [00:03, 45.75it/s]Train epoch: 88 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.079374\n","248it [00:03, 45.04it/s]Train epoch: 88 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.085148\n","273it [00:04, 43.22it/s]Train epoch: 88 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.087272\n","300it [00:05, 48.75it/s]Train epoch: 88 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.094697\n","320it [00:05, 55.38it/s]Train epoch: 88 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.086917\n","348it [00:05, 56.85it/s]Train epoch: 88 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.091575\n","374it [00:06, 55.22it/s]Train epoch: 88 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.096753\n","398it [00:06, 54.49it/s]Train epoch: 88 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.098959\n","422it [00:07, 48.94it/s]Train epoch: 88 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.092398\n","449it [00:07, 46.83it/s]Train epoch: 88 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.093437\n","474it [00:08, 41.55it/s]Train epoch: 88 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.111273\n","497it [00:09, 36.81it/s]Train epoch: 88 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.116136\n","505it [00:09, 54.03it/s]\n","epoch loss: 0.08464917897587955\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 466.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3886, 0.6080, 0.4764, 0.5342, 0.8788\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4275, 0.6881, 0.5303, 0.5990, 0.9026\n","rec_at_5: 0.5783\n","prec_at_5: 0.5865\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 89\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 89 [batch #0, batch_size 16, seq length 212]\tLoss: 0.121047\n","16it [00:00, 157.98it/s]Train epoch: 89 [batch #25, batch_size 16, seq length 571]\tLoss: 0.059944\n","47it [00:00, 129.95it/s]Train epoch: 89 [batch #50, batch_size 16, seq length 709]\tLoss: 0.068945\n","74it [00:00, 115.06it/s]Train epoch: 89 [batch #75, batch_size 16, seq length 806]\tLoss: 0.079302\n","98it [00:00, 109.16it/s]Train epoch: 89 [batch #100, batch_size 16, seq length 892]\tLoss: 0.068207\n","120it [00:01, 98.99it/s] Train epoch: 89 [batch #125, batch_size 16, seq length 978]\tLoss: 0.071826\n","150it [00:01, 91.18it/s]Train epoch: 89 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.083310\n","169it [00:01, 84.39it/s]Train epoch: 89 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.072607\n","196it [00:01, 82.99it/s]Train epoch: 89 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.077511\n","222it [00:02, 78.48it/s]Train epoch: 89 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.075769\n","246it [00:02, 68.00it/s]Train epoch: 89 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.085324\n","270it [00:03, 66.31it/s]Train epoch: 89 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.083289\n","300it [00:03, 64.37it/s]Train epoch: 89 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.088417\n","321it [00:03, 61.90it/s]Train epoch: 89 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.085164\n","349it [00:04, 62.25it/s]Train epoch: 89 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.087766\n","374it [00:04, 54.77it/s]Train epoch: 89 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.094456\n","398it [00:05, 54.88it/s]Train epoch: 89 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.095250\n","422it [00:05, 49.51it/s]Train epoch: 89 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.089520\n","447it [00:06, 45.07it/s]Train epoch: 89 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.092266\n","472it [00:06, 42.22it/s]Train epoch: 89 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.109261\n","499it [00:07, 33.09it/s]Train epoch: 89 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.112496\n","505it [00:07, 64.37it/s]\n","epoch loss: 0.08291086856238913\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.25it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3875, 0.6054, 0.4758, 0.5328, 0.8787\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4266, 0.6870, 0.5296, 0.5981, 0.9022\n","rec_at_5: 0.5773\n","prec_at_5: 0.5856\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 90\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 90 [batch #0, batch_size 16, seq length 212]\tLoss: 0.118036\n","16it [00:00, 157.73it/s]Train epoch: 90 [batch #25, batch_size 16, seq length 571]\tLoss: 0.058904\n","47it [00:00, 127.26it/s]Train epoch: 90 [batch #50, batch_size 16, seq length 709]\tLoss: 0.069836\n","73it [00:00, 121.15it/s]Train epoch: 90 [batch #75, batch_size 16, seq length 806]\tLoss: 0.080166\n","98it [00:00, 111.17it/s]Train epoch: 90 [batch #100, batch_size 16, seq length 892]\tLoss: 0.067524\n","121it [00:01, 99.97it/s] Train epoch: 90 [batch #125, batch_size 16, seq length 978]\tLoss: 0.071872\n","142it [00:01, 93.23it/s]Train epoch: 90 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.080668\n","171it [00:01, 87.50it/s]Train epoch: 90 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.069249\n","198it [00:01, 82.25it/s]Train epoch: 90 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.074778\n","224it [00:02, 75.62it/s]Train epoch: 90 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.075671\n","249it [00:02, 75.29it/s]Train epoch: 90 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.082706\n","273it [00:03, 72.57it/s]Train epoch: 90 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.085085\n","295it [00:03, 65.45it/s]Train epoch: 90 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.088408\n","323it [00:03, 62.46it/s]Train epoch: 90 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.085069\n","350it [00:04, 57.67it/s]Train epoch: 90 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.086515\n","375it [00:04, 55.97it/s]Train epoch: 90 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.093510\n","399it [00:05, 51.68it/s]Train epoch: 90 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.094072\n","421it [00:05, 48.29it/s]Train epoch: 90 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.090697\n","447it [00:06, 45.17it/s]Train epoch: 90 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.089765\n","472it [00:06, 42.69it/s]Train epoch: 90 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.108699\n","499it [00:07, 34.75it/s]Train epoch: 90 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.111912\n","505it [00:07, 64.09it/s]\n","epoch loss: 0.08175632677566592\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 473.56it/s]\n","Finish save rediction by checkpoint  90\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3859, 0.6033, 0.4743, 0.5310, 0.8787\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4254, 0.6853, 0.5286, 0.5968, 0.9018\n","rec_at_5: 0.5770\n","prec_at_5: 0.5853\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 91\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 91 [batch #0, batch_size 16, seq length 212]\tLoss: 0.113948\n","17it [00:00, 160.29it/s]Train epoch: 91 [batch #25, batch_size 16, seq length 571]\tLoss: 0.057071\n","49it [00:00, 132.70it/s]Train epoch: 91 [batch #50, batch_size 16, seq length 709]\tLoss: 0.068362\n","63it [00:00, 128.00it/s]Train epoch: 91 [batch #75, batch_size 16, seq length 806]\tLoss: 0.077616\n","100it [00:00, 107.49it/s]Train epoch: 91 [batch #100, batch_size 16, seq length 892]\tLoss: 0.066755\n","122it [00:01, 103.12it/s]Train epoch: 91 [batch #125, batch_size 16, seq length 978]\tLoss: 0.069500\n","143it [00:01, 96.83it/s]Train epoch: 91 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.080804\n","173it [00:01, 86.96it/s]Train epoch: 91 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.069478\n","200it [00:02, 81.51it/s]Train epoch: 91 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.077336\n","218it [00:02, 82.31it/s]Train epoch: 91 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.075819\n","243it [00:02, 77.70it/s]Train epoch: 91 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.081851\n","275it [00:03, 68.52it/s]Train epoch: 91 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.082937\n","299it [00:03, 66.50it/s]Train epoch: 91 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.085332\n","321it [00:03, 64.70it/s]Train epoch: 91 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.083007\n","349it [00:04, 62.44it/s]Train epoch: 91 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.086415\n","370it [00:04, 60.39it/s]Train epoch: 91 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.093278\n","396it [00:05, 55.38it/s]Train epoch: 91 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.091912\n","420it [00:05, 53.52it/s]Train epoch: 91 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.087289\n","448it [00:06, 47.29it/s]Train epoch: 91 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.088140\n","473it [00:06, 42.93it/s]Train epoch: 91 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.106197\n","500it [00:07, 34.86it/s]Train epoch: 91 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.109433\n","505it [00:07, 66.38it/s]\n","epoch loss: 0.08032805960404106\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 469.40it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3843, 0.6112, 0.4725, 0.5330, 0.8785\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4238, 0.6839, 0.5271, 0.5953, 0.9012\n","rec_at_5: 0.5760\n","prec_at_5: 0.5853\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 92\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 92 [batch #0, batch_size 16, seq length 212]\tLoss: 0.116605\n","16it [00:00, 157.01it/s]Train epoch: 92 [batch #25, batch_size 16, seq length 571]\tLoss: 0.056180\n","47it [00:00, 127.21it/s]Train epoch: 92 [batch #50, batch_size 16, seq length 709]\tLoss: 0.064146\n","73it [00:00, 120.08it/s]Train epoch: 92 [batch #75, batch_size 16, seq length 806]\tLoss: 0.075087\n","97it [00:00, 108.43it/s]Train epoch: 92 [batch #100, batch_size 16, seq length 892]\tLoss: 0.064596\n","119it [00:01, 103.41it/s]Train epoch: 92 [batch #125, batch_size 16, seq length 978]\tLoss: 0.066596\n","141it [00:01, 96.07it/s] Train epoch: 92 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.080296\n","171it [00:01, 91.31it/s]Train epoch: 92 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.069389\n","199it [00:01, 83.55it/s]Train epoch: 92 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.072307\n","217it [00:02, 82.53it/s]Train epoch: 92 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.072162\n","244it [00:02, 78.76it/s]Train epoch: 92 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.079697\n","268it [00:02, 72.04it/s]Train epoch: 92 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.082563\n","300it [00:03, 70.17it/s]Train epoch: 92 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.084318\n","322it [00:03, 67.05it/s]Train epoch: 92 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.081173\n","350it [00:04, 60.84it/s]Train epoch: 92 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.085438\n","370it [00:04, 58.38it/s]Train epoch: 92 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.093061\n","395it [00:04, 56.76it/s]Train epoch: 92 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.089490\n","425it [00:05, 51.90it/s]Train epoch: 92 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.084487\n","447it [00:06, 45.08it/s]Train epoch: 92 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.085178\n","472it [00:06, 43.63it/s]Train epoch: 92 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.100681\n","499it [00:07, 35.02it/s]Train epoch: 92 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.105136\n","505it [00:07, 66.33it/s]\n","epoch loss: 0.0785138393245121\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 479.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3834, 0.6112, 0.4711, 0.5321, 0.8785\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4232, 0.6833, 0.5266, 0.5948, 0.9009\n","rec_at_5: 0.5756\n","prec_at_5: 0.5847\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 93\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 93 [batch #0, batch_size 16, seq length 212]\tLoss: 0.115677\n","16it [00:00, 153.87it/s]Train epoch: 93 [batch #25, batch_size 16, seq length 571]\tLoss: 0.057054\n","47it [00:00, 132.39it/s]Train epoch: 93 [batch #50, batch_size 16, seq length 709]\tLoss: 0.066004\n","74it [00:00, 117.82it/s]Train epoch: 93 [batch #75, batch_size 16, seq length 806]\tLoss: 0.077455\n","98it [00:00, 105.98it/s]Train epoch: 93 [batch #100, batch_size 16, seq length 892]\tLoss: 0.065911\n","120it [00:01, 102.27it/s]Train epoch: 93 [batch #125, batch_size 16, seq length 978]\tLoss: 0.068491\n","142it [00:01, 96.80it/s]Train epoch: 93 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.078373\n","172it [00:01, 90.11it/s]Train epoch: 93 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.067313\n","200it [00:02, 80.85it/s]Train epoch: 93 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.071297\n","218it [00:02, 82.19it/s]Train epoch: 93 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.072481\n","245it [00:02, 75.79it/s]Train epoch: 93 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.079759\n","269it [00:02, 71.67it/s]Train epoch: 93 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.080460\n","293it [00:03, 70.45it/s]Train epoch: 93 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.080582\n","324it [00:03, 67.31it/s]Train epoch: 93 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.077543\n","345it [00:04, 64.67it/s]Train epoch: 93 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.084170\n","373it [00:04, 58.68it/s]Train epoch: 93 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.089843\n","397it [00:05, 55.76it/s]Train epoch: 93 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.089368\n","421it [00:05, 51.63it/s]Train epoch: 93 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.084280\n","449it [00:06, 45.21it/s]Train epoch: 93 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.085458\n","474it [00:06, 41.67it/s]Train epoch: 93 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.100841\n","500it [00:07, 33.20it/s]Train epoch: 93 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.108109\n","505it [00:07, 66.06it/s]\n","epoch loss: 0.0777455000169944\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 475.21it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3822, 0.6063, 0.4702, 0.5297, 0.8786\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4224, 0.6812, 0.5266, 0.5940, 0.9006\n","rec_at_5: 0.5734\n","prec_at_5: 0.5833\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 94\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 94 [batch #0, batch_size 16, seq length 212]\tLoss: 0.121555\n","17it [00:00, 161.19it/s]Train epoch: 94 [batch #25, batch_size 16, seq length 571]\tLoss: 0.050942\n","49it [00:00, 131.43it/s]Train epoch: 94 [batch #50, batch_size 16, seq length 709]\tLoss: 0.064881\n","63it [00:00, 126.33it/s]Train epoch: 94 [batch #75, batch_size 16, seq length 806]\tLoss: 0.076171\n","100it [00:00, 106.11it/s]Train epoch: 94 [batch #100, batch_size 16, seq length 892]\tLoss: 0.063886\n","122it [00:01, 105.28it/s]Train epoch: 94 [batch #125, batch_size 16, seq length 978]\tLoss: 0.067179\n","143it [00:01, 96.79it/s]Train epoch: 94 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.076657\n","172it [00:01, 88.58it/s]Train epoch: 94 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.067186\n","199it [00:01, 82.82it/s]Train epoch: 94 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.069178\n","217it [00:02, 78.70it/s]Train epoch: 94 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.072107\n","244it [00:02, 74.15it/s]Train epoch: 94 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.075643\n","268it [00:02, 72.04it/s]Train epoch: 94 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.079954\n","298it [00:03, 69.39it/s]Train epoch: 94 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.083494\n","319it [00:03, 64.94it/s]Train epoch: 94 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.075970\n","347it [00:04, 60.72it/s]Train epoch: 94 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.079998\n","373it [00:04, 54.59it/s]Train epoch: 94 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.088690\n","397it [00:05, 55.15it/s]Train epoch: 94 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.088462\n","421it [00:05, 48.27it/s]Train epoch: 94 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.081845\n","447it [00:06, 47.33it/s]Train epoch: 94 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.083935\n","472it [00:06, 42.40it/s]Train epoch: 94 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.099905\n","499it [00:07, 35.20it/s]Train epoch: 94 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.102933\n","505it [00:07, 65.87it/s]\n","epoch loss: 0.07623819322832445\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 472.92it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3803, 0.6057, 0.4671, 0.5275, 0.8785\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4199, 0.6822, 0.5220, 0.5914, 0.8999\n","rec_at_5: 0.5742\n","prec_at_5: 0.5845\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 95\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 95 [batch #0, batch_size 16, seq length 212]\tLoss: 0.121846\n","16it [00:00, 153.30it/s]Train epoch: 95 [batch #25, batch_size 16, seq length 571]\tLoss: 0.051515\n","46it [00:00, 125.46it/s]Train epoch: 95 [batch #50, batch_size 16, seq length 709]\tLoss: 0.065133\n","72it [00:00, 113.23it/s]Train epoch: 95 [batch #75, batch_size 16, seq length 806]\tLoss: 0.075576\n","96it [00:00, 112.15it/s]Train epoch: 95 [batch #100, batch_size 16, seq length 892]\tLoss: 0.063435\n","119it [00:01, 103.46it/s]Train epoch: 95 [batch #125, batch_size 16, seq length 978]\tLoss: 0.065965\n","150it [00:01, 94.00it/s]Train epoch: 95 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.077575\n","170it [00:01, 90.04it/s]Train epoch: 95 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.065701\n","198it [00:01, 86.37it/s]Train epoch: 95 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.070471\n","225it [00:02, 78.38it/s]Train epoch: 95 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.068554\n","250it [00:02, 74.20it/s]Train epoch: 95 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.075498\n","274it [00:03, 71.23it/s]Train epoch: 95 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.079166\n","298it [00:03, 68.47it/s]Train epoch: 95 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.079210\n","319it [00:03, 64.22it/s]Train epoch: 95 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.076024\n","347it [00:04, 63.57it/s]Train epoch: 95 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.080318\n","375it [00:04, 56.81it/s]Train epoch: 95 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.085312\n","400it [00:05, 55.42it/s]Train epoch: 95 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.085840\n","424it [00:05, 49.96it/s]Train epoch: 95 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.079441\n","447it [00:06, 47.63it/s]Train epoch: 95 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.081190\n","472it [00:06, 43.18it/s]Train epoch: 95 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.098042\n","499it [00:07, 34.02it/s]Train epoch: 95 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.101889\n","505it [00:07, 66.09it/s]\n","epoch loss: 0.07476347027770658\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 472.48it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3797, 0.6044, 0.4674, 0.5271, 0.8783\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4198, 0.6793, 0.5236, 0.5914, 0.8998\n","rec_at_5: 0.5742\n","prec_at_5: 0.5851\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 96\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 96 [batch #0, batch_size 16, seq length 212]\tLoss: 0.113667\n","16it [00:00, 153.45it/s]Train epoch: 96 [batch #25, batch_size 16, seq length 571]\tLoss: 0.051989\n","46it [00:00, 124.71it/s]Train epoch: 96 [batch #50, batch_size 16, seq length 709]\tLoss: 0.063993\n","72it [00:00, 115.00it/s]Train epoch: 96 [batch #75, batch_size 16, seq length 806]\tLoss: 0.073148\n","96it [00:00, 111.89it/s]Train epoch: 96 [batch #100, batch_size 16, seq length 892]\tLoss: 0.060983\n","119it [00:01, 101.78it/s]Train epoch: 96 [batch #125, batch_size 16, seq length 978]\tLoss: 0.063023\n","150it [00:01, 95.86it/s]Train epoch: 96 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.074264\n","170it [00:01, 88.34it/s]Train epoch: 96 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.062629\n","197it [00:01, 81.65it/s]Train epoch: 96 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.068691\n","224it [00:02, 76.50it/s]Train epoch: 96 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.070792\n","250it [00:02, 74.87it/s]Train epoch: 96 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.075982\n","274it [00:03, 69.78it/s]Train epoch: 96 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.075394\n","298it [00:03, 68.57it/s]Train epoch: 96 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.077000\n","320it [00:03, 62.85it/s]Train epoch: 96 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.076168\n","348it [00:04, 62.77it/s]Train epoch: 96 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.077392\n","369it [00:04, 60.52it/s]Train epoch: 96 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.083981\n","395it [00:05, 54.31it/s]Train epoch: 96 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.084486\n","425it [00:05, 53.18it/s]Train epoch: 96 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.081000\n","448it [00:06, 46.74it/s]Train epoch: 96 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.080047\n","473it [00:06, 43.13it/s]Train epoch: 96 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.095584\n","499it [00:07, 34.16it/s]Train epoch: 96 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.097337\n","505it [00:07, 65.90it/s]\n","epoch loss: 0.0734211856296452\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 330.78it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3796, 0.6027, 0.4679, 0.5268, 0.8782\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4189, 0.6782, 0.5228, 0.5904, 0.8994\n","rec_at_5: 0.5719\n","prec_at_5: 0.5832\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 97\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 97 [batch #0, batch_size 16, seq length 212]\tLoss: 0.114159\n","18it [00:00, 88.56it/s]Train epoch: 97 [batch #25, batch_size 16, seq length 571]\tLoss: 0.051612\n","45it [00:00, 75.49it/s]Train epoch: 97 [batch #50, batch_size 16, seq length 709]\tLoss: 0.059440\n","69it [00:00, 73.55it/s]Train epoch: 97 [batch #75, batch_size 16, seq length 806]\tLoss: 0.073439\n","100it [00:01, 65.11it/s]Train epoch: 97 [batch #100, batch_size 16, seq length 892]\tLoss: 0.060172\n","121it [00:01, 62.77it/s]Train epoch: 97 [batch #125, batch_size 16, seq length 978]\tLoss: 0.063429\n","148it [00:02, 56.65it/s]Train epoch: 97 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.074827\n","172it [00:02, 54.80it/s]Train epoch: 97 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.063624\n","200it [00:03, 46.97it/s]Train epoch: 97 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.070128\n","221it [00:03, 59.21it/s]Train epoch: 97 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.068464\n","245it [00:03, 67.16it/s]Train epoch: 97 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.072740\n","268it [00:04, 68.47it/s]Train epoch: 97 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.075406\n","297it [00:04, 62.13it/s]Train epoch: 97 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.078503\n","325it [00:05, 64.12it/s]Train epoch: 97 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.073564\n","346it [00:05, 62.97it/s]Train epoch: 97 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.077118\n","374it [00:05, 59.07it/s]Train epoch: 97 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.083787\n","398it [00:06, 53.66it/s]Train epoch: 97 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.083489\n","422it [00:06, 50.22it/s]Train epoch: 97 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.077778\n","449it [00:07, 47.08it/s]Train epoch: 97 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.078155\n","474it [00:08, 39.90it/s]Train epoch: 97 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.093407\n","500it [00:08, 34.60it/s]Train epoch: 97 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.097543\n","505it [00:09, 55.94it/s]\n","epoch loss: 0.07246231889850137\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 481.41it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3787, 0.6182, 0.4675, 0.5324, 0.8781\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4173, 0.6774, 0.5207, 0.5888, 0.8990\n","rec_at_5: 0.5727\n","prec_at_5: 0.5837\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 98\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 98 [batch #0, batch_size 16, seq length 212]\tLoss: 0.112009\n","16it [00:00, 154.02it/s]Train epoch: 98 [batch #25, batch_size 16, seq length 571]\tLoss: 0.051136\n","46it [00:00, 124.06it/s]Train epoch: 98 [batch #50, batch_size 16, seq length 709]\tLoss: 0.061736\n","72it [00:00, 117.49it/s]Train epoch: 98 [batch #75, batch_size 16, seq length 806]\tLoss: 0.072151\n","96it [00:00, 108.26it/s]Train epoch: 98 [batch #100, batch_size 16, seq length 892]\tLoss: 0.060751\n","118it [00:01, 101.88it/s]Train epoch: 98 [batch #125, batch_size 16, seq length 978]\tLoss: 0.061917\n","150it [00:01, 92.74it/s]Train epoch: 98 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.071170\n","170it [00:01, 89.77it/s]Train epoch: 98 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.062370\n","198it [00:02, 82.13it/s]Train epoch: 98 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.066870\n","224it [00:02, 76.04it/s]Train epoch: 98 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.066871\n","248it [00:02, 74.05it/s]Train epoch: 98 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.071977\n","272it [00:03, 65.68it/s]Train epoch: 98 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.073327\n","293it [00:03, 63.73it/s]Train epoch: 98 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.075459\n","323it [00:03, 65.50it/s]Train epoch: 98 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.072747\n","344it [00:04, 63.80it/s]Train epoch: 98 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.076071\n","370it [00:04, 57.62it/s]Train epoch: 98 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.081259\n","395it [00:05, 54.76it/s]Train epoch: 98 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.081219\n","425it [00:05, 49.44it/s]Train epoch: 98 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.074871\n","450it [00:06, 43.84it/s]Train epoch: 98 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.076439\n","475it [00:06, 41.20it/s]Train epoch: 98 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.090405\n","497it [00:07, 36.02it/s]Train epoch: 98 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.094267\n","505it [00:07, 64.77it/s]\n","epoch loss: 0.07074763105057254\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 482.43it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3788, 0.6156, 0.4680, 0.5317, 0.8779\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4179, 0.6767, 0.5221, 0.5894, 0.8985\n","rec_at_5: 0.5700\n","prec_at_5: 0.5818\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 99\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 99 [batch #0, batch_size 16, seq length 212]\tLoss: 0.108280\n","16it [00:00, 153.91it/s]Train epoch: 99 [batch #25, batch_size 16, seq length 571]\tLoss: 0.047992\n","47it [00:00, 130.87it/s]Train epoch: 99 [batch #50, batch_size 16, seq length 709]\tLoss: 0.060472\n","74it [00:00, 115.75it/s]Train epoch: 99 [batch #75, batch_size 16, seq length 806]\tLoss: 0.071229\n","98it [00:00, 104.24it/s]Train epoch: 99 [batch #100, batch_size 16, seq length 892]\tLoss: 0.061517\n","120it [00:01, 102.60it/s]Train epoch: 99 [batch #125, batch_size 16, seq length 978]\tLoss: 0.061062\n","141it [00:01, 97.17it/s]Train epoch: 99 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.069424\n","171it [00:01, 89.43it/s]Train epoch: 99 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.059118\n","199it [00:01, 83.99it/s]Train epoch: 99 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.065539\n","224it [00:02, 72.92it/s]Train epoch: 99 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.065554\n","249it [00:02, 73.49it/s]Train epoch: 99 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.068278\n","273it [00:03, 69.89it/s]Train epoch: 99 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.071990\n","296it [00:03, 67.54it/s]Train epoch: 99 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.076140\n","325it [00:03, 65.96it/s]Train epoch: 99 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.070611\n","346it [00:04, 63.11it/s]Train epoch: 99 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.075904\n","374it [00:04, 58.35it/s]Train epoch: 99 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.078481\n","398it [00:05, 52.82it/s]Train epoch: 99 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.079334\n","422it [00:05, 48.25it/s]Train epoch: 99 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.072049\n","448it [00:06, 45.50it/s]Train epoch: 99 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.076961\n","473it [00:06, 41.25it/s]Train epoch: 99 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.088691\n","500it [00:07, 34.64it/s]Train epoch: 99 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.092754\n","505it [00:07, 65.56it/s]\n","epoch loss: 0.06944968295421931\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.89it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3785, 0.6157, 0.4670, 0.5311, 0.8777\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4170, 0.6760, 0.5212, 0.5886, 0.8979\n","rec_at_5: 0.5721\n","prec_at_5: 0.5826\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 100\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 100 [batch #0, batch_size 16, seq length 212]\tLoss: 0.102410\n","16it [00:00, 156.24it/s]Train epoch: 100 [batch #25, batch_size 16, seq length 571]\tLoss: 0.048207\n","47it [00:00, 126.92it/s]Train epoch: 100 [batch #50, batch_size 16, seq length 709]\tLoss: 0.056016\n","73it [00:00, 115.68it/s]Train epoch: 100 [batch #75, batch_size 16, seq length 806]\tLoss: 0.067644\n","95it [00:00, 81.54it/s]Train epoch: 100 [batch #100, batch_size 16, seq length 892]\tLoss: 0.057931\n","125it [00:01, 90.82it/s]Train epoch: 100 [batch #125, batch_size 16, seq length 978]\tLoss: 0.061525\n","145it [00:01, 90.16it/s]Train epoch: 100 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.071037\n","175it [00:01, 88.28it/s]Train epoch: 100 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.059270\n","193it [00:02, 84.66it/s]Train epoch: 100 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.063975\n","220it [00:02, 78.79it/s]Train epoch: 100 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.063553\n","246it [00:02, 74.36it/s]Train epoch: 100 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.068241\n","270it [00:03, 70.04it/s]Train epoch: 100 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.072221\n","294it [00:03, 67.30it/s]Train epoch: 100 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.071400\n","322it [00:03, 63.17it/s]Train epoch: 100 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.072563\n","350it [00:04, 61.83it/s]Train epoch: 100 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.072827\n","371it [00:04, 59.83it/s]Train epoch: 100 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.078216\n","396it [00:05, 54.48it/s]Train epoch: 100 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.076855\n","420it [00:05, 51.61it/s]Train epoch: 100 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.072309\n","448it [00:06, 47.29it/s]Train epoch: 100 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.075186\n","473it [00:06, 43.14it/s]Train epoch: 100 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.087755\n","499it [00:07, 33.94it/s]Train epoch: 100 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.089349\n","505it [00:07, 64.93it/s]\n","epoch loss: 0.06827063173756447\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 479.94it/s]\n","Finish save rediction by checkpoint  100\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3761, 0.6094, 0.4643, 0.5271, 0.8774\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4154, 0.6744, 0.5197, 0.5870, 0.8975\n","rec_at_5: 0.5724\n","prec_at_5: 0.5828\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 101\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 101 [batch #0, batch_size 16, seq length 212]\tLoss: 0.110556\n","17it [00:00, 159.79it/s]Train epoch: 101 [batch #25, batch_size 16, seq length 571]\tLoss: 0.047543\n","49it [00:00, 128.54it/s]Train epoch: 101 [batch #50, batch_size 16, seq length 709]\tLoss: 0.056713\n","63it [00:00, 125.28it/s]Train epoch: 101 [batch #75, batch_size 16, seq length 806]\tLoss: 0.068328\n","100it [00:00, 110.36it/s]Train epoch: 101 [batch #100, batch_size 16, seq length 892]\tLoss: 0.056199\n","123it [00:01, 100.73it/s]Train epoch: 101 [batch #125, batch_size 16, seq length 978]\tLoss: 0.057381\n","144it [00:01, 96.47it/s]Train epoch: 101 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.068411\n","174it [00:01, 89.40it/s]Train epoch: 101 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.060394\n","192it [00:01, 85.22it/s]Train epoch: 101 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.062570\n","219it [00:02, 82.83it/s]Train epoch: 101 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.064490\n","245it [00:02, 79.06it/s]Train epoch: 101 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.068531\n","269it [00:02, 67.57it/s]Train epoch: 101 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.072501\n","300it [00:03, 68.47it/s]Train epoch: 101 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.072859\n","321it [00:03, 65.36it/s]Train epoch: 101 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.068851\n","349it [00:04, 60.64it/s]Train epoch: 101 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.073611\n","370it [00:04, 60.25it/s]Train epoch: 101 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.077299\n","396it [00:05, 55.93it/s]Train epoch: 101 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.075122\n","420it [00:05, 54.08it/s]Train epoch: 101 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.071545\n","447it [00:06, 45.82it/s]Train epoch: 101 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.071232\n","472it [00:06, 42.27it/s]Train epoch: 101 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.086629\n","500it [00:07, 34.67it/s]Train epoch: 101 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.088051\n","505it [00:07, 65.87it/s]\n","epoch loss: 0.0672525693321287\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 467.12it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3766, 0.6086, 0.4651, 0.5273, 0.8773\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4151, 0.6729, 0.5200, 0.5867, 0.8976\n","rec_at_5: 0.5696\n","prec_at_5: 0.5822\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 102\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 102 [batch #0, batch_size 16, seq length 212]\tLoss: 0.110055\n","16it [00:00, 150.62it/s]Train epoch: 102 [batch #25, batch_size 16, seq length 571]\tLoss: 0.046085\n","46it [00:00, 113.03it/s]Train epoch: 102 [batch #50, batch_size 16, seq length 709]\tLoss: 0.056811\n","70it [00:00, 107.05it/s]Train epoch: 102 [batch #75, batch_size 16, seq length 806]\tLoss: 0.065340\n","93it [00:00, 105.31it/s]Train epoch: 102 [batch #100, batch_size 16, seq length 892]\tLoss: 0.057050\n","125it [00:01, 92.68it/s]Train epoch: 102 [batch #125, batch_size 16, seq length 978]\tLoss: 0.057940\n","145it [00:01, 94.69it/s]Train epoch: 102 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.068373\n","173it [00:01, 81.18it/s]Train epoch: 102 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.056505\n","200it [00:02, 80.50it/s]Train epoch: 102 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.060348\n","225it [00:02, 75.47it/s]Train epoch: 102 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.061730\n","249it [00:02, 72.38it/s]Train epoch: 102 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.070075\n","273it [00:03, 69.29it/s]Train epoch: 102 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.068912\n","294it [00:03, 64.35it/s]Train epoch: 102 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.070824\n","322it [00:03, 64.25it/s]Train epoch: 102 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.065124\n","350it [00:04, 57.76it/s]Train epoch: 102 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.070698\n","375it [00:04, 56.93it/s]Train epoch: 102 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.076367\n","399it [00:05, 55.12it/s]Train epoch: 102 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.073445\n","422it [00:05, 50.02it/s]Train epoch: 102 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.068084\n","449it [00:06, 45.64it/s]Train epoch: 102 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.070699\n","474it [00:07, 41.61it/s]Train epoch: 102 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.084681\n","500it [00:07, 33.32it/s]Train epoch: 102 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.086938\n","505it [00:07, 63.13it/s]\n","epoch loss: 0.06563333447026734\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.81it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3756, 0.6048, 0.4634, 0.5248, 0.8770\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4137, 0.6721, 0.5184, 0.5853, 0.8967\n","rec_at_5: 0.5721\n","prec_at_5: 0.5827\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 103\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 103 [batch #0, batch_size 16, seq length 212]\tLoss: 0.096252\n","16it [00:00, 152.13it/s]Train epoch: 103 [batch #25, batch_size 16, seq length 571]\tLoss: 0.045713\n","47it [00:00, 127.89it/s]Train epoch: 103 [batch #50, batch_size 16, seq length 709]\tLoss: 0.052778\n","73it [00:00, 110.88it/s]Train epoch: 103 [batch #75, batch_size 16, seq length 806]\tLoss: 0.064491\n","97it [00:00, 106.74it/s]Train epoch: 103 [batch #100, batch_size 16, seq length 892]\tLoss: 0.054104\n","119it [00:01, 100.20it/s]Train epoch: 103 [batch #125, batch_size 16, seq length 978]\tLoss: 0.056385\n","150it [00:01, 96.30it/s]Train epoch: 103 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.065784\n","170it [00:01, 89.51it/s]Train epoch: 103 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.056593\n","198it [00:01, 84.70it/s]Train epoch: 103 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.060665\n","225it [00:02, 80.14it/s]Train epoch: 103 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.061449\n","243it [00:02, 75.00it/s]Train epoch: 103 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.065160\n","275it [00:03, 72.28it/s]Train epoch: 103 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.066641\n","298it [00:03, 68.58it/s]Train epoch: 103 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.068474\n","319it [00:03, 66.28it/s]Train epoch: 103 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.065100\n","347it [00:04, 60.82it/s]Train epoch: 103 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.069829\n","373it [00:04, 56.00it/s]Train epoch: 103 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.073592\n","397it [00:05, 52.50it/s]Train epoch: 103 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.071156\n","421it [00:05, 49.53it/s]Train epoch: 103 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.069299\n","448it [00:06, 46.64it/s]Train epoch: 103 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.065199\n","473it [00:06, 43.57it/s]Train epoch: 103 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.082750\n","500it [00:07, 35.22it/s]Train epoch: 103 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.083357\n","505it [00:07, 65.12it/s]\n","epoch loss: 0.0641567556464141\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 487.36it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3733, 0.6000, 0.4612, 0.5215, 0.8767\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4110, 0.6696, 0.5156, 0.5826, 0.8964\n","rec_at_5: 0.5652\n","prec_at_5: 0.5780\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 104\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 104 [batch #0, batch_size 16, seq length 212]\tLoss: 0.100284\n","16it [00:00, 156.80it/s]Train epoch: 104 [batch #25, batch_size 16, seq length 571]\tLoss: 0.044086\n","47it [00:00, 132.35it/s]Train epoch: 104 [batch #50, batch_size 16, seq length 709]\tLoss: 0.054945\n","74it [00:00, 119.48it/s]Train epoch: 104 [batch #75, batch_size 16, seq length 806]\tLoss: 0.063624\n","99it [00:00, 108.13it/s]Train epoch: 104 [batch #100, batch_size 16, seq length 892]\tLoss: 0.055056\n","121it [00:01, 102.16it/s]Train epoch: 104 [batch #125, batch_size 16, seq length 978]\tLoss: 0.055221\n","143it [00:01, 101.02it/s]Train epoch: 104 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.063559\n","174it [00:01, 89.53it/s]Train epoch: 104 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.055838\n","193it [00:01, 86.62it/s]Train epoch: 104 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.059419\n","220it [00:02, 81.96it/s]Train epoch: 104 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.061826\n","246it [00:02, 74.55it/s]Train epoch: 104 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.064329\n","270it [00:02, 70.57it/s]Train epoch: 104 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.065888\n","294it [00:03, 69.44it/s]Train epoch: 104 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.066494\n","323it [00:03, 65.82it/s]Train epoch: 104 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.066186\n","344it [00:04, 63.49it/s]Train epoch: 104 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.067819\n","372it [00:04, 57.67it/s]Train epoch: 104 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.071448\n","397it [00:04, 54.39it/s]Train epoch: 104 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.070240\n","421it [00:05, 51.67it/s]Train epoch: 104 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.065685\n","449it [00:06, 45.70it/s]Train epoch: 104 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.069571\n","474it [00:06, 41.46it/s]Train epoch: 104 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.082127\n","500it [00:07, 35.29it/s]Train epoch: 104 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.079066\n","505it [00:07, 66.71it/s]\n","epoch loss: 0.06307367357508381\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 475.76it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3758, 0.6018, 0.4648, 0.5245, 0.8765\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4130, 0.6687, 0.5192, 0.5846, 0.8962\n","rec_at_5: 0.5683\n","prec_at_5: 0.5804\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 105\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 105 [batch #0, batch_size 16, seq length 212]\tLoss: 0.103696\n","16it [00:00, 157.47it/s]Train epoch: 105 [batch #25, batch_size 16, seq length 571]\tLoss: 0.043536\n","47it [00:00, 131.29it/s]Train epoch: 105 [batch #50, batch_size 16, seq length 709]\tLoss: 0.052979\n","74it [00:00, 116.98it/s]Train epoch: 105 [batch #75, batch_size 16, seq length 806]\tLoss: 0.064192\n","98it [00:00, 105.96it/s]Train epoch: 105 [batch #100, batch_size 16, seq length 892]\tLoss: 0.054247\n","120it [00:01, 106.15it/s]Train epoch: 105 [batch #125, batch_size 16, seq length 978]\tLoss: 0.053163\n","142it [00:01, 98.57it/s]Train epoch: 105 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.065841\n","172it [00:01, 93.81it/s]Train epoch: 105 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.056597\n","200it [00:01, 81.94it/s]Train epoch: 105 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.056395\n","218it [00:02, 82.76it/s]Train epoch: 105 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.058256\n","244it [00:02, 74.07it/s]Train epoch: 105 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.063547\n","268it [00:02, 71.06it/s]Train epoch: 105 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.066091\n","300it [00:03, 67.01it/s]Train epoch: 105 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.067722\n","322it [00:03, 64.38it/s]Train epoch: 105 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.062984\n","350it [00:04, 60.49it/s]Train epoch: 105 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.068404\n","371it [00:04, 59.45it/s]Train epoch: 105 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.071158\n","396it [00:05, 54.75it/s]Train epoch: 105 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.067837\n","420it [00:05, 53.26it/s]Train epoch: 105 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.065849\n","448it [00:06, 46.71it/s]Train epoch: 105 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.065142\n","473it [00:06, 41.13it/s]Train epoch: 105 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.078769\n","499it [00:07, 34.74it/s]Train epoch: 105 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.081396\n","505it [00:07, 65.81it/s]\n","epoch loss: 0.062056718401537085\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 478.79it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3761, 0.5999, 0.4661, 0.5246, 0.8763\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4135, 0.6676, 0.5207, 0.5851, 0.8959\n","rec_at_5: 0.5666\n","prec_at_5: 0.5785\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 106\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 106 [batch #0, batch_size 16, seq length 212]\tLoss: 0.106106\n","16it [00:00, 157.36it/s]Train epoch: 106 [batch #25, batch_size 16, seq length 571]\tLoss: 0.042874\n","47it [00:00, 126.22it/s]Train epoch: 106 [batch #50, batch_size 16, seq length 709]\tLoss: 0.053376\n","73it [00:00, 114.80it/s]Train epoch: 106 [batch #75, batch_size 16, seq length 806]\tLoss: 0.062841\n","97it [00:00, 103.88it/s]Train epoch: 106 [batch #100, batch_size 16, seq length 892]\tLoss: 0.051094\n","119it [00:01, 99.59it/s] Train epoch: 106 [batch #125, batch_size 16, seq length 978]\tLoss: 0.054926\n","141it [00:01, 99.61it/s] Train epoch: 106 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.063431\n","172it [00:01, 88.06it/s]Train epoch: 106 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.053138\n","199it [00:01, 82.30it/s]Train epoch: 106 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.058848\n","217it [00:02, 81.32it/s]Train epoch: 106 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.056503\n","243it [00:02, 78.30it/s]Train epoch: 106 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.061717\n","275it [00:03, 70.57it/s]Train epoch: 106 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.062531\n","299it [00:03, 68.55it/s]Train epoch: 106 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.063987\n","322it [00:03, 64.97it/s]Train epoch: 106 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.064071\n","350it [00:04, 64.32it/s]Train epoch: 106 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.064534\n","371it [00:04, 57.22it/s]Train epoch: 106 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.070801\n","395it [00:05, 54.80it/s]Train epoch: 106 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.067886\n","425it [00:05, 50.38it/s]Train epoch: 106 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.064570\n","447it [00:06, 45.15it/s]Train epoch: 106 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.064008\n","472it [00:06, 42.44it/s]Train epoch: 106 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.079071\n","498it [00:07, 36.96it/s]Train epoch: 106 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.079692\n","505it [00:07, 66.04it/s]\n","epoch loss: 0.0607278444684378\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 478.79it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3766, 0.6028, 0.4659, 0.5256, 0.8760\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4143, 0.6692, 0.5210, 0.5859, 0.8957\n","rec_at_5: 0.5665\n","prec_at_5: 0.5781\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 107\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 107 [batch #0, batch_size 16, seq length 212]\tLoss: 0.094492\n","16it [00:00, 150.88it/s]Train epoch: 107 [batch #25, batch_size 16, seq length 571]\tLoss: 0.042667\n","47it [00:00, 127.59it/s]Train epoch: 107 [batch #50, batch_size 16, seq length 709]\tLoss: 0.051247\n","73it [00:00, 124.03it/s]Train epoch: 107 [batch #75, batch_size 16, seq length 806]\tLoss: 0.061673\n","98it [00:00, 112.38it/s]Train epoch: 107 [batch #100, batch_size 16, seq length 892]\tLoss: 0.050398\n","121it [00:01, 104.53it/s]Train epoch: 107 [batch #125, batch_size 16, seq length 978]\tLoss: 0.052348\n","143it [00:01, 94.74it/s] Train epoch: 107 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.061716\n","173it [00:01, 87.74it/s]Train epoch: 107 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.051749\n","192it [00:01, 85.46it/s]Train epoch: 107 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.053432\n","219it [00:02, 82.61it/s]Train epoch: 107 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.058172\n","244it [00:02, 73.93it/s]Train epoch: 107 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.061031\n","268it [00:02, 69.60it/s]Train epoch: 107 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.063201\n","299it [00:03, 68.54it/s]Train epoch: 107 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.063984\n","321it [00:03, 62.83it/s]Train epoch: 107 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.061049\n","349it [00:04, 63.39it/s]Train epoch: 107 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.065604\n","375it [00:04, 57.54it/s]Train epoch: 107 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.068989\n","399it [00:05, 54.06it/s]Train epoch: 107 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.065763\n","423it [00:05, 48.84it/s]Train epoch: 107 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.060501\n","448it [00:06, 45.38it/s]Train epoch: 107 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.064912\n","473it [00:06, 40.30it/s]Train epoch: 107 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.078757\n","499it [00:07, 35.59it/s]Train epoch: 107 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.074264\n","505it [00:07, 65.56it/s]\n","epoch loss: 0.05970476342752428\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 476.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3743, 0.5980, 0.4645, 0.5229, 0.8756\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4122, 0.6655, 0.5199, 0.5837, 0.8954\n","rec_at_5: 0.5674\n","prec_at_5: 0.5786\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 108\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 108 [batch #0, batch_size 16, seq length 212]\tLoss: 0.103856\n","11it [00:00, 104.19it/s]Train epoch: 108 [batch #25, batch_size 16, seq length 571]\tLoss: 0.041759\n","41it [00:00, 120.90it/s]Train epoch: 108 [batch #50, batch_size 16, seq length 709]\tLoss: 0.048528\n","67it [00:00, 119.58it/s]Train epoch: 108 [batch #75, batch_size 16, seq length 806]\tLoss: 0.058808\n","91it [00:00, 106.39it/s]Train epoch: 108 [batch #100, batch_size 16, seq length 892]\tLoss: 0.050181\n","124it [00:01, 96.36it/s] Train epoch: 108 [batch #125, batch_size 16, seq length 978]\tLoss: 0.052283\n","145it [00:01, 97.63it/s]Train epoch: 108 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.063321\n","175it [00:01, 86.37it/s]Train epoch: 108 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.051585\n","193it [00:01, 87.60it/s]Train epoch: 108 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.054805\n","220it [00:02, 84.01it/s]Train epoch: 108 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.054376\n","245it [00:02, 74.77it/s]Train epoch: 108 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.060299\n","269it [00:02, 71.79it/s]Train epoch: 108 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.058786\n","293it [00:03, 69.05it/s]Train epoch: 108 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.064353\n","324it [00:03, 66.40it/s]Train epoch: 108 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.058239\n","345it [00:04, 59.56it/s]Train epoch: 108 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.063591\n","372it [00:04, 57.73it/s]Train epoch: 108 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.066954\n","396it [00:05, 53.06it/s]Train epoch: 108 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.064141\n","420it [00:05, 50.12it/s]Train epoch: 108 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.061574\n","448it [00:06, 47.50it/s]Train epoch: 108 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.060000\n","473it [00:06, 42.58it/s]Train epoch: 108 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.073925\n","500it [00:07, 33.90it/s]Train epoch: 108 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.074368\n","505it [00:07, 65.58it/s]\n","epoch loss: 0.05829337957029295\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 471.55it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3731, 0.6004, 0.4618, 0.5220, 0.8753\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4107, 0.6680, 0.5160, 0.5822, 0.8944\n","rec_at_5: 0.5642\n","prec_at_5: 0.5772\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 109\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 109 [batch #0, batch_size 16, seq length 212]\tLoss: 0.091497\n","15it [00:00, 147.51it/s]Train epoch: 109 [batch #25, batch_size 16, seq length 571]\tLoss: 0.039644\n","45it [00:00, 125.33it/s]Train epoch: 109 [batch #50, batch_size 16, seq length 709]\tLoss: 0.048526\n","71it [00:00, 122.03it/s]Train epoch: 109 [batch #75, batch_size 16, seq length 806]\tLoss: 0.058978\n","96it [00:00, 109.88it/s]Train epoch: 109 [batch #100, batch_size 16, seq length 892]\tLoss: 0.049856\n","120it [00:01, 100.54it/s]Train epoch: 109 [batch #125, batch_size 16, seq length 978]\tLoss: 0.051491\n","142it [00:01, 96.76it/s] Train epoch: 109 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.060187\n","170it [00:01, 87.01it/s]Train epoch: 109 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.051049\n","197it [00:02, 78.05it/s]Train epoch: 109 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.055129\n","224it [00:02, 74.92it/s]Train epoch: 109 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.052859\n","249it [00:02, 71.92it/s]Train epoch: 109 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.061271\n","273it [00:03, 69.89it/s]Train epoch: 109 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.060445\n","297it [00:03, 66.34it/s]Train epoch: 109 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.061183\n","325it [00:03, 64.10it/s]Train epoch: 109 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.057164\n","346it [00:04, 60.22it/s]Train epoch: 109 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.061427\n","373it [00:04, 57.07it/s]Train epoch: 109 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.066213\n","398it [00:05, 56.61it/s]Train epoch: 109 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.064611\n","422it [00:05, 49.46it/s]Train epoch: 109 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.059594\n","450it [00:06, 45.97it/s]Train epoch: 109 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.060725\n","475it [00:06, 40.51it/s]Train epoch: 109 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.072673\n","498it [00:07, 34.38it/s]Train epoch: 109 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.072684\n","505it [00:07, 65.43it/s]\n","epoch loss: 0.0574792096262226\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 392.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3763, 0.6000, 0.4669, 0.5251, 0.8751\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4135, 0.6663, 0.5215, 0.5851, 0.8948\n","rec_at_5: 0.5649\n","prec_at_5: 0.5767\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 110\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 110 [batch #0, batch_size 16, seq length 212]\tLoss: 0.093223\n","22it [00:00, 96.65it/s] Train epoch: 110 [batch #25, batch_size 16, seq length 571]\tLoss: 0.039568\n","50it [00:00, 80.77it/s]Train epoch: 110 [batch #50, batch_size 16, seq length 709]\tLoss: 0.048761\n","68it [00:00, 74.58it/s]Train epoch: 110 [batch #75, batch_size 16, seq length 806]\tLoss: 0.058035\n","97it [00:01, 63.33it/s]Train epoch: 110 [batch #100, batch_size 16, seq length 892]\tLoss: 0.049807\n","121it [00:01, 71.49it/s]Train epoch: 110 [batch #125, batch_size 16, seq length 978]\tLoss: 0.049841\n","150it [00:01, 82.14it/s]Train epoch: 110 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.058686\n","168it [00:02, 81.95it/s]Train epoch: 110 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.048771\n","195it [00:02, 80.44it/s]Train epoch: 110 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.052230\n","222it [00:02, 76.54it/s]Train epoch: 110 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.053959\n","248it [00:03, 75.50it/s]Train epoch: 110 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.057799\n","272it [00:03, 68.86it/s]Train epoch: 110 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.060328\n","296it [00:03, 69.27it/s]Train epoch: 110 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.059049\n","324it [00:04, 59.54it/s]Train epoch: 110 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.056890\n","345it [00:04, 59.81it/s]Train epoch: 110 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.059934\n","372it [00:05, 58.01it/s]Train epoch: 110 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.064669\n","398it [00:05, 55.72it/s]Train epoch: 110 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.061238\n","422it [00:06, 51.29it/s]Train epoch: 110 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.061062\n","449it [00:06, 46.91it/s]Train epoch: 110 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.060781\n","474it [00:07, 40.33it/s]Train epoch: 110 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.071021\n","500it [00:08, 34.49it/s]Train epoch: 110 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.072697\n","505it [00:08, 60.61it/s]\n","epoch loss: 0.05602950889727857\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 476.00it/s]\n","Finish save rediction by checkpoint  110\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3749, 0.5989, 0.4647, 0.5234, 0.8748\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4121, 0.6663, 0.5192, 0.5836, 0.8942\n","rec_at_5: 0.5652\n","prec_at_5: 0.5771\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 111\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 111 [batch #0, batch_size 16, seq length 212]\tLoss: 0.093559\n","16it [00:00, 157.90it/s]Train epoch: 111 [batch #25, batch_size 16, seq length 571]\tLoss: 0.038834\n","47it [00:00, 126.49it/s]Train epoch: 111 [batch #50, batch_size 16, seq length 709]\tLoss: 0.049079\n","73it [00:00, 119.52it/s]Train epoch: 111 [batch #75, batch_size 16, seq length 806]\tLoss: 0.057221\n","98it [00:00, 111.37it/s]Train epoch: 111 [batch #100, batch_size 16, seq length 892]\tLoss: 0.049929\n","121it [00:01, 99.56it/s] Train epoch: 111 [batch #125, batch_size 16, seq length 978]\tLoss: 0.047433\n","142it [00:01, 91.62it/s]Train epoch: 111 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.058911\n","172it [00:01, 85.21it/s]Train epoch: 111 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.049641\n","199it [00:01, 84.46it/s]Train epoch: 111 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.052991\n","217it [00:02, 80.40it/s]Train epoch: 111 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.055989\n","244it [00:02, 77.75it/s]Train epoch: 111 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.056093\n","268it [00:02, 72.43it/s]Train epoch: 111 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.058364\n","300it [00:03, 65.60it/s]Train epoch: 111 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.058782\n","322it [00:03, 64.58it/s]Train epoch: 111 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.055933\n","350it [00:04, 60.42it/s]Train epoch: 111 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.060883\n","371it [00:04, 59.54it/s]Train epoch: 111 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.061451\n","396it [00:05, 52.82it/s]Train epoch: 111 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.058685\n","420it [00:05, 50.23it/s]Train epoch: 111 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.056352\n","448it [00:06, 45.76it/s]Train epoch: 111 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.058722\n","473it [00:06, 43.22it/s]Train epoch: 111 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.071281\n","500it [00:07, 34.60it/s]Train epoch: 111 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.071495\n","505it [00:07, 65.67it/s]\n","epoch loss: 0.05506330358399318\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 480.83it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3729, 0.5952, 0.4638, 0.5213, 0.8743\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4099, 0.6633, 0.5176, 0.5815, 0.8935\n","rec_at_5: 0.5653\n","prec_at_5: 0.5767\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 112\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 112 [batch #0, batch_size 16, seq length 212]\tLoss: 0.082560\n","16it [00:00, 156.04it/s]Train epoch: 112 [batch #25, batch_size 16, seq length 571]\tLoss: 0.038931\n","48it [00:00, 128.09it/s]Train epoch: 112 [batch #50, batch_size 16, seq length 709]\tLoss: 0.048474\n","75it [00:00, 120.97it/s]Train epoch: 112 [batch #75, batch_size 16, seq length 806]\tLoss: 0.055315\n","100it [00:00, 110.84it/s]Train epoch: 112 [batch #100, batch_size 16, seq length 892]\tLoss: 0.047054\n","123it [00:01, 100.09it/s]Train epoch: 112 [batch #125, batch_size 16, seq length 978]\tLoss: 0.050682\n","145it [00:01, 100.18it/s]Train epoch: 112 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.055696\n","166it [00:01, 93.13it/s]Train epoch: 112 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.050460\n","194it [00:01, 87.56it/s]Train epoch: 112 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.048767\n","221it [00:02, 84.38it/s]Train epoch: 112 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.052774\n","247it [00:02, 76.95it/s]Train epoch: 112 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.057541\n","271it [00:02, 69.69it/s]Train epoch: 112 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.056958\n","295it [00:03, 67.66it/s]Train epoch: 112 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.058653\n","324it [00:03, 64.05it/s]Train epoch: 112 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.056884\n","345it [00:04, 62.76it/s]Train epoch: 112 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.058431\n","373it [00:04, 59.57it/s]Train epoch: 112 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.060514\n","397it [00:05, 54.64it/s]Train epoch: 112 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.059592\n","421it [00:05, 50.64it/s]Train epoch: 112 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.056495\n","449it [00:06, 45.64it/s]Train epoch: 112 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.056645\n","474it [00:06, 40.20it/s]Train epoch: 112 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.065563\n","497it [00:07, 35.96it/s]Train epoch: 112 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.065781\n","505it [00:07, 66.23it/s]\n","epoch loss: 0.05392566517106082\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 484.66it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3734, 0.5955, 0.4645, 0.5219, 0.8741\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4107, 0.6630, 0.5191, 0.5823, 0.8934\n","rec_at_5: 0.5650\n","prec_at_5: 0.5755\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 113\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 113 [batch #0, batch_size 16, seq length 212]\tLoss: 0.084000\n","16it [00:00, 154.56it/s]Train epoch: 113 [batch #25, batch_size 16, seq length 571]\tLoss: 0.036486\n","47it [00:00, 131.41it/s]Train epoch: 113 [batch #50, batch_size 16, seq length 709]\tLoss: 0.046644\n","74it [00:00, 122.18it/s]Train epoch: 113 [batch #75, batch_size 16, seq length 806]\tLoss: 0.057107\n","99it [00:00, 110.77it/s]Train epoch: 113 [batch #100, batch_size 16, seq length 892]\tLoss: 0.045473\n","122it [00:01, 99.90it/s]Train epoch: 113 [batch #125, batch_size 16, seq length 978]\tLoss: 0.047804\n","143it [00:01, 91.42it/s]Train epoch: 113 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.054762\n","173it [00:01, 86.84it/s]Train epoch: 113 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.047182\n","200it [00:02, 82.61it/s]Train epoch: 113 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.049913\n","218it [00:02, 80.27it/s]Train epoch: 113 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.049070\n","244it [00:02, 59.78it/s]Train epoch: 113 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.055328\n","272it [00:03, 38.65it/s]Train epoch: 113 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.054599\n","297it [00:04, 37.23it/s]Train epoch: 113 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.057372\n","322it [00:04, 30.06it/s]Train epoch: 113 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.054834\n","349it [00:05, 32.42it/s]Train epoch: 113 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.056337\n","373it [00:06, 33.98it/s]Train epoch: 113 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.061800\n","398it [00:07, 26.58it/s]Train epoch: 113 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.059274\n","424it [00:08, 30.42it/s]Train epoch: 113 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.056486\n","448it [00:09, 24.28it/s]Train epoch: 113 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.056673\n","473it [00:10, 29.22it/s]Train epoch: 113 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.065833\n","499it [00:10, 33.93it/s]Train epoch: 113 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.067091\n","505it [00:11, 45.45it/s]\n","epoch loss: 0.05288865455601475\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 480.60it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3743, 0.6170, 0.4653, 0.5306, 0.8739\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4110, 0.6624, 0.5199, 0.5825, 0.8935\n","rec_at_5: 0.5630\n","prec_at_5: 0.5751\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 114\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 114 [batch #0, batch_size 16, seq length 212]\tLoss: 0.083149\n","16it [00:00, 157.16it/s]Train epoch: 114 [batch #25, batch_size 16, seq length 571]\tLoss: 0.036783\n","47it [00:00, 127.92it/s]Train epoch: 114 [batch #50, batch_size 16, seq length 709]\tLoss: 0.045944\n","73it [00:00, 123.66it/s]Train epoch: 114 [batch #75, batch_size 16, seq length 806]\tLoss: 0.053720\n","98it [00:00, 110.51it/s]Train epoch: 114 [batch #100, batch_size 16, seq length 892]\tLoss: 0.047515\n","121it [00:01, 101.79it/s]Train epoch: 114 [batch #125, batch_size 16, seq length 978]\tLoss: 0.044381\n","143it [00:01, 93.03it/s]Train epoch: 114 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.053275\n","173it [00:01, 90.38it/s]Train epoch: 114 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.045116\n","192it [00:01, 85.31it/s]Train epoch: 114 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.047387\n","219it [00:02, 76.34it/s]Train epoch: 114 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.050525\n","244it [00:02, 75.44it/s]Train epoch: 114 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.054478\n","268it [00:02, 71.82it/s]Train epoch: 114 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.054608\n","300it [00:03, 68.63it/s]Train epoch: 114 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.055686\n","321it [00:03, 62.23it/s]Train epoch: 114 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.052485\n","349it [00:04, 60.81it/s]Train epoch: 114 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.055725\n","370it [00:04, 59.38it/s]Train epoch: 114 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.058354\n","400it [00:05, 52.92it/s]Train epoch: 114 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.056501\n","424it [00:05, 50.72it/s]Train epoch: 114 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.053762\n","446it [00:06, 48.31it/s]Train epoch: 114 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.055016\n","471it [00:06, 42.34it/s]Train epoch: 114 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.066627\n","499it [00:07, 35.25it/s]Train epoch: 114 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.065810\n","505it [00:07, 65.54it/s]\n","epoch loss: 0.051877490023501435\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 471.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3717, 0.6066, 0.4615, 0.5242, 0.8736\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4089, 0.6620, 0.5168, 0.5804, 0.8929\n","rec_at_5: 0.5637\n","prec_at_5: 0.5747\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 115\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 115 [batch #0, batch_size 16, seq length 212]\tLoss: 0.084063\n","16it [00:00, 156.23it/s]Train epoch: 115 [batch #25, batch_size 16, seq length 571]\tLoss: 0.033703\n","47it [00:00, 128.89it/s]Train epoch: 115 [batch #50, batch_size 16, seq length 709]\tLoss: 0.044749\n","74it [00:00, 118.72it/s]Train epoch: 115 [batch #75, batch_size 16, seq length 806]\tLoss: 0.053272\n","98it [00:00, 108.23it/s]Train epoch: 115 [batch #100, batch_size 16, seq length 892]\tLoss: 0.046053\n","120it [00:01, 106.64it/s]Train epoch: 115 [batch #125, batch_size 16, seq length 978]\tLoss: 0.046796\n","141it [00:01, 95.63it/s]Train epoch: 115 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.053231\n","171it [00:01, 90.63it/s]Train epoch: 115 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.044334\n","199it [00:01, 84.65it/s]Train epoch: 115 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.048604\n","217it [00:02, 81.25it/s]Train epoch: 115 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.048146\n","243it [00:02, 78.01it/s]Train epoch: 115 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.050829\n","275it [00:03, 68.25it/s]Train epoch: 115 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.050818\n","299it [00:03, 65.72it/s]Train epoch: 115 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.054027\n","320it [00:03, 65.17it/s]Train epoch: 115 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.052289\n","348it [00:04, 63.20it/s]Train epoch: 115 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.053807\n","369it [00:04, 61.15it/s]Train epoch: 115 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.058011\n","400it [00:05, 51.83it/s]Train epoch: 115 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.055378\n","424it [00:05, 51.63it/s]Train epoch: 115 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.054107\n","446it [00:06, 46.05it/s]Train epoch: 115 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.051248\n","471it [00:06, 42.75it/s]Train epoch: 115 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.062283\n","498it [00:07, 35.70it/s]Train epoch: 115 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.061831\n","505it [00:07, 65.60it/s]\n","epoch loss: 0.050363116716910704\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 484.44it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3722, 0.6058, 0.4633, 0.5250, 0.8734\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4082, 0.6607, 0.5164, 0.5797, 0.8928\n","rec_at_5: 0.5629\n","prec_at_5: 0.5746\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 116\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 116 [batch #0, batch_size 16, seq length 212]\tLoss: 0.084836\n","16it [00:00, 152.13it/s]Train epoch: 116 [batch #25, batch_size 16, seq length 571]\tLoss: 0.035680\n","47it [00:00, 125.64it/s]Train epoch: 116 [batch #50, batch_size 16, seq length 709]\tLoss: 0.044433\n","73it [00:00, 115.64it/s]Train epoch: 116 [batch #75, batch_size 16, seq length 806]\tLoss: 0.051480\n","97it [00:00, 102.04it/s]Train epoch: 116 [batch #100, batch_size 16, seq length 892]\tLoss: 0.043920\n","119it [00:01, 95.92it/s] Train epoch: 116 [batch #125, batch_size 16, seq length 978]\tLoss: 0.045081\n","149it [00:01, 88.23it/s]Train epoch: 116 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.052920\n","167it [00:01, 86.68it/s]Train epoch: 116 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.043196\n","194it [00:02, 78.67it/s]Train epoch: 116 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.049400\n","221it [00:02, 77.84it/s]Train epoch: 116 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.048610\n","247it [00:02, 71.50it/s]Train epoch: 116 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.051216\n","271it [00:03, 68.58it/s]Train epoch: 116 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.053103\n","295it [00:03, 67.40it/s]Train epoch: 116 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.050813\n","324it [00:03, 61.51it/s]Train epoch: 116 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.050202\n","345it [00:04, 61.13it/s]Train epoch: 116 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.054123\n","373it [00:04, 60.35it/s]Train epoch: 116 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.057492\n","398it [00:05, 53.42it/s]Train epoch: 116 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.050588\n","422it [00:05, 53.11it/s]Train epoch: 116 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.052395\n","448it [00:06, 46.31it/s]Train epoch: 116 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.054210\n","473it [00:06, 43.03it/s]Train epoch: 116 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.061798\n","498it [00:07, 34.09it/s]Train epoch: 116 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.061865\n","505it [00:07, 64.56it/s]\n","epoch loss: 0.049784396568516104\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 471.42it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3740, 0.6074, 0.4649, 0.5267, 0.8732\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4105, 0.6629, 0.5188, 0.5821, 0.8923\n","rec_at_5: 0.5620\n","prec_at_5: 0.5737\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 117\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 117 [batch #0, batch_size 16, seq length 212]\tLoss: 0.084496\n","16it [00:00, 156.13it/s]Train epoch: 117 [batch #25, batch_size 16, seq length 571]\tLoss: 0.035065\n","47it [00:00, 127.33it/s]Train epoch: 117 [batch #50, batch_size 16, seq length 709]\tLoss: 0.041918\n","72it [00:00, 114.54it/s]Train epoch: 117 [batch #75, batch_size 16, seq length 806]\tLoss: 0.051687\n","96it [00:00, 106.28it/s]Train epoch: 117 [batch #100, batch_size 16, seq length 892]\tLoss: 0.045848\n","118it [00:01, 101.13it/s]Train epoch: 117 [batch #125, batch_size 16, seq length 978]\tLoss: 0.043360\n","150it [00:01, 93.30it/s]Train epoch: 117 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.050735\n","170it [00:01, 87.93it/s]Train epoch: 117 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.042945\n","198it [00:01, 84.93it/s]Train epoch: 117 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.045976\n","225it [00:02, 80.25it/s]Train epoch: 117 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.047216\n","243it [00:02, 74.47it/s]Train epoch: 117 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.049838\n","275it [00:03, 72.82it/s]Train epoch: 117 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.049751\n","299it [00:03, 69.72it/s]Train epoch: 117 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.053320\n","321it [00:03, 64.86it/s]Train epoch: 117 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.048692\n","349it [00:04, 61.10it/s]Train epoch: 117 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.052718\n","375it [00:04, 57.91it/s]Train epoch: 117 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.054902\n","399it [00:05, 52.73it/s]Train epoch: 117 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.052069\n","423it [00:05, 50.20it/s]Train epoch: 117 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.051325\n","449it [00:06, 43.76it/s]Train epoch: 117 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.052622\n","474it [00:06, 42.49it/s]Train epoch: 117 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.059228\n","497it [00:07, 36.87it/s]Train epoch: 117 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.059203\n","505it [00:07, 65.13it/s]\n","epoch loss: 0.04855728542782588\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 477.32it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3721, 0.6043, 0.4625, 0.5239, 0.8728\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4095, 0.6609, 0.5184, 0.5810, 0.8920\n","rec_at_5: 0.5623\n","prec_at_5: 0.5732\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 118\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 118 [batch #0, batch_size 16, seq length 212]\tLoss: 0.079014\n","15it [00:00, 147.92it/s]Train epoch: 118 [batch #25, batch_size 16, seq length 571]\tLoss: 0.034815\n","45it [00:00, 125.05it/s]Train epoch: 118 [batch #50, batch_size 16, seq length 709]\tLoss: 0.039684\n","71it [00:00, 123.95it/s]Train epoch: 118 [batch #75, batch_size 16, seq length 806]\tLoss: 0.049254\n","96it [00:00, 110.60it/s]Train epoch: 118 [batch #100, batch_size 16, seq length 892]\tLoss: 0.044119\n","120it [00:01, 101.38it/s]Train epoch: 118 [batch #125, batch_size 16, seq length 978]\tLoss: 0.044398\n","142it [00:01, 98.71it/s] Train epoch: 118 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.048664\n","172it [00:01, 92.60it/s]Train epoch: 118 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.041879\n","200it [00:01, 82.86it/s]Train epoch: 118 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.044842\n","218it [00:02, 82.15it/s]Train epoch: 118 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.045398\n","244it [00:02, 74.31it/s]Train epoch: 118 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.046863\n","268it [00:02, 69.56it/s]Train epoch: 118 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.049814\n","299it [00:03, 66.75it/s]Train epoch: 118 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.052334\n","320it [00:03, 61.49it/s]Train epoch: 118 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.050561\n","348it [00:04, 62.40it/s]Train epoch: 118 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.051611\n","375it [00:04, 58.02it/s]Train epoch: 118 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.055712\n","400it [00:05, 54.62it/s]Train epoch: 118 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.049876\n","424it [00:05, 52.21it/s]Train epoch: 118 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.045167\n","446it [00:06, 45.70it/s]Train epoch: 118 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.049877\n","471it [00:06, 41.17it/s]Train epoch: 118 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.060344\n","498it [00:07, 35.38it/s]Train epoch: 118 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.060964\n","505it [00:07, 65.59it/s]\n","epoch loss: 0.04783348482526322\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 472.98it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3738, 0.6054, 0.4651, 0.5261, 0.8722\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4103, 0.6614, 0.5193, 0.5818, 0.8918\n","rec_at_5: 0.5618\n","prec_at_5: 0.5728\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 119\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 119 [batch #0, batch_size 16, seq length 212]\tLoss: 0.083458\n","15it [00:00, 145.46it/s]Train epoch: 119 [batch #25, batch_size 16, seq length 571]\tLoss: 0.034126\n","45it [00:00, 122.62it/s]Train epoch: 119 [batch #50, batch_size 16, seq length 709]\tLoss: 0.042448\n","71it [00:00, 111.06it/s]Train epoch: 119 [batch #75, batch_size 16, seq length 806]\tLoss: 0.049106\n","95it [00:00, 105.11it/s]Train epoch: 119 [batch #100, batch_size 16, seq length 892]\tLoss: 0.040937\n","117it [00:01, 104.88it/s]Train epoch: 119 [batch #125, batch_size 16, seq length 978]\tLoss: 0.041602\n","148it [00:01, 91.37it/s]Train epoch: 119 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.049868\n","168it [00:01, 86.19it/s]Train epoch: 119 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.042602\n","196it [00:01, 80.74it/s]Train epoch: 119 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.045244\n","223it [00:02, 80.12it/s]Train epoch: 119 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.047529\n","248it [00:02, 73.15it/s]Train epoch: 119 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.050057\n","272it [00:03, 68.45it/s]Train epoch: 119 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.049770\n","295it [00:03, 65.33it/s]Train epoch: 119 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.049374\n","325it [00:03, 65.20it/s]Train epoch: 119 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.047945\n","346it [00:04, 62.96it/s]Train epoch: 119 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.050508\n","373it [00:04, 57.58it/s]Train epoch: 119 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.052057\n","399it [00:05, 52.90it/s]Train epoch: 119 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.049587\n","423it [00:05, 52.69it/s]Train epoch: 119 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.045191\n","450it [00:06, 46.62it/s]Train epoch: 119 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.047414\n","475it [00:06, 42.13it/s]Train epoch: 119 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.055133\n","498it [00:07, 36.53it/s]Train epoch: 119 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.059212\n","505it [00:07, 65.43it/s]\n","epoch loss: 0.04701321188563315\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.75it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3735, 0.6027, 0.4658, 0.5255, 0.8722\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4092, 0.6590, 0.5191, 0.5807, 0.8914\n","rec_at_5: 0.5610\n","prec_at_5: 0.5719\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 120\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 120 [batch #0, batch_size 16, seq length 212]\tLoss: 0.069761\n","16it [00:00, 154.57it/s]Train epoch: 120 [batch #25, batch_size 16, seq length 571]\tLoss: 0.032370\n","47it [00:00, 126.04it/s]Train epoch: 120 [batch #50, batch_size 16, seq length 709]\tLoss: 0.039531\n","73it [00:00, 116.63it/s]Train epoch: 120 [batch #75, batch_size 16, seq length 806]\tLoss: 0.049294\n","97it [00:00, 107.98it/s]Train epoch: 120 [batch #100, batch_size 16, seq length 892]\tLoss: 0.040956\n","119it [00:01, 102.40it/s]Train epoch: 120 [batch #125, batch_size 16, seq length 978]\tLoss: 0.042740\n","150it [00:01, 86.93it/s]Train epoch: 120 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.048597\n","170it [00:01, 89.15it/s]Train epoch: 120 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040546\n","198it [00:02, 79.43it/s]Train epoch: 120 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.045398\n","225it [00:02, 76.12it/s]Train epoch: 120 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.042336\n","243it [00:02, 78.75it/s]Train epoch: 120 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.049595\n","275it [00:03, 69.24it/s]Train epoch: 120 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.048461\n","299it [00:03, 66.36it/s]Train epoch: 120 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.049209\n","322it [00:03, 64.76it/s]Train epoch: 120 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.047946\n","350it [00:04, 61.83it/s]Train epoch: 120 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.050450\n","371it [00:04, 60.14it/s]Train epoch: 120 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.051592\n","397it [00:05, 56.99it/s]Train epoch: 120 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.048551\n","421it [00:05, 51.90it/s]Train epoch: 120 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.047227\n","449it [00:06, 44.57it/s]Train epoch: 120 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.048890\n","474it [00:06, 40.73it/s]Train epoch: 120 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.055005\n","500it [00:07, 34.68it/s]Train epoch: 120 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.052795\n","505it [00:07, 66.03it/s]\n","epoch loss: 0.04565554047280019\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 470.55it/s]\n","Finish save rediction by checkpoint  120\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3720, 0.6007, 0.4635, 0.5233, 0.8719\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4087, 0.6578, 0.5190, 0.5802, 0.8914\n","rec_at_5: 0.5605\n","prec_at_5: 0.5714\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 121\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 121 [batch #0, batch_size 16, seq length 212]\tLoss: 0.085643\n","16it [00:00, 155.99it/s]Train epoch: 121 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030938\n","47it [00:00, 128.60it/s]Train epoch: 121 [batch #50, batch_size 16, seq length 709]\tLoss: 0.039526\n","74it [00:00, 120.87it/s]Train epoch: 121 [batch #75, batch_size 16, seq length 806]\tLoss: 0.046338\n","99it [00:00, 100.55it/s]Train epoch: 121 [batch #100, batch_size 16, seq length 892]\tLoss: 0.041140\n","121it [00:01, 103.95it/s]Train epoch: 121 [batch #125, batch_size 16, seq length 978]\tLoss: 0.041456\n","143it [00:01, 97.74it/s]Train epoch: 121 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.049708\n","173it [00:01, 91.57it/s]Train epoch: 121 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.038894\n","193it [00:01, 85.43it/s]Train epoch: 121 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.043773\n","220it [00:02, 78.14it/s]Train epoch: 121 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.044524\n","244it [00:02, 75.08it/s]Train epoch: 121 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.046231\n","268it [00:02, 70.91it/s]Train epoch: 121 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.048559\n","300it [00:03, 66.52it/s]Train epoch: 121 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.048537\n","322it [00:03, 61.34it/s]Train epoch: 121 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.046363\n","350it [00:04, 61.98it/s]Train epoch: 121 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.048494\n","375it [00:04, 54.93it/s]Train epoch: 121 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.051290\n","399it [00:05, 54.12it/s]Train epoch: 121 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.047650\n","423it [00:05, 51.13it/s]Train epoch: 121 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.045038\n","450it [00:06, 45.55it/s]Train epoch: 121 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.046099\n","475it [00:06, 41.96it/s]Train epoch: 121 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.052748\n","498it [00:07, 36.53it/s]Train epoch: 121 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.054801\n","505it [00:07, 65.42it/s]\n","epoch loss: 0.044928393108161664\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 470.52it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3717, 0.5994, 0.4639, 0.5230, 0.8715\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4083, 0.6563, 0.5193, 0.5798, 0.8910\n","rec_at_5: 0.5611\n","prec_at_5: 0.5709\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 122\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 122 [batch #0, batch_size 16, seq length 212]\tLoss: 0.078664\n","16it [00:00, 156.33it/s]Train epoch: 122 [batch #25, batch_size 16, seq length 571]\tLoss: 0.032257\n","46it [00:00, 125.47it/s]Train epoch: 122 [batch #50, batch_size 16, seq length 709]\tLoss: 0.038575\n","72it [00:00, 111.51it/s]Train epoch: 122 [batch #75, batch_size 16, seq length 806]\tLoss: 0.047557\n","96it [00:00, 105.70it/s]Train epoch: 122 [batch #100, batch_size 16, seq length 892]\tLoss: 0.040599\n","118it [00:01, 106.01it/s]Train epoch: 122 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039252\n","150it [00:01, 92.57it/s]Train epoch: 122 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.047698\n","170it [00:01, 84.88it/s]Train epoch: 122 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.039386\n","198it [00:01, 81.97it/s]Train epoch: 122 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.040859\n","225it [00:02, 78.80it/s]Train epoch: 122 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.041902\n","249it [00:02, 73.60it/s]Train epoch: 122 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.046008\n","273it [00:03, 72.57it/s]Train epoch: 122 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.046510\n","297it [00:03, 67.01it/s]Train epoch: 122 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.046443\n","325it [00:03, 64.20it/s]Train epoch: 122 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.043674\n","346it [00:04, 62.62it/s]Train epoch: 122 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.047577\n","374it [00:04, 57.90it/s]Train epoch: 122 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.050841\n","399it [00:05, 53.99it/s]Train epoch: 122 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.045755\n","423it [00:05, 46.75it/s]Train epoch: 122 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.045182\n","448it [00:06, 43.61it/s]Train epoch: 122 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.045973\n","473it [00:06, 42.37it/s]Train epoch: 122 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.050519\n","499it [00:07, 34.47it/s]Train epoch: 122 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.052859\n","505it [00:07, 64.88it/s]\n","epoch loss: 0.04369721633686435\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 485.47it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3729, 0.5972, 0.4652, 0.5230, 0.8712\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4086, 0.6574, 0.5192, 0.5802, 0.8907\n","rec_at_5: 0.5598\n","prec_at_5: 0.5699\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 123\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 123 [batch #0, batch_size 16, seq length 212]\tLoss: 0.070366\n","17it [00:00, 159.98it/s]Train epoch: 123 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030773\n","49it [00:00, 128.09it/s]Train epoch: 123 [batch #50, batch_size 16, seq length 709]\tLoss: 0.039451\n","63it [00:00, 124.00it/s]Train epoch: 123 [batch #75, batch_size 16, seq length 806]\tLoss: 0.046066\n","89it [00:00, 112.18it/s]Train epoch: 123 [batch #100, batch_size 16, seq length 892]\tLoss: 0.039727\n","124it [00:01, 99.93it/s] Train epoch: 123 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039392\n","146it [00:01, 99.16it/s]Train epoch: 123 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.047192\n","166it [00:01, 90.62it/s]Train epoch: 123 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040369\n","195it [00:01, 84.91it/s]Train epoch: 123 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.042504\n","222it [00:02, 80.57it/s]Train epoch: 123 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.040187\n","248it [00:02, 72.51it/s]Train epoch: 123 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.047210\n","272it [00:02, 68.77it/s]Train epoch: 123 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.044070\n","296it [00:03, 68.59it/s]Train epoch: 123 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.046014\n","324it [00:03, 61.95it/s]Train epoch: 123 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.043293\n","345it [00:04, 62.08it/s]Train epoch: 123 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.046693\n","373it [00:04, 59.65it/s]Train epoch: 123 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.047783\n","399it [00:05, 54.20it/s]Train epoch: 123 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.044811\n","423it [00:05, 49.81it/s]Train epoch: 123 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.043088\n","450it [00:06, 45.72it/s]Train epoch: 123 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.044943\n","475it [00:06, 40.30it/s]Train epoch: 123 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.052802\n","497it [00:07, 35.53it/s]Train epoch: 123 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.052186\n","505it [00:07, 65.62it/s]\n","epoch loss: 0.043158146360022305\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 480.23it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3725, 0.5985, 0.4642, 0.5229, 0.8710\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4086, 0.6591, 0.5180, 0.5801, 0.8905\n","rec_at_5: 0.5599\n","prec_at_5: 0.5705\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 124\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 124 [batch #0, batch_size 16, seq length 212]\tLoss: 0.069620\n","17it [00:00, 160.71it/s]Train epoch: 124 [batch #25, batch_size 16, seq length 571]\tLoss: 0.031245\n","49it [00:00, 124.94it/s]Train epoch: 124 [batch #50, batch_size 16, seq length 709]\tLoss: 0.037747\n","75it [00:00, 113.17it/s]Train epoch: 124 [batch #75, batch_size 16, seq length 806]\tLoss: 0.043050\n","99it [00:00, 111.29it/s]Train epoch: 124 [batch #100, batch_size 16, seq length 892]\tLoss: 0.037636\n","122it [00:01, 103.34it/s]Train epoch: 124 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039982\n","143it [00:01, 93.88it/s]Train epoch: 124 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.043850\n","173it [00:01, 91.02it/s]Train epoch: 124 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.035406\n","192it [00:01, 85.35it/s]Train epoch: 124 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.042396\n","219it [00:02, 82.90it/s]Train epoch: 124 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.039534\n","245it [00:02, 73.08it/s]Train epoch: 124 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.043048\n","269it [00:02, 72.04it/s]Train epoch: 124 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.046626\n","293it [00:03, 66.05it/s]Train epoch: 124 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.044094\n","322it [00:03, 64.30it/s]Train epoch: 124 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.042612\n","350it [00:04, 59.93it/s]Train epoch: 124 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.044930\n","371it [00:04, 57.55it/s]Train epoch: 124 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.048448\n","396it [00:05, 57.29it/s]Train epoch: 124 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.044833\n","420it [00:05, 51.16it/s]Train epoch: 124 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.045519\n","447it [00:06, 46.71it/s]Train epoch: 124 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.043270\n","472it [00:06, 40.11it/s]Train epoch: 124 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.049960\n","499it [00:07, 34.05it/s]Train epoch: 124 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.049751\n","505it [00:07, 65.61it/s]\n","epoch loss: 0.04195823756479981\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 467.11it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3711, 0.6013, 0.4620, 0.5225, 0.8708\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4077, 0.6581, 0.5173, 0.5793, 0.8903\n","rec_at_5: 0.5562\n","prec_at_5: 0.5687\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 125\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 125 [batch #0, batch_size 16, seq length 212]\tLoss: 0.069792\n","15it [00:00, 144.51it/s]Train epoch: 125 [batch #25, batch_size 16, seq length 571]\tLoss: 0.028081\n","45it [00:00, 124.45it/s]Train epoch: 125 [batch #50, batch_size 16, seq length 709]\tLoss: 0.036193\n","70it [00:00, 115.70it/s]Train epoch: 125 [batch #75, batch_size 16, seq length 806]\tLoss: 0.042962\n","94it [00:00, 105.10it/s]Train epoch: 125 [batch #100, batch_size 16, seq length 892]\tLoss: 0.037032\n","116it [00:01, 96.46it/s] Train epoch: 125 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039481\n","146it [00:01, 94.52it/s]Train epoch: 125 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.045790\n","166it [00:01, 89.55it/s]Train epoch: 125 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.037121\n","194it [00:01, 85.20it/s]Train epoch: 125 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.038916\n","221it [00:02, 75.64it/s]Train epoch: 125 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.040050\n","247it [00:02, 75.09it/s]Train epoch: 125 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.043380\n","271it [00:03, 68.45it/s]Train epoch: 125 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.043027\n","300it [00:03, 66.80it/s]Train epoch: 125 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.045667\n","322it [00:03, 65.13it/s]Train epoch: 125 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.039723\n","350it [00:04, 59.68it/s]Train epoch: 125 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.045218\n","369it [00:04, 55.95it/s]Train epoch: 125 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.045803\n","400it [00:05, 50.59it/s]Train epoch: 125 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.042593\n","424it [00:05, 51.14it/s]Train epoch: 125 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.041782\n","450it [00:06, 44.87it/s]Train epoch: 125 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.040935\n","475it [00:06, 40.60it/s]Train epoch: 125 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.049685\n","497it [00:07, 34.75it/s]Train epoch: 125 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.049742\n","505it [00:07, 65.05it/s]\n","epoch loss: 0.04114059504435057\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 482.44it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3698, 0.5980, 0.4605, 0.5203, 0.8704\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4059, 0.6554, 0.5161, 0.5775, 0.8899\n","rec_at_5: 0.5579\n","prec_at_5: 0.5696\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 126\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 126 [batch #0, batch_size 16, seq length 212]\tLoss: 0.069396\n","16it [00:00, 159.45it/s]Train epoch: 126 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030109\n","48it [00:00, 129.77it/s]Train epoch: 126 [batch #50, batch_size 16, seq length 709]\tLoss: 0.035615\n","75it [00:00, 123.60it/s]Train epoch: 126 [batch #75, batch_size 16, seq length 806]\tLoss: 0.044504\n","100it [00:00, 110.12it/s]Train epoch: 126 [batch #100, batch_size 16, seq length 892]\tLoss: 0.038231\n","123it [00:01, 99.19it/s] Train epoch: 126 [batch #125, batch_size 16, seq length 978]\tLoss: 0.036860\n","145it [00:01, 99.07it/s]Train epoch: 126 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.042586\n","175it [00:01, 91.45it/s]Train epoch: 126 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.037148\n","194it [00:01, 84.88it/s]Train epoch: 126 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.039919\n","221it [00:02, 81.50it/s]Train epoch: 126 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.040022\n","246it [00:02, 73.72it/s]Train epoch: 126 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.041481\n","270it [00:02, 69.56it/s]Train epoch: 126 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.042919\n","300it [00:03, 63.63it/s]Train epoch: 126 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.043773\n","321it [00:03, 63.91it/s]Train epoch: 126 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.042579\n","350it [00:04, 63.42it/s]Train epoch: 126 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.042794\n","371it [00:04, 61.38it/s]Train epoch: 126 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.046181\n","396it [00:05, 54.13it/s]Train epoch: 126 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.043117\n","420it [00:05, 50.51it/s]Train epoch: 126 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.039929\n","448it [00:06, 45.63it/s]Train epoch: 126 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.039799\n","473it [00:06, 42.65it/s]Train epoch: 126 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.046799\n","500it [00:07, 33.18it/s]Train epoch: 126 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.048011\n","505it [00:07, 65.38it/s]\n","epoch loss: 0.04032048545962218\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 480.68it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3704, 0.5981, 0.4615, 0.5210, 0.8703\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4056, 0.6559, 0.5152, 0.5771, 0.8896\n","rec_at_5: 0.5579\n","prec_at_5: 0.5700\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 127\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 127 [batch #0, batch_size 16, seq length 212]\tLoss: 0.066356\n","16it [00:00, 154.27it/s]Train epoch: 127 [batch #25, batch_size 16, seq length 571]\tLoss: 0.028150\n","47it [00:00, 127.20it/s]Train epoch: 127 [batch #50, batch_size 16, seq length 709]\tLoss: 0.037429\n","73it [00:00, 114.86it/s]Train epoch: 127 [batch #75, batch_size 16, seq length 806]\tLoss: 0.042278\n","97it [00:00, 104.11it/s]Train epoch: 127 [batch #100, batch_size 16, seq length 892]\tLoss: 0.036705\n","119it [00:01, 104.24it/s]Train epoch: 127 [batch #125, batch_size 16, seq length 978]\tLoss: 0.035562\n","150it [00:01, 88.08it/s]Train epoch: 127 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.042181\n","170it [00:01, 90.73it/s]Train epoch: 127 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.036450\n","198it [00:02, 81.42it/s]Train epoch: 127 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.039342\n","224it [00:02, 77.90it/s]Train epoch: 127 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.038254\n","249it [00:02, 73.65it/s]Train epoch: 127 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.042075\n","273it [00:03, 69.75it/s]Train epoch: 127 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.039148\n","297it [00:03, 66.94it/s]Train epoch: 127 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.042290\n","319it [00:03, 66.86it/s]Train epoch: 127 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.040471\n","347it [00:04, 60.85it/s]Train epoch: 127 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.043686\n","374it [00:04, 57.79it/s]Train epoch: 127 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.044081\n","399it [00:05, 56.53it/s]Train epoch: 127 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.040613\n","423it [00:05, 51.05it/s]Train epoch: 127 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.038136\n","450it [00:06, 45.19it/s]Train epoch: 127 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.039935\n","475it [00:06, 42.41it/s]Train epoch: 127 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.046095\n","497it [00:07, 36.21it/s]Train epoch: 127 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.047186\n","505it [00:07, 65.47it/s]\n","epoch loss: 0.03937503334300795\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 477.16it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3675, 0.5966, 0.4569, 0.5175, 0.8701\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4048, 0.6565, 0.5135, 0.5763, 0.8894\n","rec_at_5: 0.5571\n","prec_at_5: 0.5697\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 128\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 128 [batch #0, batch_size 16, seq length 212]\tLoss: 0.070456\n","17it [00:00, 160.16it/s]Train epoch: 128 [batch #25, batch_size 16, seq length 571]\tLoss: 0.028165\n","49it [00:00, 130.85it/s]Train epoch: 128 [batch #50, batch_size 16, seq length 709]\tLoss: 0.033617\n","63it [00:00, 118.60it/s]Train epoch: 128 [batch #75, batch_size 16, seq length 806]\tLoss: 0.040601\n","100it [00:00, 109.33it/s]Train epoch: 128 [batch #100, batch_size 16, seq length 892]\tLoss: 0.036756\n","122it [00:01, 105.45it/s]Train epoch: 128 [batch #125, batch_size 16, seq length 978]\tLoss: 0.037021\n","143it [00:01, 97.87it/s]Train epoch: 128 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.042347\n","173it [00:01, 89.03it/s]Train epoch: 128 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.035493\n","193it [00:01, 82.93it/s]Train epoch: 128 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.039337\n","220it [00:02, 79.96it/s]Train epoch: 128 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.036566\n","246it [00:02, 76.49it/s]Train epoch: 128 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.038771\n","270it [00:02, 72.26it/s]Train epoch: 128 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.042412\n","294it [00:03, 70.31it/s]Train epoch: 128 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.041378\n","324it [00:03, 67.65it/s]Train epoch: 128 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.037603\n","345it [00:04, 60.15it/s]Train epoch: 128 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.043791\n","373it [00:04, 60.40it/s]Train epoch: 128 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.044201\n","398it [00:05, 52.45it/s]Train epoch: 128 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.039858\n","422it [00:05, 51.77it/s]Train epoch: 128 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.040326\n","449it [00:06, 46.55it/s]Train epoch: 128 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.038510\n","474it [00:06, 42.02it/s]Train epoch: 128 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.043906\n","500it [00:07, 34.93it/s]Train epoch: 128 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.045910\n","505it [00:07, 66.14it/s]\n","epoch loss: 0.038684618482273996\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 478.68it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3698, 0.5943, 0.4617, 0.5197, 0.8699\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4053, 0.6529, 0.5166, 0.5768, 0.8895\n","rec_at_5: 0.5572\n","prec_at_5: 0.5685\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 129\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 129 [batch #0, batch_size 16, seq length 212]\tLoss: 0.065882\n","16it [00:00, 156.52it/s]Train epoch: 129 [batch #25, batch_size 16, seq length 571]\tLoss: 0.026095\n","47it [00:00, 125.83it/s]Train epoch: 129 [batch #50, batch_size 16, seq length 709]\tLoss: 0.033710\n","73it [00:00, 117.70it/s]Train epoch: 129 [batch #75, batch_size 16, seq length 806]\tLoss: 0.039335\n","97it [00:00, 111.06it/s]Train epoch: 129 [batch #100, batch_size 16, seq length 892]\tLoss: 0.035560\n","120it [00:01, 102.85it/s]Train epoch: 129 [batch #125, batch_size 16, seq length 978]\tLoss: 0.034382\n","142it [00:01, 95.09it/s] Train epoch: 129 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.040236\n","172it [00:01, 89.06it/s]Train epoch: 129 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.035903\n","199it [00:02, 81.32it/s]Train epoch: 129 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.037440\n","217it [00:02, 78.61it/s]Train epoch: 129 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.036368\n","250it [00:02, 74.81it/s]Train epoch: 129 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.040008\n","274it [00:03, 68.06it/s]Train epoch: 129 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.039190\n","298it [00:03, 67.14it/s]Train epoch: 129 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.041347\n","319it [00:03, 64.61it/s]Train epoch: 129 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.040133\n","347it [00:04, 60.04it/s]Train epoch: 129 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.041955\n","373it [00:04, 54.11it/s]Train epoch: 129 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.043244\n","399it [00:05, 54.92it/s]Train epoch: 129 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.039473\n","423it [00:05, 50.21it/s]Train epoch: 129 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.040451\n","450it [00:06, 43.78it/s]Train epoch: 129 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.038914\n","475it [00:06, 40.51it/s]Train epoch: 129 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.043847\n","497it [00:07, 34.52it/s]Train epoch: 129 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.043364\n","505it [00:07, 64.99it/s]\n","epoch loss: 0.03779649422181272\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 482.39it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3686, 0.5930, 0.4606, 0.5185, 0.8698\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4041, 0.6517, 0.5155, 0.5756, 0.8891\n","rec_at_5: 0.5561\n","prec_at_5: 0.5682\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 130\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 130 [batch #0, batch_size 16, seq length 212]\tLoss: 0.077350\n","16it [00:00, 152.43it/s]Train epoch: 130 [batch #25, batch_size 16, seq length 571]\tLoss: 0.026757\n","46it [00:00, 124.57it/s]Train epoch: 130 [batch #50, batch_size 16, seq length 709]\tLoss: 0.032920\n","72it [00:00, 112.35it/s]Train epoch: 130 [batch #75, batch_size 16, seq length 806]\tLoss: 0.039586\n","96it [00:00, 110.27it/s]Train epoch: 130 [batch #100, batch_size 16, seq length 892]\tLoss: 0.034667\n","119it [00:01, 101.52it/s]Train epoch: 130 [batch #125, batch_size 16, seq length 978]\tLoss: 0.034024\n","149it [00:01, 71.88it/s]Train epoch: 130 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.040421\n","171it [00:01, 58.71it/s]Train epoch: 130 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.033452\n","200it [00:02, 46.54it/s]Train epoch: 130 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.036175\n","225it [00:03, 47.58it/s]Train epoch: 130 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.033779\n","250it [00:03, 40.99it/s]Train epoch: 130 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.038244\n","275it [00:04, 36.97it/s]Train epoch: 130 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.038989\n","299it [00:05, 40.48it/s]Train epoch: 130 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.041656\n","324it [00:05, 39.82it/s]Train epoch: 130 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.035774\n","348it [00:06, 34.81it/s]Train epoch: 130 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.040424\n","372it [00:07, 33.77it/s]Train epoch: 130 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.038917\n","400it [00:08, 28.80it/s]Train epoch: 130 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.039283\n","423it [00:08, 28.66it/s]Train epoch: 130 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.035712\n","450it [00:09, 38.32it/s]Train epoch: 130 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.039376\n","473it [00:10, 38.88it/s]Train epoch: 130 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.041545\n","499it [00:10, 34.53it/s]Train epoch: 130 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.043114\n","505it [00:11, 45.31it/s]\n","epoch loss: 0.03681757998200926\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 478.73it/s]\n","Finish save rediction by checkpoint  130\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3681, 0.5942, 0.4589, 0.5178, 0.8695\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4035, 0.6525, 0.5140, 0.5750, 0.8890\n","rec_at_5: 0.5560\n","prec_at_5: 0.5678\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 131\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 131 [batch #0, batch_size 16, seq length 212]\tLoss: 0.068128\n","17it [00:00, 161.42it/s]Train epoch: 131 [batch #25, batch_size 16, seq length 571]\tLoss: 0.026409\n","49it [00:00, 128.14it/s]Train epoch: 131 [batch #50, batch_size 16, seq length 709]\tLoss: 0.031437\n","75it [00:00, 116.85it/s]Train epoch: 131 [batch #75, batch_size 16, seq length 806]\tLoss: 0.038174\n","99it [00:00, 112.62it/s]Train epoch: 131 [batch #100, batch_size 16, seq length 892]\tLoss: 0.033492\n","122it [00:01, 96.26it/s] Train epoch: 131 [batch #125, batch_size 16, seq length 978]\tLoss: 0.033979\n","142it [00:01, 96.48it/s]Train epoch: 131 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.039231\n","172it [00:01, 85.63it/s]Train epoch: 131 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.032451\n","200it [00:02, 81.47it/s]Train epoch: 131 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.037242\n","218it [00:02, 78.98it/s]Train epoch: 131 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.034273\n","244it [00:02, 76.03it/s]Train epoch: 131 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.036816\n","268it [00:02, 71.25it/s]Train epoch: 131 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.039273\n","299it [00:03, 67.21it/s]Train epoch: 131 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.040459\n","321it [00:03, 65.59it/s]Train epoch: 131 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.036304\n","349it [00:04, 63.00it/s]Train epoch: 131 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.038364\n","370it [00:04, 57.49it/s]Train epoch: 131 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.037572\n","400it [00:05, 54.09it/s]Train epoch: 131 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.037787\n","424it [00:05, 50.14it/s]Train epoch: 131 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.038614\n","446it [00:06, 45.16it/s]Train epoch: 131 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.037128\n","471it [00:06, 42.68it/s]Train epoch: 131 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.041635\n","498it [00:07, 35.54it/s]Train epoch: 131 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.041786\n","505it [00:07, 65.25it/s]\n","epoch loss: 0.03592674262120877\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 473.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3665, 0.5897, 0.4595, 0.5165, 0.8694\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4012, 0.6461, 0.5142, 0.5726, 0.8889\n","rec_at_5: 0.5564\n","prec_at_5: 0.5672\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 132\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 132 [batch #0, batch_size 16, seq length 212]\tLoss: 0.054627\n","16it [00:00, 159.41it/s]Train epoch: 132 [batch #25, batch_size 16, seq length 571]\tLoss: 0.025808\n","48it [00:00, 129.45it/s]Train epoch: 132 [batch #50, batch_size 16, seq length 709]\tLoss: 0.030126\n","75it [00:00, 124.27it/s]Train epoch: 132 [batch #75, batch_size 16, seq length 806]\tLoss: 0.039428\n","100it [00:00, 108.74it/s]Train epoch: 132 [batch #100, batch_size 16, seq length 892]\tLoss: 0.032617\n","124it [00:01, 100.79it/s]Train epoch: 132 [batch #125, batch_size 16, seq length 978]\tLoss: 0.034446\n","146it [00:01, 99.27it/s] Train epoch: 132 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.038580\n","166it [00:01, 91.95it/s]Train epoch: 132 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.033526\n","196it [00:01, 86.00it/s]Train epoch: 132 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.035160\n","223it [00:02, 82.89it/s]Train epoch: 132 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.033021\n","249it [00:02, 74.62it/s]Train epoch: 132 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.036259\n","273it [00:02, 70.44it/s]Train epoch: 132 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.037741\n","297it [00:03, 67.21it/s]Train epoch: 132 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.036876\n","319it [00:03, 65.82it/s]Train epoch: 132 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.037058\n","347it [00:04, 63.88it/s]Train epoch: 132 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.039864\n","375it [00:04, 57.94it/s]Train epoch: 132 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.037517\n","400it [00:05, 52.60it/s]Train epoch: 132 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.034832\n","424it [00:05, 52.15it/s]Train epoch: 132 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.034954\n","447it [00:06, 45.44it/s]Train epoch: 132 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.036683\n","472it [00:06, 42.12it/s]Train epoch: 132 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.038889\n","499it [00:07, 34.69it/s]Train epoch: 132 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.041092\n","505it [00:07, 66.44it/s]\n","epoch loss: 0.035117385547497486\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 481.01it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3655, 0.5880, 0.4580, 0.5149, 0.8692\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4008, 0.6474, 0.5128, 0.5723, 0.8886\n","rec_at_5: 0.5552\n","prec_at_5: 0.5671\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 133\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 133 [batch #0, batch_size 16, seq length 212]\tLoss: 0.057398\n","16it [00:00, 155.14it/s]Train epoch: 133 [batch #25, batch_size 16, seq length 571]\tLoss: 0.024513\n","47it [00:00, 127.55it/s]Train epoch: 133 [batch #50, batch_size 16, seq length 709]\tLoss: 0.029299\n","73it [00:00, 115.00it/s]Train epoch: 133 [batch #75, batch_size 16, seq length 806]\tLoss: 0.036851\n","97it [00:00, 110.25it/s]Train epoch: 133 [batch #100, batch_size 16, seq length 892]\tLoss: 0.032996\n","120it [00:01, 102.81it/s]Train epoch: 133 [batch #125, batch_size 16, seq length 978]\tLoss: 0.031442\n","142it [00:01, 95.53it/s] Train epoch: 133 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.036068\n","172it [00:01, 91.27it/s]Train epoch: 133 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.034163\n","200it [00:02, 80.63it/s]Train epoch: 133 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.035294\n","218it [00:02, 80.86it/s]Train epoch: 133 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.032075\n","244it [00:02, 75.77it/s]Train epoch: 133 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.035833\n","268it [00:02, 73.40it/s]Train epoch: 133 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.038762\n","300it [00:03, 68.11it/s]Train epoch: 133 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.036658\n","322it [00:03, 65.40it/s]Train epoch: 133 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.033772\n","350it [00:04, 61.39it/s]Train epoch: 133 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.037973\n","370it [00:04, 57.83it/s]Train epoch: 133 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.036235\n","400it [00:05, 52.98it/s]Train epoch: 133 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.034712\n","424it [00:05, 51.31it/s]Train epoch: 133 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.033611\n","450it [00:06, 45.97it/s]Train epoch: 133 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.035684\n","475it [00:06, 41.28it/s]Train epoch: 133 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.037807\n","497it [00:07, 34.61it/s]Train epoch: 133 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.040129\n","505it [00:07, 65.08it/s]\n","epoch loss: 0.03450176303948064\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 341.71it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3662, 0.5864, 0.4584, 0.5145, 0.8688\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4012, 0.6474, 0.5134, 0.5727, 0.8885\n","rec_at_5: 0.5547\n","prec_at_5: 0.5667\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 134\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 134 [batch #0, batch_size 16, seq length 212]\tLoss: 0.060856\n","16it [00:00, 153.32it/s]Train epoch: 134 [batch #25, batch_size 16, seq length 571]\tLoss: 0.024654\n","47it [00:00, 126.99it/s]Train epoch: 134 [batch #50, batch_size 16, seq length 709]\tLoss: 0.029722\n","73it [00:00, 124.34it/s]Train epoch: 134 [batch #75, batch_size 16, seq length 806]\tLoss: 0.038082\n","98it [00:00, 109.88it/s]Train epoch: 134 [batch #100, batch_size 16, seq length 892]\tLoss: 0.033064\n","122it [00:01, 101.97it/s]Train epoch: 134 [batch #125, batch_size 16, seq length 978]\tLoss: 0.032170\n","144it [00:01, 100.75it/s]Train epoch: 134 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.037685\n","175it [00:01, 91.69it/s]Train epoch: 134 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.031844\n","195it [00:01, 87.09it/s]Train epoch: 134 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.031923\n","222it [00:02, 82.24it/s]Train epoch: 134 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.032375\n","247it [00:02, 72.80it/s]Train epoch: 134 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.035964\n","271it [00:02, 72.73it/s]Train epoch: 134 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.033452\n","295it [00:03, 70.48it/s]Train epoch: 134 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.036852\n","325it [00:03, 62.61it/s]Train epoch: 134 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.031511\n","346it [00:04, 62.15it/s]Train epoch: 134 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.040257\n","374it [00:04, 56.47it/s]Train epoch: 134 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.035730\n","398it [00:05, 54.26it/s]Train epoch: 134 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.036991\n","422it [00:05, 48.37it/s]Train epoch: 134 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.033405\n","450it [00:06, 46.29it/s]Train epoch: 134 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.035464\n","475it [00:06, 40.25it/s]Train epoch: 134 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.039607\n","497it [00:07, 36.12it/s]Train epoch: 134 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.038193\n","505it [00:07, 65.77it/s]\n","epoch loss: 0.03377713959405918\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 482.88it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3655, 0.5864, 0.4574, 0.5139, 0.8688\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4012, 0.6483, 0.5129, 0.5727, 0.8885\n","rec_at_5: 0.5535\n","prec_at_5: 0.5649\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 135\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 135 [batch #0, batch_size 16, seq length 212]\tLoss: 0.062696\n","17it [00:00, 159.48it/s]Train epoch: 135 [batch #25, batch_size 16, seq length 571]\tLoss: 0.025799\n","48it [00:00, 124.95it/s]Train epoch: 135 [batch #50, batch_size 16, seq length 709]\tLoss: 0.027418\n","74it [00:00, 114.32it/s]Train epoch: 135 [batch #75, batch_size 16, seq length 806]\tLoss: 0.034937\n","98it [00:00, 113.96it/s]Train epoch: 135 [batch #100, batch_size 16, seq length 892]\tLoss: 0.032669\n","121it [00:01, 103.73it/s]Train epoch: 135 [batch #125, batch_size 16, seq length 978]\tLoss: 0.030623\n","143it [00:01, 93.72it/s] Train epoch: 135 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.035332\n","173it [00:01, 90.93it/s]Train epoch: 135 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.031402\n","193it [00:01, 88.79it/s]Train epoch: 135 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.032027\n","220it [00:02, 81.92it/s]Train epoch: 135 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.031242\n","246it [00:02, 75.46it/s]Train epoch: 135 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.034884\n","270it [00:02, 73.37it/s]Train epoch: 135 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.035405\n","294it [00:03, 68.91it/s]Train epoch: 135 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.035471\n","322it [00:03, 66.92it/s]Train epoch: 135 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.033135\n","350it [00:04, 62.02it/s]Train epoch: 135 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.033822\n","371it [00:04, 58.74it/s]Train epoch: 135 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.035767\n","395it [00:04, 56.88it/s]Train epoch: 135 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.035090\n","425it [00:05, 51.64it/s]Train epoch: 135 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.032347\n","446it [00:06, 43.39it/s]Train epoch: 135 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.033783\n","471it [00:06, 42.10it/s]Train epoch: 135 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.037602\n","498it [00:07, 35.84it/s]Train epoch: 135 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.039560\n","505it [00:07, 65.94it/s]\n","epoch loss: 0.03308480410458575\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 473.45it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3673, 0.5856, 0.4614, 0.5162, 0.8686\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4015, 0.6453, 0.5151, 0.5729, 0.8885\n","rec_at_5: 0.5535\n","prec_at_5: 0.5631\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 136\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 136 [batch #0, batch_size 16, seq length 212]\tLoss: 0.053233\n","14it [00:00, 137.87it/s]Train epoch: 136 [batch #25, batch_size 16, seq length 571]\tLoss: 0.023732\n","42it [00:00, 118.68it/s]Train epoch: 136 [batch #50, batch_size 16, seq length 709]\tLoss: 0.027376\n","67it [00:00, 102.20it/s]Train epoch: 136 [batch #75, batch_size 16, seq length 806]\tLoss: 0.036553\n","100it [00:00, 98.69it/s]Train epoch: 136 [batch #100, batch_size 16, seq length 892]\tLoss: 0.033236\n","120it [00:01, 98.19it/s]Train epoch: 136 [batch #125, batch_size 16, seq length 978]\tLoss: 0.029824\n","150it [00:01, 88.01it/s]Train epoch: 136 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.033900\n","170it [00:01, 88.67it/s]Train epoch: 136 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.030666\n","197it [00:02, 84.71it/s]Train epoch: 136 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.033431\n","223it [00:02, 77.82it/s]Train epoch: 136 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.030878\n","247it [00:02, 69.53it/s]Train epoch: 136 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.034584\n","269it [00:03, 61.17it/s]Train epoch: 136 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.036547\n","298it [00:03, 62.09it/s]Train epoch: 136 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.034471\n","319it [00:03, 60.90it/s]Train epoch: 136 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.033750\n","345it [00:04, 57.25it/s]Train epoch: 136 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.034406\n","370it [00:04, 52.43it/s]Train epoch: 136 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.033473\n","400it [00:05, 47.99it/s]Train epoch: 136 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.030985\n","424it [00:06, 47.84it/s]Train epoch: 136 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.034713\n","450it [00:06, 45.27it/s]Train epoch: 136 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.032743\n","475it [00:07, 41.02it/s]Train epoch: 136 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.035005\n","497it [00:07, 35.31it/s]Train epoch: 136 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.036342\n","505it [00:08, 62.07it/s]\n","epoch loss: 0.03224919008109534\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 460.82it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3641, 0.5832, 0.4576, 0.5128, 0.8683\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3989, 0.6449, 0.5111, 0.5703, 0.8882\n","rec_at_5: 0.5552\n","prec_at_5: 0.5641\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 137\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 137 [batch #0, batch_size 16, seq length 212]\tLoss: 0.060159\n","16it [00:00, 153.98it/s]Train epoch: 137 [batch #25, batch_size 16, seq length 571]\tLoss: 0.026691\n","47it [00:00, 124.86it/s]Train epoch: 137 [batch #50, batch_size 16, seq length 709]\tLoss: 0.026470\n","72it [00:00, 112.66it/s]Train epoch: 137 [batch #75, batch_size 16, seq length 806]\tLoss: 0.033498\n","96it [00:00, 103.67it/s]Train epoch: 137 [batch #100, batch_size 16, seq length 892]\tLoss: 0.030037\n","118it [00:01, 98.24it/s] Train epoch: 137 [batch #125, batch_size 16, seq length 978]\tLoss: 0.029273\n","148it [00:01, 92.40it/s]Train epoch: 137 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.036541\n","168it [00:01, 87.20it/s]Train epoch: 137 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.030337\n","195it [00:01, 83.31it/s]Train epoch: 137 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.031714\n","221it [00:02, 73.59it/s]Train epoch: 137 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.029577\n","245it [00:02, 71.90it/s]Train epoch: 137 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.031962\n","268it [00:03, 65.47it/s]Train epoch: 137 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.034199\n","297it [00:03, 63.48it/s]Train epoch: 137 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.033474\n","325it [00:03, 64.00it/s]Train epoch: 137 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.029972\n","346it [00:04, 62.11it/s]Train epoch: 137 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.034393\n","374it [00:04, 57.02it/s]Train epoch: 137 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.033503\n","399it [00:05, 52.20it/s]Train epoch: 137 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.031665\n","423it [00:05, 50.54it/s]Train epoch: 137 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.031994\n","446it [00:06, 45.65it/s]Train epoch: 137 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.031413\n","471it [00:06, 42.99it/s]Train epoch: 137 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.036121\n","498it [00:07, 33.68it/s]Train epoch: 137 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.035055\n","505it [00:07, 63.77it/s]\n","epoch loss: 0.031423933293190924\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 471.14it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3626, 0.5830, 0.4554, 0.5114, 0.8676\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3976, 0.6438, 0.5097, 0.5690, 0.8877\n","rec_at_5: 0.5515\n","prec_at_5: 0.5626\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 138\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 138 [batch #0, batch_size 16, seq length 212]\tLoss: 0.052427\n","15it [00:00, 146.82it/s]Train epoch: 138 [batch #25, batch_size 16, seq length 571]\tLoss: 0.022187\n","45it [00:00, 125.23it/s]Train epoch: 138 [batch #50, batch_size 16, seq length 709]\tLoss: 0.026203\n","71it [00:00, 121.96it/s]Train epoch: 138 [batch #75, batch_size 16, seq length 806]\tLoss: 0.031089\n","96it [00:00, 109.36it/s]Train epoch: 138 [batch #100, batch_size 16, seq length 892]\tLoss: 0.030232\n","119it [00:01, 100.99it/s]Train epoch: 138 [batch #125, batch_size 16, seq length 978]\tLoss: 0.029306\n","150it [00:01, 91.13it/s]Train epoch: 138 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.032854\n","170it [00:01, 87.86it/s]Train epoch: 138 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.026841\n","197it [00:02, 80.10it/s]Train epoch: 138 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.030264\n","224it [00:02, 76.63it/s]Train epoch: 138 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.028481\n","248it [00:02, 71.56it/s]Train epoch: 138 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.031117\n","272it [00:03, 68.88it/s]Train epoch: 138 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.034611\n","300it [00:03, 62.38it/s]Train epoch: 138 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.032407\n","321it [00:03, 62.34it/s]Train epoch: 138 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.029617\n","349it [00:04, 57.89it/s]Train epoch: 138 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.033938\n","374it [00:04, 56.17it/s]Train epoch: 138 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.031122\n","398it [00:05, 52.47it/s]Train epoch: 138 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.031706\n","422it [00:05, 50.21it/s]Train epoch: 138 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.031540\n","449it [00:06, 45.58it/s]Train epoch: 138 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.030935\n","474it [00:07, 38.22it/s]Train epoch: 138 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.032735\n","498it [00:07, 33.86it/s]Train epoch: 138 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.034865\n","505it [00:07, 63.29it/s]\n","epoch loss: 0.030315515690363278\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 463.30it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3644, 0.5837, 0.4574, 0.5129, 0.8676\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3988, 0.6447, 0.5111, 0.5702, 0.8876\n","rec_at_5: 0.5523\n","prec_at_5: 0.5616\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 139\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 139 [batch #0, batch_size 16, seq length 212]\tLoss: 0.060474\n","16it [00:00, 155.40it/s]Train epoch: 139 [batch #25, batch_size 16, seq length 571]\tLoss: 0.023620\n","47it [00:00, 124.77it/s]Train epoch: 139 [batch #50, batch_size 16, seq length 709]\tLoss: 0.025259\n","73it [00:00, 108.76it/s]Train epoch: 139 [batch #75, batch_size 16, seq length 806]\tLoss: 0.032009\n","96it [00:00, 108.20it/s]Train epoch: 139 [batch #100, batch_size 16, seq length 892]\tLoss: 0.029370\n","118it [00:01, 101.48it/s]Train epoch: 139 [batch #125, batch_size 16, seq length 978]\tLoss: 0.028821\n","150it [00:01, 86.24it/s]Train epoch: 139 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.032447\n","169it [00:01, 86.67it/s]Train epoch: 139 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.027828\n","196it [00:02, 83.40it/s]Train epoch: 139 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.032736\n","223it [00:02, 75.91it/s]Train epoch: 139 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.028706\n","248it [00:02, 70.17it/s]Train epoch: 139 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.031478\n","272it [00:03, 69.11it/s]Train epoch: 139 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.031976\n","295it [00:03, 66.55it/s]Train epoch: 139 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.031192\n","323it [00:03, 63.48it/s]Train epoch: 139 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.028444\n","344it [00:04, 58.79it/s]Train epoch: 139 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.030327\n","371it [00:04, 52.59it/s]Train epoch: 139 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.033993\n","396it [00:05, 54.41it/s]Train epoch: 139 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.029652\n","420it [00:05, 47.42it/s]Train epoch: 139 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.031051\n","446it [00:06, 44.12it/s]Train epoch: 139 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.030828\n","471it [00:06, 41.01it/s]Train epoch: 139 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.033874\n","498it [00:07, 36.00it/s]Train epoch: 139 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.033491\n","505it [00:07, 63.70it/s]\n","epoch loss: 0.029902653017606917\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 465.51it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3625, 0.5817, 0.4555, 0.5109, 0.8675\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3974, 0.6430, 0.5099, 0.5687, 0.8874\n","rec_at_5: 0.5518\n","prec_at_5: 0.5622\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 140\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 140 [batch #0, batch_size 16, seq length 212]\tLoss: 0.069614\n","16it [00:00, 158.08it/s]Train epoch: 140 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021861\n","47it [00:00, 124.02it/s]Train epoch: 140 [batch #50, batch_size 16, seq length 709]\tLoss: 0.027026\n","73it [00:00, 122.05it/s]Train epoch: 140 [batch #75, batch_size 16, seq length 806]\tLoss: 0.029253\n","98it [00:00, 110.01it/s]Train epoch: 140 [batch #100, batch_size 16, seq length 892]\tLoss: 0.030611\n","121it [00:01, 98.86it/s] Train epoch: 140 [batch #125, batch_size 16, seq length 978]\tLoss: 0.028667\n","142it [00:01, 92.33it/s]Train epoch: 140 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.031556\n","171it [00:01, 88.26it/s]Train epoch: 140 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.027810\n","199it [00:02, 82.43it/s]Train epoch: 140 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.029886\n","225it [00:02, 78.31it/s]Train epoch: 140 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.029098\n","249it [00:02, 73.15it/s]Train epoch: 140 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.032492\n","273it [00:03, 70.39it/s]Train epoch: 140 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.031378\n","297it [00:03, 67.57it/s]Train epoch: 140 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.032361\n","319it [00:03, 65.30it/s]Train epoch: 140 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.029098\n","347it [00:04, 63.78it/s]Train epoch: 140 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.030770\n","375it [00:04, 55.85it/s]Train epoch: 140 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.031256\n","395it [00:05, 54.62it/s]Train epoch: 140 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.029986\n","425it [00:05, 48.95it/s]Train epoch: 140 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.030078\n","446it [00:06, 46.05it/s]Train epoch: 140 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.029458\n","471it [00:06, 41.90it/s]Train epoch: 140 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.031971\n","497it [00:07, 33.76it/s]Train epoch: 140 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.035057\n","505it [00:07, 65.16it/s]\n","epoch loss: 0.029297662681282158\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 476.72it/s]\n","Finish save rediction by checkpoint  140\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3631, 0.5838, 0.4552, 0.5115, 0.8673\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3980, 0.6435, 0.5105, 0.5694, 0.8875\n","rec_at_5: 0.5511\n","prec_at_5: 0.5619\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 141\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 141 [batch #0, batch_size 16, seq length 212]\tLoss: 0.040060\n","16it [00:00, 153.68it/s]Train epoch: 141 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021446\n","47it [00:00, 127.84it/s]Train epoch: 141 [batch #50, batch_size 16, seq length 709]\tLoss: 0.024218\n","74it [00:00, 114.00it/s]Train epoch: 141 [batch #75, batch_size 16, seq length 806]\tLoss: 0.031824\n","98it [00:00, 104.85it/s]Train epoch: 141 [batch #100, batch_size 16, seq length 892]\tLoss: 0.029010\n","120it [00:01, 103.34it/s]Train epoch: 141 [batch #125, batch_size 16, seq length 978]\tLoss: 0.026875\n","141it [00:01, 95.49it/s]Train epoch: 141 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.031590\n","171it [00:01, 91.28it/s]Train epoch: 141 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.025258\n","200it [00:02, 79.40it/s]Train epoch: 141 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.029035\n","218it [00:02, 80.99it/s]Train epoch: 141 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.027317\n","243it [00:02, 73.96it/s]Train epoch: 141 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.028575\n","275it [00:03, 72.34it/s]Train epoch: 141 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.030778\n","297it [00:03, 65.87it/s]Train epoch: 141 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.030793\n","319it [00:03, 65.60it/s]Train epoch: 141 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.028932\n","347it [00:04, 64.08it/s]Train epoch: 141 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.030071\n","375it [00:04, 55.63it/s]Train epoch: 141 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.031056\n","400it [00:05, 54.54it/s]Train epoch: 141 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.029430\n","424it [00:05, 52.61it/s]Train epoch: 141 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.028492\n","447it [00:06, 46.40it/s]Train epoch: 141 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.028827\n","472it [00:06, 41.51it/s]Train epoch: 141 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.031030\n","499it [00:07, 35.00it/s]Train epoch: 141 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.029812\n","505it [00:07, 65.64it/s]\n","epoch loss: 0.028527642478520916\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 475.99it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3614, 0.5807, 0.4537, 0.5094, 0.8669\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3946, 0.6410, 0.5065, 0.5659, 0.8874\n","rec_at_5: 0.5496\n","prec_at_5: 0.5598\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 142\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 142 [batch #0, batch_size 16, seq length 212]\tLoss: 0.051834\n","16it [00:00, 153.79it/s]Train epoch: 142 [batch #25, batch_size 16, seq length 571]\tLoss: 0.022810\n","47it [00:00, 126.66it/s]Train epoch: 142 [batch #50, batch_size 16, seq length 709]\tLoss: 0.024937\n","72it [00:00, 115.11it/s]Train epoch: 142 [batch #75, batch_size 16, seq length 806]\tLoss: 0.027773\n","95it [00:00, 105.59it/s]Train epoch: 142 [batch #100, batch_size 16, seq length 892]\tLoss: 0.028141\n","117it [00:01, 99.98it/s] Train epoch: 142 [batch #125, batch_size 16, seq length 978]\tLoss: 0.027186\n","148it [00:01, 95.42it/s]Train epoch: 142 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.032524\n","168it [00:01, 90.48it/s]Train epoch: 142 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.026755\n","196it [00:01, 84.62it/s]Train epoch: 142 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.028349\n","223it [00:02, 80.76it/s]Train epoch: 142 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.026798\n","249it [00:02, 73.24it/s]Train epoch: 142 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.029658\n","273it [00:03, 71.13it/s]Train epoch: 142 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.030895\n","296it [00:03, 68.18it/s]Train epoch: 142 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.029163\n","325it [00:03, 62.60it/s]Train epoch: 142 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.027270\n","346it [00:04, 59.17it/s]Train epoch: 142 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.032357\n","373it [00:04, 57.80it/s]Train epoch: 142 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.028935\n","398it [00:05, 52.55it/s]Train epoch: 142 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.028653\n","422it [00:05, 51.83it/s]Train epoch: 142 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.028168\n","449it [00:06, 44.60it/s]Train epoch: 142 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.028453\n","474it [00:06, 43.67it/s]Train epoch: 142 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.031286\n","497it [00:07, 36.87it/s]Train epoch: 142 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.032954\n","505it [00:07, 65.45it/s]\n","epoch loss: 0.028075695689306547\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 479.10it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3647, 0.5815, 0.4574, 0.5120, 0.8669\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3984, 0.6443, 0.5107, 0.5698, 0.8871\n","rec_at_5: 0.5508\n","prec_at_5: 0.5613\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 143\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 143 [batch #0, batch_size 16, seq length 212]\tLoss: 0.054664\n","16it [00:00, 158.89it/s]Train epoch: 143 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021928\n","48it [00:00, 127.65it/s]Train epoch: 143 [batch #50, batch_size 16, seq length 709]\tLoss: 0.023751\n","75it [00:00, 122.12it/s]Train epoch: 143 [batch #75, batch_size 16, seq length 806]\tLoss: 0.029876\n","100it [00:00, 109.74it/s]Train epoch: 143 [batch #100, batch_size 16, seq length 892]\tLoss: 0.026481\n","123it [00:01, 102.61it/s]Train epoch: 143 [batch #125, batch_size 16, seq length 978]\tLoss: 0.024415\n","145it [00:01, 94.31it/s] Train epoch: 143 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.028704\n","175it [00:01, 88.86it/s]Train epoch: 143 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.025577\n","193it [00:01, 82.97it/s]Train epoch: 143 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.027518\n","220it [00:02, 81.59it/s]Train epoch: 143 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.026918\n","247it [00:02, 75.03it/s]Train epoch: 143 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.031467\n","271it [00:02, 73.37it/s]Train epoch: 143 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.030502\n","295it [00:03, 69.83it/s]Train epoch: 143 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.028536\n","325it [00:03, 62.70it/s]Train epoch: 143 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.027911\n","346it [00:04, 62.58it/s]Train epoch: 143 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.030659\n","374it [00:04, 57.15it/s]Train epoch: 143 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.030386\n","398it [00:05, 55.07it/s]Train epoch: 143 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.027856\n","422it [00:05, 52.76it/s]Train epoch: 143 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.026383\n","450it [00:06, 46.87it/s]Train epoch: 143 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.027394\n","475it [00:06, 42.40it/s]Train epoch: 143 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.028761\n","498it [00:07, 35.59it/s]Train epoch: 143 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.031299\n","505it [00:07, 66.20it/s]\n","epoch loss: 0.027475778836904483\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 479.23it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3610, 0.5773, 0.4530, 0.5077, 0.8667\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3955, 0.6425, 0.5072, 0.5669, 0.8871\n","rec_at_5: 0.5511\n","prec_at_5: 0.5613\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 144\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 144 [batch #0, batch_size 16, seq length 212]\tLoss: 0.042260\n","16it [00:00, 155.46it/s]Train epoch: 144 [batch #25, batch_size 16, seq length 571]\tLoss: 0.019852\n","47it [00:00, 128.52it/s]Train epoch: 144 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021076\n","74it [00:00, 119.70it/s]Train epoch: 144 [batch #75, batch_size 16, seq length 806]\tLoss: 0.028434\n","99it [00:00, 109.54it/s]Train epoch: 144 [batch #100, batch_size 16, seq length 892]\tLoss: 0.026729\n","122it [00:01, 100.72it/s]Train epoch: 144 [batch #125, batch_size 16, seq length 978]\tLoss: 0.026382\n","143it [00:01, 96.64it/s]Train epoch: 144 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.029536\n","173it [00:01, 87.00it/s]Train epoch: 144 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.024522\n","200it [00:02, 81.26it/s]Train epoch: 144 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.025876\n","218it [00:02, 81.42it/s]Train epoch: 144 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.025000\n","243it [00:02, 76.94it/s]Train epoch: 144 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.027503\n","275it [00:03, 67.96it/s]Train epoch: 144 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.028311\n","298it [00:03, 66.29it/s]Train epoch: 144 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.029325\n","319it [00:03, 63.62it/s]Train epoch: 144 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.027260\n","347it [00:04, 63.33it/s]Train epoch: 144 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.028028\n","375it [00:04, 55.00it/s]Train epoch: 144 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.028569\n","399it [00:05, 51.47it/s]Train epoch: 144 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.028174\n","423it [00:05, 50.50it/s]Train epoch: 144 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.028180\n","449it [00:06, 44.94it/s]Train epoch: 144 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.026444\n","474it [00:06, 41.36it/s]Train epoch: 144 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.028092\n","497it [00:07, 36.03it/s]Train epoch: 144 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.030280\n","505it [00:07, 64.84it/s]\n","epoch loss: 0.02666421903361852\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 477.90it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3626, 0.5762, 0.4559, 0.5090, 0.8663\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3966, 0.6409, 0.5100, 0.5680, 0.8868\n","rec_at_5: 0.5525\n","prec_at_5: 0.5621\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 145\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 145 [batch #0, batch_size 16, seq length 212]\tLoss: 0.042737\n","16it [00:00, 158.40it/s]Train epoch: 145 [batch #25, batch_size 16, seq length 571]\tLoss: 0.020275\n","47it [00:00, 127.20it/s]Train epoch: 145 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021974\n","74it [00:00, 113.89it/s]Train epoch: 145 [batch #75, batch_size 16, seq length 806]\tLoss: 0.027853\n","98it [00:00, 104.94it/s]Train epoch: 145 [batch #100, batch_size 16, seq length 892]\tLoss: 0.026540\n","120it [00:01, 102.94it/s]Train epoch: 145 [batch #125, batch_size 16, seq length 978]\tLoss: 0.026634\n","142it [00:01, 98.28it/s]Train epoch: 145 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.028111\n","172it [00:01, 92.66it/s]Train epoch: 145 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.023527\n","200it [00:01, 83.56it/s]Train epoch: 145 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.029161\n","218it [00:02, 83.06it/s]Train epoch: 145 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.024425\n","243it [00:02, 74.13it/s]Train epoch: 145 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.026678\n","275it [00:03, 71.40it/s]Train epoch: 145 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.027746\n","299it [00:03, 70.22it/s]Train epoch: 145 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.027697\n","322it [00:03, 62.49it/s]Train epoch: 145 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.025266\n","350it [00:04, 62.04it/s]Train epoch: 145 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.028076\n","371it [00:04, 56.43it/s]Train epoch: 145 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.027356\n","396it [00:05, 55.43it/s]Train epoch: 145 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.026059\n","420it [00:05, 51.27it/s]Train epoch: 145 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.026323\n","447it [00:06, 44.96it/s]Train epoch: 145 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.024787\n","472it [00:06, 43.26it/s]Train epoch: 145 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.028887\n","500it [00:07, 36.68it/s]Train epoch: 145 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026767\n","505it [00:07, 65.99it/s]\n","epoch loss: 0.025537178691679447\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 472.78it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3608, 0.5753, 0.4529, 0.5068, 0.8663\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3950, 0.6415, 0.5069, 0.5663, 0.8863\n","rec_at_5: 0.5515\n","prec_at_5: 0.5621\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 146\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 146 [batch #0, batch_size 16, seq length 212]\tLoss: 0.043148\n","17it [00:00, 160.41it/s]Train epoch: 146 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021426\n","50it [00:00, 124.55it/s]Train epoch: 146 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021246\n","63it [00:00, 123.96it/s]Train epoch: 146 [batch #75, batch_size 16, seq length 806]\tLoss: 0.026442\n","89it [00:00, 110.57it/s]Train epoch: 146 [batch #100, batch_size 16, seq length 892]\tLoss: 0.026615\n","124it [00:01, 100.54it/s]Train epoch: 146 [batch #125, batch_size 16, seq length 978]\tLoss: 0.024396\n","146it [00:01, 97.02it/s]Train epoch: 146 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.028743\n","166it [00:01, 89.87it/s]Train epoch: 146 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.024149\n","194it [00:01, 82.24it/s]Train epoch: 146 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.024450\n","221it [00:02, 78.80it/s]Train epoch: 146 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.024362\n","245it [00:02, 75.04it/s]Train epoch: 146 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.026449\n","269it [00:02, 73.40it/s]Train epoch: 146 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.026868\n","300it [00:03, 65.04it/s]Train epoch: 146 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.027669\n","322it [00:03, 64.51it/s]Train epoch: 146 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.023887\n","350it [00:04, 64.06it/s]Train epoch: 146 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.025874\n","371it [00:04, 59.29it/s]Train epoch: 146 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.028199\n","397it [00:05, 54.52it/s]Train epoch: 146 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.024693\n","421it [00:05, 52.01it/s]Train epoch: 146 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.024717\n","448it [00:06, 46.36it/s]Train epoch: 146 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.024113\n","473it [00:06, 41.75it/s]Train epoch: 146 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.026861\n","499it [00:07, 31.95it/s]Train epoch: 146 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.028658\n","505it [00:07, 65.60it/s]\n","epoch loss: 0.025377651712036518\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 346.95it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3613, 0.5773, 0.4521, 0.5071, 0.8661\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3959, 0.6419, 0.5081, 0.5673, 0.8866\n","rec_at_5: 0.5523\n","prec_at_5: 0.5622\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 147\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 147 [batch #0, batch_size 16, seq length 212]\tLoss: 0.037040\n","18it [00:00, 81.17it/s]Train epoch: 147 [batch #25, batch_size 16, seq length 571]\tLoss: 0.018123\n","43it [00:00, 70.75it/s]Train epoch: 147 [batch #50, batch_size 16, seq length 709]\tLoss: 0.023168\n","75it [00:01, 69.61it/s]Train epoch: 147 [batch #75, batch_size 16, seq length 806]\tLoss: 0.028872\n","98it [00:01, 66.32it/s]Train epoch: 147 [batch #100, batch_size 16, seq length 892]\tLoss: 0.025176\n","119it [00:01, 64.75it/s]Train epoch: 147 [batch #125, batch_size 16, seq length 978]\tLoss: 0.025239\n","146it [00:02, 58.27it/s]Train epoch: 147 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.026736\n","170it [00:02, 56.38it/s]Train epoch: 147 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.023280\n","200it [00:03, 51.80it/s]Train epoch: 147 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.025057\n","223it [00:03, 48.44it/s]Train epoch: 147 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.024793\n","247it [00:04, 62.57it/s]Train epoch: 147 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.025980\n","270it [00:04, 67.16it/s]Train epoch: 147 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.026941\n","293it [00:04, 67.53it/s]Train epoch: 147 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.026436\n","323it [00:05, 66.89it/s]Train epoch: 147 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.024979\n","344it [00:05, 63.29it/s]Train epoch: 147 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.024793\n","372it [00:06, 57.31it/s]Train epoch: 147 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.027203\n","398it [00:06, 56.34it/s]Train epoch: 147 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.026665\n","422it [00:06, 48.36it/s]Train epoch: 147 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.023181\n","449it [00:07, 44.30it/s]Train epoch: 147 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.022851\n","474it [00:08, 40.84it/s]Train epoch: 147 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.023731\n","497it [00:08, 36.17it/s]Train epoch: 147 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026526\n","505it [00:09, 55.66it/s]\n","epoch loss: 0.024764274839901984\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 472.65it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3632, 0.5761, 0.4566, 0.5094, 0.8662\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3959, 0.6391, 0.5099, 0.5672, 0.8869\n","rec_at_5: 0.5534\n","prec_at_5: 0.5624\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 148\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 148 [batch #0, batch_size 16, seq length 212]\tLoss: 0.042236\n","15it [00:00, 149.63it/s]Train epoch: 148 [batch #25, batch_size 16, seq length 571]\tLoss: 0.022167\n","45it [00:00, 121.24it/s]Train epoch: 148 [batch #50, batch_size 16, seq length 709]\tLoss: 0.024189\n","71it [00:00, 121.23it/s]Train epoch: 148 [batch #75, batch_size 16, seq length 806]\tLoss: 0.025094\n","96it [00:00, 105.05it/s]Train epoch: 148 [batch #100, batch_size 16, seq length 892]\tLoss: 0.024196\n","118it [00:01, 104.18it/s]Train epoch: 148 [batch #125, batch_size 16, seq length 978]\tLoss: 0.022242\n","150it [00:01, 93.34it/s]Train epoch: 148 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.027172\n","170it [00:01, 84.62it/s]Train epoch: 148 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.021285\n","198it [00:02, 81.71it/s]Train epoch: 148 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.024423\n","225it [00:02, 79.07it/s]Train epoch: 148 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.022863\n","249it [00:02, 71.92it/s]Train epoch: 148 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.025455\n","273it [00:03, 70.24it/s]Train epoch: 148 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.025801\n","297it [00:03, 66.69it/s]Train epoch: 148 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.025067\n","319it [00:03, 63.44it/s]Train epoch: 148 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.023236\n","347it [00:04, 63.16it/s]Train epoch: 148 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.027582\n","375it [00:04, 60.14it/s]Train epoch: 148 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.026726\n","400it [00:05, 52.90it/s]Train epoch: 148 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.023109\n","424it [00:05, 49.96it/s]Train epoch: 148 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.026521\n","446it [00:06, 45.78it/s]Train epoch: 148 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.022850\n","471it [00:06, 42.57it/s]Train epoch: 148 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.027457\n","499it [00:07, 36.23it/s]Train epoch: 148 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026014\n","505it [00:07, 65.71it/s]\n","epoch loss: 0.024222612858630053\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 478.90it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3608, 0.5730, 0.4538, 0.5065, 0.8656\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3949, 0.6379, 0.5090, 0.5662, 0.8868\n","rec_at_5: 0.5502\n","prec_at_5: 0.5611\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 149\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 149 [batch #0, batch_size 16, seq length 212]\tLoss: 0.031808\n","17it [00:00, 160.87it/s]Train epoch: 149 [batch #25, batch_size 16, seq length 571]\tLoss: 0.020722\n","49it [00:00, 125.46it/s]Train epoch: 149 [batch #50, batch_size 16, seq length 709]\tLoss: 0.019593\n","75it [00:00, 122.57it/s]Train epoch: 149 [batch #75, batch_size 16, seq length 806]\tLoss: 0.025116\n","100it [00:00, 111.41it/s]Train epoch: 149 [batch #100, batch_size 16, seq length 892]\tLoss: 0.024581\n","123it [00:01, 104.25it/s]Train epoch: 149 [batch #125, batch_size 16, seq length 978]\tLoss: 0.023034\n","145it [00:01, 91.67it/s] Train epoch: 149 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.025216\n","175it [00:01, 89.53it/s]Train epoch: 149 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.021477\n","194it [00:01, 84.76it/s]Train epoch: 149 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.023489\n","221it [00:02, 75.05it/s]Train epoch: 149 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.022751\n","246it [00:02, 73.18it/s]Train epoch: 149 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.024351\n","270it [00:02, 72.09it/s]Train epoch: 149 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.023450\n","300it [00:03, 68.06it/s]Train epoch: 149 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.025956\n","322it [00:03, 62.52it/s]Train epoch: 149 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.023225\n","350it [00:04, 61.47it/s]Train epoch: 149 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.023179\n","370it [00:04, 55.78it/s]Train epoch: 149 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.023307\n","395it [00:05, 55.32it/s]Train epoch: 149 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.023875\n","425it [00:05, 51.36it/s]Train epoch: 149 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.023685\n","448it [00:06, 46.46it/s]Train epoch: 149 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.023884\n","473it [00:06, 41.46it/s]Train epoch: 149 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.023064\n","499it [00:07, 34.41it/s]Train epoch: 149 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.025293\n","505it [00:07, 65.38it/s]\n","epoch loss: 0.023248089381060242\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 482.11it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3626, 0.5719, 0.4570, 0.5080, 0.8659\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3965, 0.6371, 0.5121, 0.5678, 0.8870\n","rec_at_5: 0.5535\n","prec_at_5: 0.5630\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 150\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 150 [batch #0, batch_size 16, seq length 212]\tLoss: 0.045708\n","15it [00:00, 141.66it/s]Train epoch: 150 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017634\n","45it [00:00, 124.99it/s]Train epoch: 150 [batch #50, batch_size 16, seq length 709]\tLoss: 0.020636\n","71it [00:00, 115.36it/s]Train epoch: 150 [batch #75, batch_size 16, seq length 806]\tLoss: 0.023280\n","95it [00:00, 113.13it/s]Train epoch: 150 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021472\n","118it [00:01, 101.02it/s]Train epoch: 150 [batch #125, batch_size 16, seq length 978]\tLoss: 0.021174\n","149it [00:01, 90.80it/s]Train epoch: 150 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.027215\n","169it [00:01, 88.37it/s]Train epoch: 150 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.020932\n","197it [00:01, 83.97it/s]Train epoch: 150 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.023553\n","224it [00:02, 75.13it/s]Train epoch: 150 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.023587\n","249it [00:02, 74.56it/s]Train epoch: 150 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.022951\n","273it [00:03, 72.33it/s]Train epoch: 150 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.025068\n","296it [00:03, 64.91it/s]Train epoch: 150 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.025071\n","319it [00:03, 64.60it/s]Train epoch: 150 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.021480\n","347it [00:04, 60.86it/s]Train epoch: 150 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.024768\n","375it [00:04, 60.08it/s]Train epoch: 150 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.023836\n","400it [00:05, 54.41it/s]Train epoch: 150 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.025410\n","424it [00:05, 50.01it/s]Train epoch: 150 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.023089\n","446it [00:06, 46.67it/s]Train epoch: 150 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.023046\n","471it [00:06, 43.24it/s]Train epoch: 150 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.022173\n","498it [00:07, 35.98it/s]Train epoch: 150 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.024601\n","505it [00:07, 65.37it/s]\n","epoch loss: 0.022655153127460935\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 473.41it/s]\n","Finish save rediction by checkpoint  150\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3600, 0.5711, 0.4529, 0.5051, 0.8656\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3939, 0.6381, 0.5073, 0.5652, 0.8863\n","rec_at_5: 0.5522\n","prec_at_5: 0.5619\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 151\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 151 [batch #0, batch_size 16, seq length 212]\tLoss: 0.029355\n","16it [00:00, 158.57it/s]Train epoch: 151 [batch #25, batch_size 16, seq length 571]\tLoss: 0.016837\n","47it [00:00, 127.28it/s]Train epoch: 151 [batch #50, batch_size 16, seq length 709]\tLoss: 0.019063\n","73it [00:00, 122.96it/s]Train epoch: 151 [batch #75, batch_size 16, seq length 806]\tLoss: 0.026143\n","98it [00:00, 110.18it/s]Train epoch: 151 [batch #100, batch_size 16, seq length 892]\tLoss: 0.022791\n","121it [00:01, 101.73it/s]Train epoch: 151 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020658\n","143it [00:01, 93.79it/s] Train epoch: 151 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.024806\n","173it [00:01, 87.02it/s]Train epoch: 151 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.021344\n","200it [00:02, 80.21it/s]Train epoch: 151 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.021523\n","218it [00:02, 82.53it/s]Train epoch: 151 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.024392\n","243it [00:02, 72.92it/s]Train epoch: 151 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.023346\n","275it [00:03, 70.79it/s]Train epoch: 151 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.023196\n","299it [00:03, 68.48it/s]Train epoch: 151 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.023140\n","321it [00:03, 64.28it/s]Train epoch: 151 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.022972\n","349it [00:04, 61.73it/s]Train epoch: 151 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.024702\n","370it [00:04, 59.55it/s]Train epoch: 151 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.023901\n","395it [00:05, 52.40it/s]Train epoch: 151 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.022648\n","425it [00:05, 50.48it/s]Train epoch: 151 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.022131\n","448it [00:06, 47.13it/s]Train epoch: 151 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.021685\n","473it [00:06, 40.88it/s]Train epoch: 151 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.022598\n","500it [00:07, 34.87it/s]Train epoch: 151 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.022293\n","505it [00:07, 65.66it/s]\n","epoch loss: 0.022155753750639905\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 464.11it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3617, 0.5716, 0.4555, 0.5070, 0.8654\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3943, 0.6361, 0.5092, 0.5656, 0.8862\n","rec_at_5: 0.5506\n","prec_at_5: 0.5596\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 152\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 152 [batch #0, batch_size 16, seq length 212]\tLoss: 0.037912\n","17it [00:00, 159.92it/s]Train epoch: 152 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017135\n","48it [00:00, 125.45it/s]Train epoch: 152 [batch #50, batch_size 16, seq length 709]\tLoss: 0.020253\n","74it [00:00, 118.80it/s]Train epoch: 152 [batch #75, batch_size 16, seq length 806]\tLoss: 0.022868\n","98it [00:00, 109.71it/s]Train epoch: 152 [batch #100, batch_size 16, seq length 892]\tLoss: 0.022353\n","121it [00:01, 101.84it/s]Train epoch: 152 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020256\n","142it [00:01, 94.17it/s]Train epoch: 152 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.023788\n","172it [00:01, 88.95it/s]Train epoch: 152 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.022757\n","199it [00:01, 83.35it/s]Train epoch: 152 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.020299\n","217it [00:02, 78.92it/s]Train epoch: 152 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.020260\n","244it [00:02, 75.90it/s]Train epoch: 152 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.023341\n","269it [00:02, 71.19it/s]Train epoch: 152 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.021214\n","293it [00:03, 69.13it/s]Train epoch: 152 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.023173\n","323it [00:03, 66.31it/s]Train epoch: 152 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.022319\n","344it [00:04, 64.06it/s]Train epoch: 152 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.023118\n","372it [00:04, 60.36it/s]Train epoch: 152 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.024222\n","398it [00:05, 53.80it/s]Train epoch: 152 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.021806\n","422it [00:05, 52.19it/s]Train epoch: 152 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.022074\n","450it [00:06, 45.54it/s]Train epoch: 152 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.018742\n","475it [00:06, 42.68it/s]Train epoch: 152 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.022871\n","497it [00:07, 36.22it/s]Train epoch: 152 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.023949\n","505it [00:07, 66.24it/s]\n","epoch loss: 0.02172438555540019\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 484.91it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3616, 0.5714, 0.4551, 0.5066, 0.8653\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3937, 0.6366, 0.5078, 0.5650, 0.8857\n","rec_at_5: 0.5495\n","prec_at_5: 0.5605\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 153\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 153 [batch #0, batch_size 16, seq length 212]\tLoss: 0.031894\n","16it [00:00, 153.22it/s]Train epoch: 153 [batch #25, batch_size 16, seq length 571]\tLoss: 0.016994\n","47it [00:00, 129.30it/s]Train epoch: 153 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016613\n","74it [00:00, 124.84it/s]Train epoch: 153 [batch #75, batch_size 16, seq length 806]\tLoss: 0.022562\n","99it [00:00, 110.38it/s]Train epoch: 153 [batch #100, batch_size 16, seq length 892]\tLoss: 0.023549\n","123it [00:01, 100.50it/s]Train epoch: 153 [batch #125, batch_size 16, seq length 978]\tLoss: 0.021087\n","145it [00:01, 100.06it/s]Train epoch: 153 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.024989\n","166it [00:01, 92.66it/s]Train epoch: 153 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.018320\n","194it [00:01, 85.29it/s]Train epoch: 153 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019586\n","221it [00:02, 81.61it/s]Train epoch: 153 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.020580\n","246it [00:02, 72.21it/s]Train epoch: 153 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.022058\n","270it [00:02, 70.57it/s]Train epoch: 153 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.022098\n","293it [00:03, 68.60it/s]Train epoch: 153 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.024407\n","324it [00:03, 65.98it/s]Train epoch: 153 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.018966\n","345it [00:04, 60.40it/s]Train epoch: 153 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022246\n","373it [00:04, 57.96it/s]Train epoch: 153 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.023080\n","398it [00:05, 55.27it/s]Train epoch: 153 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.020511\n","422it [00:05, 51.52it/s]Train epoch: 153 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.019684\n","449it [00:06, 43.80it/s]Train epoch: 153 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.020861\n","474it [00:06, 42.39it/s]Train epoch: 153 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.021969\n","497it [00:07, 37.20it/s]Train epoch: 153 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.021121\n","505it [00:07, 65.96it/s]\n","epoch loss: 0.02110663590224294\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.84it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3635, 0.5737, 0.4580, 0.5094, 0.8651\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3959, 0.6358, 0.5120, 0.5672, 0.8864\n","rec_at_5: 0.5500\n","prec_at_5: 0.5607\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 154\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 154 [batch #0, batch_size 16, seq length 212]\tLoss: 0.044457\n","16it [00:00, 157.71it/s]Train epoch: 154 [batch #25, batch_size 16, seq length 571]\tLoss: 0.016938\n","48it [00:00, 129.15it/s]Train epoch: 154 [batch #50, batch_size 16, seq length 709]\tLoss: 0.017449\n","75it [00:00, 120.33it/s]Train epoch: 154 [batch #75, batch_size 16, seq length 806]\tLoss: 0.022882\n","100it [00:00, 110.90it/s]Train epoch: 154 [batch #100, batch_size 16, seq length 892]\tLoss: 0.023127\n","124it [00:01, 102.32it/s]Train epoch: 154 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020442\n","146it [00:01, 100.31it/s]Train epoch: 154 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.023104\n","167it [00:01, 90.53it/s]Train epoch: 154 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.019895\n","196it [00:01, 86.52it/s]Train epoch: 154 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019160\n","223it [00:02, 82.68it/s]Train epoch: 154 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.020321\n","248it [00:02, 71.32it/s]Train epoch: 154 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.022148\n","272it [00:02, 71.60it/s]Train epoch: 154 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.022031\n","296it [00:03, 69.50it/s]Train epoch: 154 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.022594\n","320it [00:03, 65.78it/s]Train epoch: 154 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.020603\n","348it [00:04, 61.63it/s]Train epoch: 154 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.021983\n","374it [00:04, 55.32it/s]Train epoch: 154 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.022093\n","399it [00:05, 56.09it/s]Train epoch: 154 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.019855\n","423it [00:05, 51.19it/s]Train epoch: 154 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.019619\n","449it [00:06, 43.72it/s]Train epoch: 154 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.021460\n","474it [00:06, 40.24it/s]Train epoch: 154 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.021912\n","500it [00:07, 34.79it/s]Train epoch: 154 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.023912\n","505it [00:07, 65.63it/s]\n","epoch loss: 0.020754873084070365\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 479.66it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3635, 0.5731, 0.4582, 0.5093, 0.8647\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3950, 0.6355, 0.5107, 0.5663, 0.8858\n","rec_at_5: 0.5510\n","prec_at_5: 0.5610\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 155\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 155 [batch #0, batch_size 16, seq length 212]\tLoss: 0.030109\n","16it [00:00, 157.96it/s]Train epoch: 155 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015180\n","48it [00:00, 123.69it/s]Train epoch: 155 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016596\n","74it [00:00, 115.00it/s]Train epoch: 155 [batch #75, batch_size 16, seq length 806]\tLoss: 0.021421\n","98it [00:00, 104.92it/s]Train epoch: 155 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021204\n","120it [00:01, 99.15it/s] Train epoch: 155 [batch #125, batch_size 16, seq length 978]\tLoss: 0.022155\n","142it [00:01, 98.09it/s]Train epoch: 155 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.022466\n","172it [00:01, 92.01it/s]Train epoch: 155 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.018302\n","192it [00:01, 87.44it/s]Train epoch: 155 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019807\n","219it [00:02, 81.98it/s]Train epoch: 155 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.019372\n","245it [00:02, 78.28it/s]Train epoch: 155 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.019958\n","269it [00:02, 69.02it/s]Train epoch: 155 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.022216\n","293it [00:03, 67.52it/s]Train epoch: 155 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.020226\n","322it [00:03, 64.56it/s]Train epoch: 155 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.019757\n","350it [00:04, 60.19it/s]Train epoch: 155 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022452\n","371it [00:04, 58.81it/s]Train epoch: 155 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.020444\n","396it [00:05, 57.17it/s]Train epoch: 155 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.019772\n","420it [00:05, 50.89it/s]Train epoch: 155 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.020855\n","449it [00:06, 47.24it/s]Train epoch: 155 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.018467\n","474it [00:06, 42.40it/s]Train epoch: 155 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.019774\n","500it [00:07, 36.12it/s]Train epoch: 155 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.021010\n","505it [00:07, 66.15it/s]\n","epoch loss: 0.020030920539341756\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 460.12it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3618, 0.5711, 0.4567, 0.5075, 0.8648\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3945, 0.6342, 0.5107, 0.5658, 0.8859\n","rec_at_5: 0.5509\n","prec_at_5: 0.5615\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 156\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 156 [batch #0, batch_size 16, seq length 212]\tLoss: 0.029672\n","22it [00:00, 99.40it/s] Train epoch: 156 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015100\n","50it [00:00, 76.16it/s]Train epoch: 156 [batch #50, batch_size 16, seq length 709]\tLoss: 0.018048\n","74it [00:00, 72.19it/s]Train epoch: 156 [batch #75, batch_size 16, seq length 806]\tLoss: 0.023513\n","96it [00:01, 65.58it/s]Train epoch: 156 [batch #100, batch_size 16, seq length 892]\tLoss: 0.020606\n","124it [00:01, 61.60it/s]Train epoch: 156 [batch #125, batch_size 16, seq length 978]\tLoss: 0.021278\n","145it [00:02, 57.92it/s]Train epoch: 156 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.020267\n","175it [00:02, 55.62it/s]Train epoch: 156 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.019155\n","194it [00:03, 57.55it/s]Train epoch: 156 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.020071\n","218it [00:03, 69.70it/s]Train epoch: 156 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.019536\n","243it [00:03, 72.29it/s]Train epoch: 156 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.019455\n","275it [00:04, 71.90it/s]Train epoch: 156 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.020050\n","297it [00:04, 64.49it/s]Train epoch: 156 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.021976\n","320it [00:04, 65.72it/s]Train epoch: 156 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.018564\n","349it [00:05, 59.09it/s]Train epoch: 156 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022044\n","370it [00:05, 56.44it/s]Train epoch: 156 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.021078\n","396it [00:06, 55.10it/s]Train epoch: 156 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.019673\n","420it [00:06, 51.39it/s]Train epoch: 156 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.018168\n","447it [00:07, 43.09it/s]Train epoch: 156 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.018715\n","472it [00:07, 43.65it/s]Train epoch: 156 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.018074\n","500it [00:08, 36.59it/s]Train epoch: 156 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.020246\n","505it [00:08, 57.56it/s]\n","epoch loss: 0.019639275067181574\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 478.30it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3622, 0.5710, 0.4569, 0.5076, 0.8644\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3941, 0.6338, 0.5103, 0.5654, 0.8860\n","rec_at_5: 0.5502\n","prec_at_5: 0.5602\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 157\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 157 [batch #0, batch_size 16, seq length 212]\tLoss: 0.026442\n","16it [00:00, 158.50it/s]Train epoch: 157 [batch #25, batch_size 16, seq length 571]\tLoss: 0.014672\n","48it [00:00, 127.90it/s]Train epoch: 157 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016672\n","75it [00:00, 124.50it/s]Train epoch: 157 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018736\n","100it [00:00, 107.46it/s]Train epoch: 157 [batch #100, batch_size 16, seq length 892]\tLoss: 0.018538\n","122it [00:01, 100.44it/s]Train epoch: 157 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020820\n","143it [00:01, 96.04it/s]Train epoch: 157 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.021451\n","173it [00:01, 92.24it/s]Train epoch: 157 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017437\n","192it [00:01, 85.62it/s]Train epoch: 157 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019490\n","219it [00:02, 82.05it/s]Train epoch: 157 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.018209\n","246it [00:02, 79.58it/s]Train epoch: 157 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.019942\n","271it [00:02, 67.30it/s]Train epoch: 157 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.019907\n","295it [00:03, 67.12it/s]Train epoch: 157 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.020219\n","319it [00:03, 65.39it/s]Train epoch: 157 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.019314\n","347it [00:04, 62.22it/s]Train epoch: 157 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.021754\n","375it [00:04, 56.46it/s]Train epoch: 157 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.020150\n","399it [00:05, 54.49it/s]Train epoch: 157 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.020288\n","423it [00:05, 53.07it/s]Train epoch: 157 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.017921\n","450it [00:06, 46.05it/s]Train epoch: 157 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.018788\n","475it [00:06, 42.40it/s]Train epoch: 157 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.018064\n","497it [00:07, 33.32it/s]Train epoch: 157 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.019392\n","505it [00:07, 65.75it/s]\n","epoch loss: 0.019063960773741256\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 483.98it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3595, 0.5730, 0.4520, 0.5054, 0.8646\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3921, 0.6360, 0.5055, 0.5633, 0.8857\n","rec_at_5: 0.5502\n","prec_at_5: 0.5598\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 158\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 158 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035374\n","16it [00:00, 155.95it/s]Train epoch: 158 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015196\n","47it [00:00, 127.35it/s]Train epoch: 158 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016251\n","74it [00:00, 123.10it/s]Train epoch: 158 [batch #75, batch_size 16, seq length 806]\tLoss: 0.020281\n","99it [00:00, 109.86it/s]Train epoch: 158 [batch #100, batch_size 16, seq length 892]\tLoss: 0.019414\n","123it [00:01, 102.66it/s]Train epoch: 158 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017692\n","145it [00:01, 99.56it/s] Train epoch: 158 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.021086\n","175it [00:01, 86.38it/s]Train epoch: 158 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017011\n","193it [00:01, 81.55it/s]Train epoch: 158 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.017634\n","220it [00:02, 80.83it/s]Train epoch: 158 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.017287\n","246it [00:02, 75.19it/s]Train epoch: 158 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.018218\n","270it [00:02, 71.50it/s]Train epoch: 158 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.019860\n","294it [00:03, 66.62it/s]Train epoch: 158 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.021481\n","323it [00:03, 64.69it/s]Train epoch: 158 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.018206\n","344it [00:04, 64.02it/s]Train epoch: 158 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022173\n","372it [00:04, 58.77it/s]Train epoch: 158 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.017665\n","396it [00:05, 55.70it/s]Train epoch: 158 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.018144\n","420it [00:05, 53.84it/s]Train epoch: 158 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.017249\n","448it [00:06, 45.87it/s]Train epoch: 158 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.017212\n","473it [00:06, 41.99it/s]Train epoch: 158 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.018378\n","500it [00:07, 33.83it/s]Train epoch: 158 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.021042\n","505it [00:07, 65.58it/s]\n","epoch loss: 0.018453088626818787\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 472.03it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3621, 0.5711, 0.4568, 0.5076, 0.8644\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3948, 0.6344, 0.5111, 0.5661, 0.8863\n","rec_at_5: 0.5504\n","prec_at_5: 0.5606\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 159\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 159 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025760\n","17it [00:00, 158.11it/s]Train epoch: 159 [batch #25, batch_size 16, seq length 571]\tLoss: 0.014280\n","48it [00:00, 126.82it/s]Train epoch: 159 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016149\n","74it [00:00, 113.97it/s]Train epoch: 159 [batch #75, batch_size 16, seq length 806]\tLoss: 0.020264\n","98it [00:00, 105.01it/s]Train epoch: 159 [batch #100, batch_size 16, seq length 892]\tLoss: 0.018568\n","120it [00:01, 105.52it/s]Train epoch: 159 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017903\n","141it [00:01, 97.50it/s]Train epoch: 159 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.020803\n","171it [00:01, 89.37it/s]Train epoch: 159 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017005\n","198it [00:01, 79.51it/s]Train epoch: 159 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.018442\n","225it [00:02, 76.83it/s]Train epoch: 159 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016617\n","249it [00:02, 73.27it/s]Train epoch: 159 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.018097\n","273it [00:03, 69.43it/s]Train epoch: 159 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.019522\n","297it [00:03, 68.17it/s]Train epoch: 159 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.018706\n","320it [00:03, 66.04it/s]Train epoch: 159 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.018945\n","348it [00:04, 61.18it/s]Train epoch: 159 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.017992\n","369it [00:04, 60.27it/s]Train epoch: 159 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.019584\n","400it [00:05, 55.30it/s]Train epoch: 159 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.016790\n","424it [00:05, 53.09it/s]Train epoch: 159 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.016235\n","450it [00:06, 46.96it/s]Train epoch: 159 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.021022\n","475it [00:06, 42.13it/s]Train epoch: 159 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.016643\n","500it [00:07, 33.69it/s]Train epoch: 159 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.019363\n","505it [00:07, 65.39it/s]\n","epoch loss: 0.017850793167340135\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 481.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3614, 0.5712, 0.4556, 0.5069, 0.8642\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3936, 0.6337, 0.5094, 0.5648, 0.8862\n","rec_at_5: 0.5493\n","prec_at_5: 0.5596\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 160\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 160 [batch #0, batch_size 16, seq length 212]\tLoss: 0.036796\n","16it [00:00, 154.22it/s]Train epoch: 160 [batch #25, batch_size 16, seq length 571]\tLoss: 0.014544\n","47it [00:00, 128.53it/s]Train epoch: 160 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016525\n","74it [00:00, 119.66it/s]Train epoch: 160 [batch #75, batch_size 16, seq length 806]\tLoss: 0.021216\n","99it [00:00, 109.29it/s]Train epoch: 160 [batch #100, batch_size 16, seq length 892]\tLoss: 0.019250\n","122it [00:01, 101.90it/s]Train epoch: 160 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017306\n","143it [00:01, 93.71it/s]Train epoch: 160 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.020398\n","172it [00:01, 87.01it/s]Train epoch: 160 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017334\n","200it [00:02, 81.28it/s]Train epoch: 160 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.018362\n","218it [00:02, 81.70it/s]Train epoch: 160 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016972\n","243it [00:02, 72.21it/s]Train epoch: 160 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.021474\n","275it [00:03, 71.51it/s]Train epoch: 160 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.018257\n","299it [00:03, 70.02it/s]Train epoch: 160 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.019771\n","321it [00:03, 65.64it/s]Train epoch: 160 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015236\n","349it [00:04, 60.49it/s]Train epoch: 160 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.019184\n","374it [00:04, 52.53it/s]Train epoch: 160 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.019283\n","398it [00:05, 55.70it/s]Train epoch: 160 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.017186\n","422it [00:05, 49.86it/s]Train epoch: 160 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.017309\n","449it [00:06, 46.55it/s]Train epoch: 160 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.018730\n","474it [00:06, 43.04it/s]Train epoch: 160 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.018635\n","497it [00:07, 37.46it/s]Train epoch: 160 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.018577\n","505it [00:07, 65.29it/s]\n","epoch loss: 0.017712567310485216\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 481.47it/s]\n","Finish save rediction by checkpoint  160\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3598, 0.5715, 0.4523, 0.5050, 0.8640\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3930, 0.6350, 0.5077, 0.5643, 0.8854\n","rec_at_5: 0.5496\n","prec_at_5: 0.5597\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 161\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 161 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025509\n","16it [00:00, 152.06it/s]Train epoch: 161 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013962\n","46it [00:00, 122.13it/s]Train epoch: 161 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014805\n","71it [00:00, 114.73it/s]Train epoch: 161 [batch #75, batch_size 16, seq length 806]\tLoss: 0.016582\n","95it [00:00, 106.66it/s]Train epoch: 161 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016552\n","117it [00:01, 100.63it/s]Train epoch: 161 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017320\n","149it [00:01, 92.58it/s]Train epoch: 161 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.019575\n","169it [00:01, 88.69it/s]Train epoch: 161 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015010\n","196it [00:02, 80.42it/s]Train epoch: 161 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015763\n","223it [00:02, 76.47it/s]Train epoch: 161 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.015724\n","248it [00:02, 75.58it/s]Train epoch: 161 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.018616\n","272it [00:03, 72.74it/s]Train epoch: 161 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.019333\n","296it [00:03, 68.67it/s]Train epoch: 161 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.018776\n","324it [00:03, 61.91it/s]Train epoch: 161 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.017277\n","345it [00:04, 61.43it/s]Train epoch: 161 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.018270\n","373it [00:04, 57.28it/s]Train epoch: 161 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.016521\n","398it [00:05, 53.31it/s]Train epoch: 161 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.016831\n","422it [00:05, 51.81it/s]Train epoch: 161 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015542\n","449it [00:06, 46.72it/s]Train epoch: 161 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.016862\n","474it [00:06, 42.41it/s]Train epoch: 161 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.016399\n","497it [00:07, 34.22it/s]Train epoch: 161 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017418\n","505it [00:07, 65.54it/s]\n","epoch loss: 0.01687611636664202\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 469.22it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3592, 0.5696, 0.4521, 0.5041, 0.8638\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3922, 0.6331, 0.5075, 0.5634, 0.8859\n","rec_at_5: 0.5466\n","prec_at_5: 0.5580\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 162\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 162 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035367\n","16it [00:00, 152.75it/s]Train epoch: 162 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013433\n","47it [00:00, 126.65it/s]Train epoch: 162 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013819\n","72it [00:00, 115.53it/s]Train epoch: 162 [batch #75, batch_size 16, seq length 806]\tLoss: 0.017382\n","96it [00:00, 106.68it/s]Train epoch: 162 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017169\n","118it [00:01, 97.00it/s] Train epoch: 162 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016830\n","149it [00:01, 93.21it/s]Train epoch: 162 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017676\n","169it [00:01, 88.68it/s]Train epoch: 162 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.016755\n","196it [00:01, 84.15it/s]Train epoch: 162 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.018885\n","223it [00:02, 80.94it/s]Train epoch: 162 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.015086\n","248it [00:02, 71.05it/s]Train epoch: 162 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.016963\n","272it [00:03, 69.70it/s]Train epoch: 162 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.018104\n","294it [00:03, 66.77it/s]Train epoch: 162 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.017163\n","323it [00:03, 62.36it/s]Train epoch: 162 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.016647\n","344it [00:04, 59.62it/s]Train epoch: 162 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016635\n","372it [00:04, 59.63it/s]Train epoch: 162 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.016879\n","397it [00:05, 54.29it/s]Train epoch: 162 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.016844\n","421it [00:05, 50.65it/s]Train epoch: 162 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.017071\n","449it [00:06, 45.02it/s]Train epoch: 162 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015185\n","474it [00:06, 40.08it/s]Train epoch: 162 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.016394\n","500it [00:07, 35.23it/s]Train epoch: 162 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017511\n","505it [00:07, 64.75it/s]\n","epoch loss: 0.016585792472014332\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 481.84it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3611, 0.5691, 0.4559, 0.5062, 0.8636\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3932, 0.6313, 0.5105, 0.5645, 0.8858\n","rec_at_5: 0.5479\n","prec_at_5: 0.5594\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 163\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 163 [batch #0, batch_size 16, seq length 212]\tLoss: 0.026550\n","16it [00:00, 153.14it/s]Train epoch: 163 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013333\n","47it [00:00, 125.72it/s]Train epoch: 163 [batch #50, batch_size 16, seq length 709]\tLoss: 0.018482\n","73it [00:00, 121.13it/s]Train epoch: 163 [batch #75, batch_size 16, seq length 806]\tLoss: 0.020574\n","98it [00:00, 109.48it/s]Train epoch: 163 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017581\n","121it [00:01, 102.69it/s]Train epoch: 163 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017586\n","142it [00:01, 96.14it/s]Train epoch: 163 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.019865\n","172it [00:01, 89.72it/s]Train epoch: 163 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015327\n","200it [00:02, 81.15it/s]Train epoch: 163 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014794\n","218it [00:02, 79.15it/s]Train epoch: 163 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016802\n","243it [00:02, 74.47it/s]Train epoch: 163 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.016742\n","275it [00:03, 70.17it/s]Train epoch: 163 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016799\n","296it [00:03, 48.01it/s]Train epoch: 163 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.016354\n","322it [00:04, 40.49it/s]Train epoch: 163 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.016818\n","348it [00:05, 33.10it/s]Train epoch: 163 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.017016\n","375it [00:05, 28.22it/s]Train epoch: 163 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.018204\n","397it [00:06, 28.66it/s]Train epoch: 163 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.015543\n","424it [00:07, 27.48it/s]Train epoch: 163 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015694\n","448it [00:08, 21.93it/s]Train epoch: 163 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015980\n","475it [00:10, 24.50it/s]Train epoch: 163 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.017700\n","499it [00:10, 29.68it/s]Train epoch: 163 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017562\n","505it [00:11, 45.58it/s]\n","epoch loss: 0.01651291894325341\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 483.68it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3577, 0.5690, 0.4508, 0.5031, 0.8633\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3906, 0.6317, 0.5059, 0.5618, 0.8853\n","rec_at_5: 0.5475\n","prec_at_5: 0.5577\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 164\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 164 [batch #0, batch_size 16, seq length 212]\tLoss: 0.028326\n","16it [00:00, 157.90it/s]Train epoch: 164 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013036\n","47it [00:00, 129.86it/s]Train epoch: 164 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014786\n","74it [00:00, 119.04it/s]Train epoch: 164 [batch #75, batch_size 16, seq length 806]\tLoss: 0.019471\n","98it [00:00, 103.59it/s]Train epoch: 164 [batch #100, batch_size 16, seq length 892]\tLoss: 0.015649\n","120it [00:01, 105.30it/s]Train epoch: 164 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015944\n","141it [00:01, 98.50it/s]Train epoch: 164 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017985\n","171it [00:01, 84.82it/s]Train epoch: 164 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015901\n","198it [00:02, 78.78it/s]Train epoch: 164 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015175\n","225it [00:02, 76.28it/s]Train epoch: 164 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.015698\n","250it [00:02, 72.27it/s]Train epoch: 164 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015930\n","274it [00:03, 71.43it/s]Train epoch: 164 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.015996\n","298it [00:03, 67.84it/s]Train epoch: 164 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.017569\n","320it [00:03, 64.67it/s]Train epoch: 164 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015352\n","348it [00:04, 61.28it/s]Train epoch: 164 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016878\n","374it [00:04, 58.13it/s]Train epoch: 164 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.016058\n","398it [00:05, 52.82it/s]Train epoch: 164 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.015630\n","422it [00:05, 51.23it/s]Train epoch: 164 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015415\n","449it [00:06, 47.16it/s]Train epoch: 164 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015371\n","474it [00:06, 38.41it/s]Train epoch: 164 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014804\n","497it [00:07, 37.27it/s]Train epoch: 164 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017546\n","505it [00:07, 65.37it/s]\n","epoch loss: 0.01572090473595435\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.54it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3594, 0.5695, 0.4531, 0.5046, 0.8634\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3913, 0.6328, 0.5063, 0.5625, 0.8852\n","rec_at_5: 0.5475\n","prec_at_5: 0.5587\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 165\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 165 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035675\n","16it [00:00, 156.54it/s]Train epoch: 165 [batch #25, batch_size 16, seq length 571]\tLoss: 0.012540\n","47it [00:00, 123.49it/s]Train epoch: 165 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014415\n","73it [00:00, 121.27it/s]Train epoch: 165 [batch #75, batch_size 16, seq length 806]\tLoss: 0.016443\n","98it [00:00, 105.59it/s]Train epoch: 165 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016875\n","120it [00:01, 103.44it/s]Train epoch: 165 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015466\n","141it [00:01, 93.91it/s]Train epoch: 165 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017734\n","171it [00:01, 90.41it/s]Train epoch: 165 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015541\n","199it [00:01, 86.16it/s]Train epoch: 165 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015487\n","225it [00:02, 75.41it/s]Train epoch: 165 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.014223\n","242it [00:02, 73.29it/s]Train epoch: 165 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.016822\n","275it [00:03, 71.42it/s]Train epoch: 165 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.017240\n","298it [00:03, 68.50it/s]Train epoch: 165 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.018246\n","320it [00:03, 63.90it/s]Train epoch: 165 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.014844\n","348it [00:04, 61.55it/s]Train epoch: 165 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.015629\n","369it [00:04, 57.01it/s]Train epoch: 165 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.017369\n","400it [00:05, 55.35it/s]Train epoch: 165 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.015237\n","424it [00:05, 50.57it/s]Train epoch: 165 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013814\n","447it [00:06, 47.24it/s]Train epoch: 165 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.013808\n","472it [00:06, 42.39it/s]Train epoch: 165 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.015496\n","499it [00:07, 33.87it/s]Train epoch: 165 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017688\n","505it [00:07, 65.54it/s]\n","epoch loss: 0.015336219386524973\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 483.94it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3601, 0.5689, 0.4555, 0.5059, 0.8633\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3930, 0.6291, 0.5115, 0.5642, 0.8859\n","rec_at_5: 0.5471\n","prec_at_5: 0.5570\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 166\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 166 [batch #0, batch_size 16, seq length 212]\tLoss: 0.028309\n","16it [00:00, 159.06it/s]Train epoch: 166 [batch #25, batch_size 16, seq length 571]\tLoss: 0.012413\n","47it [00:00, 125.85it/s]Train epoch: 166 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013794\n","73it [00:00, 123.50it/s]Train epoch: 166 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018202\n","98it [00:00, 112.12it/s]Train epoch: 166 [batch #100, batch_size 16, seq length 892]\tLoss: 0.015739\n","122it [00:01, 102.49it/s]Train epoch: 166 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016071\n","144it [00:01, 94.23it/s] Train epoch: 166 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017206\n","174it [00:01, 87.65it/s]Train epoch: 166 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014596\n","193it [00:01, 88.46it/s]Train epoch: 166 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014144\n","220it [00:02, 81.28it/s]Train epoch: 166 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016555\n","246it [00:02, 74.33it/s]Train epoch: 166 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.014727\n","270it [00:02, 70.12it/s]Train epoch: 166 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.017216\n","294it [00:03, 66.35it/s]Train epoch: 166 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.017046\n","324it [00:03, 62.44it/s]Train epoch: 166 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015589\n","345it [00:04, 60.57it/s]Train epoch: 166 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016110\n","372it [00:04, 58.20it/s]Train epoch: 166 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.014180\n","397it [00:05, 54.30it/s]Train epoch: 166 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.014577\n","421it [00:05, 50.07it/s]Train epoch: 166 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011672\n","449it [00:06, 44.83it/s]Train epoch: 166 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014053\n","474it [00:06, 41.94it/s]Train epoch: 166 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014060\n","497it [00:07, 37.42it/s]Train epoch: 166 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.016656\n","505it [00:07, 65.73it/s]\n","epoch loss: 0.01514861652498493\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 472.93it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3608, 0.5695, 0.4562, 0.5066, 0.8632\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3926, 0.6303, 0.5101, 0.5639, 0.8854\n","rec_at_5: 0.5471\n","prec_at_5: 0.5577\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 167\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 167 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035870\n","16it [00:00, 159.10it/s]Train epoch: 167 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010569\n","48it [00:00, 126.07it/s]Train epoch: 167 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011631\n","75it [00:00, 121.28it/s]Train epoch: 167 [batch #75, batch_size 16, seq length 806]\tLoss: 0.016819\n","100it [00:00, 109.29it/s]Train epoch: 167 [batch #100, batch_size 16, seq length 892]\tLoss: 0.015397\n","123it [00:01, 100.95it/s]Train epoch: 167 [batch #125, batch_size 16, seq length 978]\tLoss: 0.013741\n","144it [00:01, 95.48it/s]Train epoch: 167 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.016318\n","174it [00:01, 90.55it/s]Train epoch: 167 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014043\n","193it [00:01, 83.88it/s]Train epoch: 167 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014573\n","219it [00:02, 74.55it/s]Train epoch: 167 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013868\n","244it [00:02, 74.84it/s]Train epoch: 167 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.013537\n","275it [00:03, 68.46it/s]Train epoch: 167 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016487\n","297it [00:03, 67.37it/s]Train epoch: 167 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.014818\n","325it [00:03, 61.76it/s]Train epoch: 167 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015610\n","346it [00:04, 58.72it/s]Train epoch: 167 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.017838\n","373it [00:04, 57.62it/s]Train epoch: 167 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.014620\n","398it [00:05, 52.28it/s]Train epoch: 167 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.015299\n","422it [00:05, 50.92it/s]Train epoch: 167 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013909\n","450it [00:06, 44.29it/s]Train epoch: 167 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.013139\n","475it [00:06, 42.89it/s]Train epoch: 167 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014438\n","497it [00:07, 33.84it/s]Train epoch: 167 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.015300\n","505it [00:07, 64.71it/s]\n","epoch loss: 0.014660613659750854\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 476.50it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3594, 0.5681, 0.4550, 0.5053, 0.8630\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3917, 0.6286, 0.5096, 0.5629, 0.8854\n","rec_at_5: 0.5496\n","prec_at_5: 0.5582\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 168\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 168 [batch #0, batch_size 16, seq length 212]\tLoss: 0.024597\n","16it [00:00, 153.53it/s]Train epoch: 168 [batch #25, batch_size 16, seq length 571]\tLoss: 0.012709\n","47it [00:00, 128.73it/s]Train epoch: 168 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014424\n","74it [00:00, 123.58it/s]Train epoch: 168 [batch #75, batch_size 16, seq length 806]\tLoss: 0.017741\n","99it [00:00, 108.56it/s]Train epoch: 168 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014258\n","122it [00:01, 102.55it/s]Train epoch: 168 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015384\n","143it [00:01, 98.93it/s]Train epoch: 168 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014294\n","173it [00:01, 91.70it/s]Train epoch: 168 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014807\n","192it [00:01, 84.95it/s]Train epoch: 168 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.013654\n","219it [00:02, 83.31it/s]Train epoch: 168 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012132\n","246it [00:02, 75.94it/s]Train epoch: 168 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.013510\n","270it [00:02, 70.62it/s]Train epoch: 168 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.014567\n","294it [00:03, 67.49it/s]Train epoch: 168 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013758\n","323it [00:03, 62.04it/s]Train epoch: 168 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013637\n","344it [00:04, 62.57it/s]Train epoch: 168 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.015751\n","372it [00:04, 61.35it/s]Train epoch: 168 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.014095\n","397it [00:05, 55.23it/s]Train epoch: 168 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.014039\n","421it [00:05, 54.09it/s]Train epoch: 168 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.012804\n","449it [00:06, 46.44it/s]Train epoch: 168 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.013536\n","474it [00:06, 40.44it/s]Train epoch: 168 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014574\n","499it [00:07, 33.89it/s]Train epoch: 168 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013162\n","505it [00:07, 65.95it/s]\n","epoch loss: 0.014007906117107682\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 476.57it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3582, 0.5701, 0.4515, 0.5039, 0.8628\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3907, 0.6309, 0.5065, 0.5619, 0.8852\n","rec_at_5: 0.5465\n","prec_at_5: 0.5573\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 169\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 169 [batch #0, batch_size 16, seq length 212]\tLoss: 0.033924\n","16it [00:00, 158.89it/s]Train epoch: 169 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010071\n","47it [00:00, 129.14it/s]Train epoch: 169 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013721\n","74it [00:00, 113.68it/s]Train epoch: 169 [batch #75, batch_size 16, seq length 806]\tLoss: 0.016551\n","98it [00:00, 105.47it/s]Train epoch: 169 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014067\n","120it [00:01, 99.03it/s] Train epoch: 169 [batch #125, batch_size 16, seq length 978]\tLoss: 0.014534\n","141it [00:01, 93.03it/s]Train epoch: 169 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013751\n","170it [00:01, 87.21it/s]Train epoch: 169 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.013970\n","198it [00:01, 84.59it/s]Train epoch: 169 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.013538\n","225it [00:02, 81.38it/s]Train epoch: 169 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.014508\n","250it [00:02, 71.38it/s]Train epoch: 169 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015943\n","274it [00:03, 71.86it/s]Train epoch: 169 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.015310\n","298it [00:03, 67.74it/s]Train epoch: 169 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.015008\n","319it [00:03, 64.13it/s]Train epoch: 169 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.014250\n","347it [00:04, 63.53it/s]Train epoch: 169 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.015719\n","375it [00:04, 55.47it/s]Train epoch: 169 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.014279\n","395it [00:05, 54.59it/s]Train epoch: 169 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.012717\n","425it [00:05, 53.36it/s]Train epoch: 169 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.012781\n","448it [00:06, 47.13it/s]Train epoch: 169 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011958\n","473it [00:06, 42.66it/s]Train epoch: 169 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.013373\n","500it [00:07, 34.02it/s]Train epoch: 169 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.016869\n","505it [00:07, 65.77it/s]\n","epoch loss: 0.014066655419324295\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 476.59it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3607, 0.5698, 0.4554, 0.5062, 0.8632\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3914, 0.6306, 0.5078, 0.5626, 0.8855\n","rec_at_5: 0.5481\n","prec_at_5: 0.5575\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 170\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 170 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025840\n","16it [00:00, 156.31it/s]Train epoch: 170 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011383\n","47it [00:00, 126.59it/s]Train epoch: 170 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011810\n","73it [00:00, 121.98it/s]Train epoch: 170 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013754\n","98it [00:00, 112.37it/s]Train epoch: 170 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013644\n","122it [00:01, 100.18it/s]Train epoch: 170 [batch #125, batch_size 16, seq length 978]\tLoss: 0.013672\n","144it [00:01, 99.86it/s] Train epoch: 170 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015928\n","175it [00:01, 87.70it/s]Train epoch: 170 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015746\n","193it [00:01, 85.43it/s]Train epoch: 170 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014978\n","220it [00:02, 80.87it/s]Train epoch: 170 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013337\n","245it [00:02, 73.06it/s]Train epoch: 170 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.013548\n","270it [00:02, 71.52it/s]Train epoch: 170 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.015056\n","294it [00:03, 67.46it/s]Train epoch: 170 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.015400\n","324it [00:03, 65.19it/s]Train epoch: 170 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013942\n","345it [00:04, 63.16it/s]Train epoch: 170 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012929\n","371it [00:04, 53.15it/s]Train epoch: 170 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.013533\n","397it [00:05, 55.43it/s]Train epoch: 170 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013912\n","421it [00:05, 50.32it/s]Train epoch: 170 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013896\n","448it [00:06, 46.96it/s]Train epoch: 170 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.013068\n","473it [00:06, 40.48it/s]Train epoch: 170 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011984\n","500it [00:07, 33.71it/s]Train epoch: 170 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013020\n","505it [00:07, 65.62it/s]\n","epoch loss: 0.01329771909138907\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 482.97it/s]\n","Finish save rediction by checkpoint  170\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3600, 0.5690, 0.4548, 0.5055, 0.8629\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3919, 0.6306, 0.5087, 0.5631, 0.8853\n","rec_at_5: 0.5480\n","prec_at_5: 0.5582\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 171\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 171 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025702\n","15it [00:00, 148.73it/s]Train epoch: 171 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013122\n","45it [00:00, 127.77it/s]Train epoch: 171 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013573\n","72it [00:00, 123.87it/s]Train epoch: 171 [batch #75, batch_size 16, seq length 806]\tLoss: 0.017122\n","97it [00:00, 110.92it/s]Train epoch: 171 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012924\n","120it [00:01, 103.04it/s]Train epoch: 171 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012369\n","142it [00:01, 98.21it/s]Train epoch: 171 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014672\n","172it [00:01, 91.65it/s]Train epoch: 171 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.013144\n","200it [00:02, 82.28it/s]Train epoch: 171 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014170\n","218it [00:02, 77.23it/s]Train epoch: 171 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012561\n","244it [00:02, 75.53it/s]Train epoch: 171 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015157\n","268it [00:02, 72.88it/s]Train epoch: 171 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.013266\n","300it [00:03, 70.67it/s]Train epoch: 171 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013907\n","323it [00:03, 67.34it/s]Train epoch: 171 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.012906\n","344it [00:04, 59.79it/s]Train epoch: 171 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013525\n","370it [00:04, 50.99it/s]Train epoch: 171 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.013569\n","400it [00:05, 53.24it/s]Train epoch: 171 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013097\n","424it [00:05, 50.27it/s]Train epoch: 171 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011352\n","447it [00:06, 43.32it/s]Train epoch: 171 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011855\n","472it [00:06, 41.29it/s]Train epoch: 171 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.012245\n","498it [00:07, 34.12it/s]Train epoch: 171 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013240\n","505it [00:07, 64.66it/s]\n","epoch loss: 0.013264548656240216\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 475.56it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3600, 0.5699, 0.4544, 0.5056, 0.8626\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3904, 0.6293, 0.5069, 0.5615, 0.8855\n","rec_at_5: 0.5470\n","prec_at_5: 0.5577\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 172\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 172 [batch #0, batch_size 16, seq length 212]\tLoss: 0.024919\n","15it [00:00, 144.29it/s]Train epoch: 172 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011360\n","45it [00:00, 126.24it/s]Train epoch: 172 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011259\n","71it [00:00, 125.10it/s]Train epoch: 172 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015039\n","96it [00:00, 109.33it/s]Train epoch: 172 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012981\n","119it [00:01, 105.09it/s]Train epoch: 172 [batch #125, batch_size 16, seq length 978]\tLoss: 0.013883\n","150it [00:01, 89.83it/s]Train epoch: 172 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014459\n","170it [00:01, 91.95it/s]Train epoch: 172 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014025\n","198it [00:01, 84.82it/s]Train epoch: 172 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011089\n","225it [00:02, 76.65it/s]Train epoch: 172 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012549\n","250it [00:02, 72.66it/s]Train epoch: 172 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012461\n","274it [00:03, 71.75it/s]Train epoch: 172 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.014894\n","296it [00:03, 62.83it/s]Train epoch: 172 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.014073\n","319it [00:03, 65.70it/s]Train epoch: 172 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013817\n","347it [00:04, 60.76it/s]Train epoch: 172 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013547\n","375it [00:04, 56.41it/s]Train epoch: 172 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012299\n","400it [00:05, 55.29it/s]Train epoch: 172 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013976\n","424it [00:05, 49.53it/s]Train epoch: 172 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013224\n","446it [00:06, 46.67it/s]Train epoch: 172 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.012497\n","471it [00:06, 41.81it/s]Train epoch: 172 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011328\n","498it [00:07, 35.69it/s]Train epoch: 172 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013073\n","505it [00:07, 65.57it/s]\n","epoch loss: 0.012819301308879611\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 475.40it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3602, 0.5693, 0.4552, 0.5059, 0.8623\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3914, 0.6299, 0.5082, 0.5626, 0.8850\n","rec_at_5: 0.5464\n","prec_at_5: 0.5566\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 173\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 173 [batch #0, batch_size 16, seq length 212]\tLoss: 0.028192\n","16it [00:00, 157.25it/s]Train epoch: 173 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011978\n","47it [00:00, 127.73it/s]Train epoch: 173 [batch #50, batch_size 16, seq length 709]\tLoss: 0.015635\n","74it [00:00, 123.39it/s]Train epoch: 173 [batch #75, batch_size 16, seq length 806]\tLoss: 0.014746\n","99it [00:00, 112.20it/s]Train epoch: 173 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013378\n","122it [00:01, 99.63it/s] Train epoch: 173 [batch #125, batch_size 16, seq length 978]\tLoss: 0.013455\n","143it [00:01, 95.14it/s]Train epoch: 173 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.016152\n","172it [00:01, 86.68it/s]Train epoch: 173 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.012588\n","199it [00:02, 78.93it/s]Train epoch: 173 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.013290\n","217it [00:02, 81.70it/s]Train epoch: 173 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013827\n","244it [00:02, 79.74it/s]Train epoch: 173 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011939\n","269it [00:02, 72.90it/s]Train epoch: 173 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.014176\n","293it [00:03, 66.36it/s]Train epoch: 173 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013675\n","323it [00:03, 65.87it/s]Train epoch: 173 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.011855\n","344it [00:04, 58.43it/s]Train epoch: 173 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013744\n","371it [00:04, 58.53it/s]Train epoch: 173 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.011217\n","397it [00:05, 44.63it/s]Train epoch: 173 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.012590\n","420it [00:05, 49.34it/s]Train epoch: 173 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011719\n","447it [00:06, 46.10it/s]Train epoch: 173 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010874\n","472it [00:06, 42.34it/s]Train epoch: 173 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011821\n","498it [00:07, 34.38it/s]Train epoch: 173 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013195\n","505it [00:07, 64.61it/s]\n","epoch loss: 0.012563636248451795\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 473.48it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3590, 0.5690, 0.4522, 0.5039, 0.8622\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3922, 0.6316, 0.5086, 0.5634, 0.8849\n","rec_at_5: 0.5467\n","prec_at_5: 0.5568\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 174\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 174 [batch #0, batch_size 16, seq length 212]\tLoss: 0.021967\n","16it [00:00, 152.00it/s]Train epoch: 174 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008984\n","47it [00:00, 129.41it/s]Train epoch: 174 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011677\n","74it [00:00, 119.20it/s]Train epoch: 174 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015153\n","98it [00:00, 103.07it/s]Train epoch: 174 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013555\n","120it [00:01, 104.26it/s]Train epoch: 174 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012307\n","141it [00:01, 97.03it/s]Train epoch: 174 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013532\n","171it [00:01, 90.18it/s]Train epoch: 174 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011483\n","199it [00:01, 84.02it/s]Train epoch: 174 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.013886\n","217it [00:02, 76.57it/s]Train epoch: 174 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010841\n","243it [00:02, 74.11it/s]Train epoch: 174 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011220\n","275it [00:03, 68.97it/s]Train epoch: 174 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010924\n","299it [00:03, 66.32it/s]Train epoch: 174 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013323\n","320it [00:03, 64.17it/s]Train epoch: 174 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010776\n","348it [00:04, 63.22it/s]Train epoch: 174 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012307\n","369it [00:04, 60.11it/s]Train epoch: 174 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012183\n","396it [00:05, 53.73it/s]Train epoch: 174 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011523\n","420it [00:05, 48.84it/s]Train epoch: 174 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011303\n","447it [00:06, 44.35it/s]Train epoch: 174 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011028\n","472it [00:06, 43.05it/s]Train epoch: 174 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011213\n","499it [00:07, 34.55it/s]Train epoch: 174 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.011844\n","505it [00:07, 65.47it/s]\n","epoch loss: 0.01188290086044644\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 477.70it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3595, 0.5675, 0.4540, 0.5045, 0.8623\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3906, 0.6283, 0.5080, 0.5618, 0.8852\n","rec_at_5: 0.5471\n","prec_at_5: 0.5566\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 175\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 175 [batch #0, batch_size 16, seq length 212]\tLoss: 0.021065\n","16it [00:00, 151.80it/s]Train epoch: 175 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009910\n","47it [00:00, 125.60it/s]Train epoch: 175 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010728\n","72it [00:00, 110.20it/s]Train epoch: 175 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015337\n","96it [00:00, 103.12it/s]Train epoch: 175 [batch #100, batch_size 16, seq length 892]\tLoss: 0.011412\n","118it [00:01, 102.70it/s]Train epoch: 175 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012931\n","149it [00:01, 89.87it/s]Train epoch: 175 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014674\n","168it [00:01, 88.02it/s]Train epoch: 175 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010479\n","195it [00:01, 83.54it/s]Train epoch: 175 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011112\n","222it [00:02, 80.27it/s]Train epoch: 175 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012459\n","247it [00:02, 71.93it/s]Train epoch: 175 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012558\n","271it [00:03, 68.19it/s]Train epoch: 175 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.015894\n","295it [00:03, 66.03it/s]Train epoch: 175 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012716\n","323it [00:03, 63.55it/s]Train epoch: 175 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010471\n","344it [00:04, 61.05it/s]Train epoch: 175 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012363\n","371it [00:04, 59.55it/s]Train epoch: 175 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.011925\n","395it [00:05, 52.70it/s]Train epoch: 175 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011031\n","425it [00:05, 47.37it/s]Train epoch: 175 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010798\n","446it [00:06, 45.78it/s]Train epoch: 175 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009571\n","471it [00:06, 40.71it/s]Train epoch: 175 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010867\n","497it [00:07, 36.19it/s]Train epoch: 175 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012665\n","505it [00:07, 64.37it/s]\n","epoch loss: 0.011801083749943956\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 471.37it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3598, 0.5678, 0.4550, 0.5051, 0.8624\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3916, 0.6279, 0.5100, 0.5628, 0.8854\n","rec_at_5: 0.5460\n","prec_at_5: 0.5572\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 176\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 176 [batch #0, batch_size 16, seq length 212]\tLoss: 0.026157\n","15it [00:00, 140.78it/s]Train epoch: 176 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009919\n","44it [00:00, 124.48it/s]Train epoch: 176 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011685\n","70it [00:00, 113.42it/s]Train epoch: 176 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015346\n","94it [00:00, 104.57it/s]Train epoch: 176 [batch #100, batch_size 16, seq length 892]\tLoss: 0.011733\n","117it [00:01, 105.50it/s]Train epoch: 176 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009851\n","148it [00:01, 95.34it/s]Train epoch: 176 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014162\n","168it [00:01, 90.34it/s]Train epoch: 176 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010381\n","196it [00:01, 80.65it/s]Train epoch: 176 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010779\n","223it [00:02, 77.43it/s]Train epoch: 176 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011145\n","248it [00:02, 74.49it/s]Train epoch: 176 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011556\n","272it [00:03, 73.27it/s]Train epoch: 176 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011774\n","295it [00:03, 63.79it/s]Train epoch: 176 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010739\n","323it [00:03, 63.92it/s]Train epoch: 176 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.011263\n","344it [00:04, 60.24it/s]Train epoch: 176 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012642\n","371it [00:04, 57.73it/s]Train epoch: 176 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009859\n","395it [00:05, 54.61it/s]Train epoch: 176 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009590\n","425it [00:05, 49.22it/s]Train epoch: 176 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010322\n","450it [00:06, 46.17it/s]Train epoch: 176 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009623\n","475it [00:06, 41.73it/s]Train epoch: 176 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.012275\n","497it [00:07, 34.69it/s]Train epoch: 176 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012236\n","505it [00:07, 64.91it/s]\n","epoch loss: 0.01143985066371555\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 475.78it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3590, 0.5693, 0.4527, 0.5043, 0.8622\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3923, 0.6301, 0.5097, 0.5636, 0.8856\n","rec_at_5: 0.5449\n","prec_at_5: 0.5566\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 177\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 177 [batch #0, batch_size 16, seq length 212]\tLoss: 0.015111\n","16it [00:00, 158.34it/s]Train epoch: 177 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009425\n","47it [00:00, 127.61it/s]Train epoch: 177 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010299\n","72it [00:00, 117.45it/s]Train epoch: 177 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013020\n","96it [00:00, 108.06it/s]Train epoch: 177 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010728\n","118it [00:01, 105.65it/s]Train epoch: 177 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011823\n","150it [00:01, 91.01it/s]Train epoch: 177 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011941\n","170it [00:01, 91.86it/s]Train epoch: 177 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011967\n","198it [00:01, 78.82it/s]Train epoch: 177 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011472\n","225it [00:02, 76.74it/s]Train epoch: 177 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012424\n","249it [00:02, 74.54it/s]Train epoch: 177 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012015\n","273it [00:03, 72.14it/s]Train epoch: 177 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.013238\n","297it [00:03, 67.36it/s]Train epoch: 177 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.011856\n","320it [00:03, 64.59it/s]Train epoch: 177 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010839\n","348it [00:04, 62.23it/s]Train epoch: 177 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011621\n","375it [00:04, 56.34it/s]Train epoch: 177 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009950\n","400it [00:05, 55.78it/s]Train epoch: 177 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010408\n","424it [00:05, 50.47it/s]Train epoch: 177 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011573\n","446it [00:06, 47.03it/s]Train epoch: 177 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009538\n","471it [00:06, 42.79it/s]Train epoch: 177 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.012084\n","499it [00:08, 21.54it/s]Train epoch: 177 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010545\n","505it [00:08, 60.26it/s]\n","epoch loss: 0.011288771325823769\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 390.71it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3561, 0.5659, 0.4487, 0.5006, 0.8617\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3886, 0.6295, 0.5038, 0.5597, 0.8847\n","rec_at_5: 0.5456\n","prec_at_5: 0.5570\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 178\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 178 [batch #0, batch_size 16, seq length 212]\tLoss: 0.023973\n","16it [00:00, 156.13it/s]Train epoch: 178 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009306\n","47it [00:00, 126.29it/s]Train epoch: 178 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012455\n","73it [00:00, 112.85it/s]Train epoch: 178 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013676\n","97it [00:00, 112.35it/s]Train epoch: 178 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010756\n","120it [00:01, 103.65it/s]Train epoch: 178 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012087\n","141it [00:01, 96.64it/s]Train epoch: 178 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011731\n","171it [00:01, 89.39it/s]Train epoch: 178 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011453\n","200it [00:02, 81.50it/s]Train epoch: 178 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011152\n","218it [00:02, 82.42it/s]Train epoch: 178 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010131\n","244it [00:02, 77.65it/s]Train epoch: 178 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010012\n","268it [00:02, 70.18it/s]Train epoch: 178 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011925\n","300it [00:03, 69.71it/s]Train epoch: 178 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.011772\n","323it [00:03, 62.90it/s]Train epoch: 178 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.011244\n","344it [00:04, 59.06it/s]Train epoch: 178 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012252\n","371it [00:04, 58.67it/s]Train epoch: 178 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009551\n","396it [00:05, 54.44it/s]Train epoch: 178 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010615\n","420it [00:05, 49.36it/s]Train epoch: 178 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011830\n","448it [00:06, 46.31it/s]Train epoch: 178 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009583\n","473it [00:06, 42.21it/s]Train epoch: 178 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011166\n","498it [00:07, 33.97it/s]Train epoch: 178 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.011182\n","505it [00:07, 65.40it/s]\n","epoch loss: 0.011065557651915633\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 479.01it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3613, 0.5676, 0.4579, 0.5069, 0.8617\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3921, 0.6274, 0.5111, 0.5633, 0.8848\n","rec_at_5: 0.5450\n","prec_at_5: 0.5558\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 179\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 179 [batch #0, batch_size 16, seq length 212]\tLoss: 0.027895\n","16it [00:00, 155.83it/s]Train epoch: 179 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008905\n","47it [00:00, 125.51it/s]Train epoch: 179 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011352\n","73it [00:00, 114.55it/s]Train epoch: 179 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012683\n","97it [00:00, 104.38it/s]Train epoch: 179 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010997\n","119it [00:01, 95.27it/s] Train epoch: 179 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011437\n","150it [00:01, 89.24it/s]Train epoch: 179 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.012845\n","170it [00:01, 90.71it/s]Train epoch: 179 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010342\n","199it [00:02, 81.71it/s]Train epoch: 179 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.012379\n","217it [00:02, 77.04it/s]Train epoch: 179 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010845\n","244it [00:02, 75.13it/s]Train epoch: 179 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.008899\n","268it [00:02, 70.09it/s]Train epoch: 179 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011447\n","299it [00:03, 67.33it/s]Train epoch: 179 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.011377\n","320it [00:03, 64.79it/s]Train epoch: 179 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009084\n","348it [00:04, 59.74it/s]Train epoch: 179 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011841\n","375it [00:04, 57.83it/s]Train epoch: 179 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010199\n","399it [00:05, 53.19it/s]Train epoch: 179 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008453\n","423it [00:05, 50.92it/s]Train epoch: 179 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009408\n","450it [00:06, 46.10it/s]Train epoch: 179 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008916\n","475it [00:06, 41.90it/s]Train epoch: 179 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010442\n","497it [00:07, 34.60it/s]Train epoch: 179 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.011626\n","505it [00:07, 65.18it/s]\n","epoch loss: 0.010748276245517491\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 356.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3603, 0.5650, 0.4562, 0.5048, 0.8617\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3909, 0.6261, 0.5099, 0.5620, 0.8850\n","rec_at_5: 0.5453\n","prec_at_5: 0.5554\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 180\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 180 [batch #0, batch_size 16, seq length 212]\tLoss: 0.019884\n","24it [00:00, 70.67it/s]Train epoch: 180 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009052\n","48it [00:00, 68.07it/s]Train epoch: 180 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011020\n","71it [00:01, 68.91it/s]Train epoch: 180 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012869\n","94it [00:01, 70.14it/s]Train epoch: 180 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009486\n","123it [00:01, 60.00it/s]Train epoch: 180 [batch #125, batch_size 16, seq length 978]\tLoss: 0.010490\n","148it [00:02, 52.17it/s]Train epoch: 180 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011841\n","172it [00:02, 52.78it/s]Train epoch: 180 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010229\n","200it [00:03, 46.11it/s]Train epoch: 180 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009512\n","224it [00:03, 55.43it/s]Train epoch: 180 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009067\n","247it [00:04, 66.65it/s]Train epoch: 180 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009623\n","269it [00:04, 66.45it/s]Train epoch: 180 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.013190\n","298it [00:04, 65.78it/s]Train epoch: 180 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012134\n","319it [00:05, 64.30it/s]Train epoch: 180 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009993\n","347it [00:05, 62.85it/s]Train epoch: 180 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010222\n","374it [00:06, 58.17it/s]Train epoch: 180 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010309\n","399it [00:06, 57.18it/s]Train epoch: 180 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010058\n","423it [00:07, 50.76it/s]Train epoch: 180 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008647\n","446it [00:07, 49.14it/s]Train epoch: 180 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009996\n","471it [00:08, 42.60it/s]Train epoch: 180 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010865\n","500it [00:09, 35.53it/s]Train epoch: 180 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008918\n","505it [00:09, 55.05it/s]\n","epoch loss: 0.010166873718787096\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 483.75it/s]\n","Finish save rediction by checkpoint  180\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3610, 0.5676, 0.4573, 0.5065, 0.8613\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3920, 0.6275, 0.5109, 0.5632, 0.8850\n","rec_at_5: 0.5475\n","prec_at_5: 0.5564\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 181\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 181 [batch #0, batch_size 16, seq length 212]\tLoss: 0.019658\n","16it [00:00, 156.32it/s]Train epoch: 181 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009246\n","47it [00:00, 125.75it/s]Train epoch: 181 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010393\n","73it [00:00, 112.58it/s]Train epoch: 181 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013477\n","97it [00:00, 113.18it/s]Train epoch: 181 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009550\n","120it [00:01, 102.55it/s]Train epoch: 181 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009116\n","141it [00:01, 93.80it/s]Train epoch: 181 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013178\n","170it [00:01, 88.54it/s]Train epoch: 181 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010141\n","198it [00:01, 85.76it/s]Train epoch: 181 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009441\n","225it [00:02, 76.52it/s]Train epoch: 181 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010523\n","243it [00:02, 78.84it/s]Train epoch: 181 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010034\n","275it [00:03, 72.09it/s]Train epoch: 181 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010784\n","299it [00:03, 64.64it/s]Train epoch: 181 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.011131\n","322it [00:03, 64.31it/s]Train epoch: 181 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009792\n","350it [00:04, 62.93it/s]Train epoch: 181 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010281\n","371it [00:04, 59.38it/s]Train epoch: 181 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.011315\n","398it [00:05, 56.89it/s]Train epoch: 181 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009266\n","422it [00:05, 48.00it/s]Train epoch: 181 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008633\n","449it [00:06, 45.49it/s]Train epoch: 181 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008508\n","474it [00:06, 42.86it/s]Train epoch: 181 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009105\n","497it [00:07, 37.60it/s]Train epoch: 181 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010155\n","505it [00:07, 65.85it/s]\n","epoch loss: 0.00992023186333994\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.95it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3605, 0.5674, 0.4566, 0.5060, 0.8612\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3915, 0.6274, 0.5102, 0.5627, 0.8846\n","rec_at_5: 0.5467\n","prec_at_5: 0.5572\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 182\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 182 [batch #0, batch_size 16, seq length 212]\tLoss: 0.016911\n","16it [00:00, 152.94it/s]Train epoch: 182 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009226\n","47it [00:00, 125.03it/s]Train epoch: 182 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010730\n","73it [00:00, 116.43it/s]Train epoch: 182 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011978\n","97it [00:00, 109.89it/s]Train epoch: 182 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009799\n","120it [00:01, 103.33it/s]Train epoch: 182 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011152\n","141it [00:01, 93.50it/s]Train epoch: 182 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.012681\n","171it [00:01, 90.01it/s]Train epoch: 182 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010434\n","200it [00:01, 85.73it/s]Train epoch: 182 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009003\n","218it [00:02, 79.22it/s]Train epoch: 182 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010636\n","245it [00:02, 77.00it/s]Train epoch: 182 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009984\n","269it [00:02, 75.08it/s]Train epoch: 182 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010850\n","293it [00:03, 65.99it/s]Train epoch: 182 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010851\n","323it [00:03, 66.33it/s]Train epoch: 182 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008655\n","344it [00:04, 63.51it/s]Train epoch: 182 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009488\n","372it [00:04, 57.37it/s]Train epoch: 182 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010240\n","396it [00:05, 55.08it/s]Train epoch: 182 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008676\n","420it [00:05, 49.31it/s]Train epoch: 182 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011196\n","448it [00:06, 45.61it/s]Train epoch: 182 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008903\n","473it [00:06, 39.99it/s]Train epoch: 182 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009742\n","500it [00:07, 34.88it/s]Train epoch: 182 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009911\n","505it [00:07, 65.92it/s]\n","epoch loss: 0.009915175490798244\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 477.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3605, 0.5711, 0.4549, 0.5064, 0.8610\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3913, 0.6299, 0.5081, 0.5625, 0.8847\n","rec_at_5: 0.5450\n","prec_at_5: 0.5554\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 183\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 183 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020477\n","15it [00:00, 142.27it/s]Train epoch: 183 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008217\n","45it [00:00, 126.67it/s]Train epoch: 183 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010048\n","71it [00:00, 121.74it/s]Train epoch: 183 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011201\n","96it [00:00, 111.37it/s]Train epoch: 183 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008699\n","120it [00:01, 98.99it/s] Train epoch: 183 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009103\n","142it [00:01, 99.40it/s] Train epoch: 183 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010681\n","173it [00:01, 92.10it/s]Train epoch: 183 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009184\n","192it [00:01, 86.81it/s]Train epoch: 183 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010797\n","219it [00:02, 80.56it/s]Train epoch: 183 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010689\n","245it [00:02, 76.67it/s]Train epoch: 183 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010933\n","269it [00:02, 72.61it/s]Train epoch: 183 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010820\n","293it [00:03, 68.54it/s]Train epoch: 183 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.009979\n","323it [00:03, 67.30it/s]Train epoch: 183 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008895\n","344it [00:04, 59.42it/s]Train epoch: 183 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009490\n","372it [00:04, 58.83it/s]Train epoch: 183 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009965\n","397it [00:05, 56.55it/s]Train epoch: 183 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009334\n","421it [00:05, 51.55it/s]Train epoch: 183 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008009\n","449it [00:06, 45.61it/s]Train epoch: 183 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007458\n","474it [00:06, 43.74it/s]Train epoch: 183 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008655\n","497it [00:07, 34.78it/s]Train epoch: 183 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009255\n","505it [00:07, 66.09it/s]\n","epoch loss: 0.009604815443018729\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 485.89it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3601, 0.5688, 0.4549, 0.5055, 0.8613\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3917, 0.6287, 0.5095, 0.5629, 0.8849\n","rec_at_5: 0.5466\n","prec_at_5: 0.5569\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 184\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 184 [batch #0, batch_size 16, seq length 212]\tLoss: 0.016749\n","16it [00:00, 151.88it/s]Train epoch: 184 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010016\n","47it [00:00, 124.02it/s]Train epoch: 184 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010022\n","73it [00:00, 111.26it/s]Train epoch: 184 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012265\n","97it [00:00, 110.20it/s]Train epoch: 184 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008519\n","120it [00:01, 100.41it/s]Train epoch: 184 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009980\n","141it [00:01, 92.46it/s]Train epoch: 184 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009706\n","170it [00:01, 86.30it/s]Train epoch: 184 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008435\n","197it [00:02, 77.52it/s]Train epoch: 184 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007953\n","224it [00:02, 78.48it/s]Train epoch: 184 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009335\n","248it [00:02, 74.57it/s]Train epoch: 184 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.008028\n","272it [00:03, 70.90it/s]Train epoch: 184 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011314\n","295it [00:03, 68.53it/s]Train epoch: 184 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010412\n","324it [00:03, 62.17it/s]Train epoch: 184 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009576\n","345it [00:04, 61.62it/s]Train epoch: 184 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010200\n","371it [00:04, 54.36it/s]Train epoch: 184 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008391\n","397it [00:05, 55.29it/s]Train epoch: 184 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008217\n","421it [00:05, 51.45it/s]Train epoch: 184 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.007889\n","449it [00:06, 43.73it/s]Train epoch: 184 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007071\n","474it [00:06, 42.47it/s]Train epoch: 184 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008340\n","497it [00:07, 36.78it/s]Train epoch: 184 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008968\n","505it [00:07, 64.83it/s]\n","epoch loss: 0.00913367604786106\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 479.51it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3616, 0.5701, 0.4571, 0.5074, 0.8613\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3912, 0.6292, 0.5085, 0.5624, 0.8846\n","rec_at_5: 0.5452\n","prec_at_5: 0.5547\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 185\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 185 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020472\n","16it [00:00, 156.47it/s]Train epoch: 185 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007791\n","47it [00:00, 125.71it/s]Train epoch: 185 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009617\n","73it [00:00, 122.85it/s]Train epoch: 185 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012228\n","98it [00:00, 110.00it/s]Train epoch: 185 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009365\n","121it [00:01, 98.07it/s] Train epoch: 185 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009445\n","141it [00:01, 91.58it/s]Train epoch: 185 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011341\n","171it [00:01, 88.48it/s]Train epoch: 185 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009496\n","200it [00:02, 84.57it/s]Train epoch: 185 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008823\n","218it [00:02, 79.01it/s]Train epoch: 185 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009492\n","245it [00:02, 75.60it/s]Train epoch: 185 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009404\n","269it [00:02, 71.04it/s]Train epoch: 185 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010932\n","300it [00:03, 65.23it/s]Train epoch: 185 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010326\n","321it [00:03, 63.72it/s]Train epoch: 185 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008455\n","349it [00:04, 60.38it/s]Train epoch: 185 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007921\n","370it [00:04, 58.63it/s]Train epoch: 185 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008798\n","400it [00:05, 53.17it/s]Train epoch: 185 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008293\n","424it [00:05, 51.44it/s]Train epoch: 185 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009861\n","446it [00:06, 45.71it/s]Train epoch: 185 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007759\n","471it [00:06, 40.71it/s]Train epoch: 185 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008426\n","498it [00:07, 35.97it/s]Train epoch: 185 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009550\n","505it [00:07, 65.75it/s]\n","epoch loss: 0.009205687024093481\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 470.51it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3610, 0.5688, 0.4556, 0.5059, 0.8610\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3924, 0.6301, 0.5099, 0.5637, 0.8846\n","rec_at_5: 0.5462\n","prec_at_5: 0.5568\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 186\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 186 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020625\n","16it [00:00, 152.48it/s]Train epoch: 186 [batch #25, batch_size 16, seq length 571]\tLoss: 0.006956\n","47it [00:00, 129.08it/s]Train epoch: 186 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008359\n","74it [00:00, 123.22it/s]Train epoch: 186 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012603\n","99it [00:00, 111.78it/s]Train epoch: 186 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009955\n","123it [00:01, 100.80it/s]Train epoch: 186 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009356\n","145it [00:01, 100.16it/s]Train epoch: 186 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.010774\n","166it [00:01, 93.57it/s]Train epoch: 186 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009028\n","194it [00:01, 85.31it/s]Train epoch: 186 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008705\n","221it [00:02, 82.52it/s]Train epoch: 186 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008737\n","247it [00:02, 74.80it/s]Train epoch: 186 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009291\n","271it [00:02, 71.21it/s]Train epoch: 186 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008998\n","295it [00:03, 68.17it/s]Train epoch: 186 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012041\n","325it [00:03, 62.82it/s]Train epoch: 186 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008055\n","346it [00:04, 62.11it/s]Train epoch: 186 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008799\n","374it [00:04, 60.03it/s]Train epoch: 186 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008737\n","399it [00:05, 57.06it/s]Train epoch: 186 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008297\n","423it [00:05, 52.08it/s]Train epoch: 186 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008547\n","446it [00:06, 44.78it/s]Train epoch: 186 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007894\n","471it [00:06, 42.32it/s]Train epoch: 186 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008494\n","498it [00:07, 33.46it/s]Train epoch: 186 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008997\n","505it [00:07, 66.51it/s]\n","epoch loss: 0.009049501998832974\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 474.60it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3599, 0.5682, 0.4547, 0.5052, 0.8609\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3914, 0.6289, 0.5089, 0.5626, 0.8844\n","rec_at_5: 0.5468\n","prec_at_5: 0.5550\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 187\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 187 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020721\n","16it [00:00, 158.80it/s]Train epoch: 187 [batch #25, batch_size 16, seq length 571]\tLoss: 0.006586\n","46it [00:00, 127.99it/s]Train epoch: 187 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008957\n","71it [00:00, 115.23it/s]Train epoch: 187 [batch #75, batch_size 16, seq length 806]\tLoss: 0.010165\n","95it [00:00, 107.04it/s]Train epoch: 187 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008991\n","117it [00:01, 101.24it/s]Train epoch: 187 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008786\n","150it [00:01, 93.05it/s] Train epoch: 187 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011582\n","170it [00:01, 87.55it/s]Train epoch: 187 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009667\n","197it [00:01, 83.99it/s]Train epoch: 187 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008493\n","224it [00:02, 78.92it/s]Train epoch: 187 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010349\n","250it [00:02, 74.96it/s]Train epoch: 187 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.007295\n","274it [00:03, 70.02it/s]Train epoch: 187 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010304\n","297it [00:03, 67.30it/s]Train epoch: 187 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.009740\n","325it [00:03, 65.36it/s]Train epoch: 187 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008302\n","346it [00:04, 64.26it/s]Train epoch: 187 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008941\n","373it [00:04, 57.10it/s]Train epoch: 187 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.007972\n","398it [00:05, 57.14it/s]Train epoch: 187 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008747\n","422it [00:05, 51.32it/s]Train epoch: 187 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.007752\n","450it [00:06, 45.17it/s]Train epoch: 187 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007426\n","475it [00:06, 40.30it/s]Train epoch: 187 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009209\n","497it [00:07, 34.91it/s]Train epoch: 187 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010673\n","505it [00:07, 65.68it/s]\n","epoch loss: 0.008792026907592865\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:03, 479.39it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3594, 0.5657, 0.4546, 0.5041, 0.8608\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.3912, 0.6276, 0.5094, 0.5624, 0.8847\n","rec_at_5: 0.5486\n","prec_at_5: 0.5565\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_03:42:52\n","\n","EPOCH 188\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 188 [batch #0, batch_size 16, seq length 212]\tLoss: 0.026748\n","16it [00:00, 157.39it/s]Train epoch: 188 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009715\n","48it [00:00, 126.63it/s]Train epoch: 188 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009922\n","75it [00:00, 123.50it/s]Train epoch: 188 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011327\n","100it [00:00, 112.42it/s]Train epoch: 188 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009786\n","124it [00:01, 97.10it/s] Train epoch: 188 [batch #125, batch_size 16, seq length 978]\tLoss: 0.007503\n","145it [00:01, 96.30it/s]Train epoch: 188 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011887\n","175it [00:01, 85.95it/s]Train epoch: 188 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010052\n","193it [00:01, 87.04it/s]Train epoch: 188 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008100\n","220it [00:02, 83.31it/s]Train epoch: 188 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009280\n","246it [00:02, 77.82it/s]Train epoch: 188 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.007855\n","270it [00:02, 71.71it/s]Train epoch: 188 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008419\n","278it [00:03, 72.16it/s]"]}],"source":["# Train\n","#!python ./learn/training.py ./mimicdata/mimic3/train_50.csv  ./mimicdata/mimic3/vocab.csv full conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --lmbda 0.01 --embed-file ./mimicdata/mimic3/processed_full.embed --gpu\n","!python ./learn/training.py ./mimicdata/mimic3/train_50.csv ./mimicdata/mimic3/vocab.csv 50 conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --embed-file ./mimicdata/mimic3/processed_full.embed --gpu\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"o3sqH3rXs60v","outputId":"cebb7ce5-da24-49b4-befc-1bc1dd01d6e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 87 [batch #0, batch_size 16, seq length 212]\tLoss: 0.140077\n","24it [00:00, 36.71it/s]Train epoch: 87 [batch #25, batch_size 16, seq length 571]\tLoss: 0.076685\n","48it [00:01, 36.10it/s]Train epoch: 87 [batch #50, batch_size 16, seq length 709]\tLoss: 0.077460\n","72it [00:02, 34.31it/s]Train epoch: 87 [batch #75, batch_size 16, seq length 806]\tLoss: 0.096907\n","100it [00:02, 33.78it/s]Train epoch: 87 [batch #100, batch_size 16, seq length 892]\tLoss: 0.082812\n","124it [00:03, 32.31it/s]Train epoch: 87 [batch #125, batch_size 16, seq length 978]\tLoss: 0.081228\n","148it [00:04, 31.89it/s]Train epoch: 87 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.096994\n","172it [00:05, 29.82it/s]Train epoch: 87 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.083362\n","200it [00:06, 29.43it/s]Train epoch: 87 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.088411\n","224it [00:06, 30.74it/s]Train epoch: 87 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.090530\n","248it [00:07, 28.25it/s]Train epoch: 87 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.099447\n","273it [00:08, 29.32it/s]Train epoch: 87 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.098310\n","300it [00:09, 27.87it/s]Train epoch: 87 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.107135\n","325it [00:10, 28.19it/s]Train epoch: 87 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.100918\n","349it [00:11, 26.23it/s]Train epoch: 87 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.104861\n","373it [00:12, 26.58it/s]Train epoch: 87 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.118858\n","400it [00:13, 25.71it/s]Train epoch: 87 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.111798\n","424it [00:14, 24.03it/s]Train epoch: 87 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.107506\n","448it [00:15, 22.08it/s]Train epoch: 87 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.113264\n","475it [00:16, 21.87it/s]Train epoch: 87 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.134339\n","499it [00:17, 20.19it/s]Train epoch: 87 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.138314\n","505it [00:18, 28.01it/s]\n","epoch loss: 0.09830894913192433\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 341.14it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3986, 0.6190, 0.4930, 0.5489, 0.8724\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4356, 0.6824, 0.5464, 0.6068, 0.9040\n","rec_at_5: 0.5759\n","prec_at_5: 0.5873\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 88\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 88 [batch #0, batch_size 16, seq length 212]\tLoss: 0.123321\n","24it [00:00, 36.34it/s]Train epoch: 88 [batch #25, batch_size 16, seq length 571]\tLoss: 0.076078\n","48it [00:01, 34.27it/s]Train epoch: 88 [batch #50, batch_size 16, seq length 709]\tLoss: 0.072750\n","72it [00:02, 34.05it/s]Train epoch: 88 [batch #75, batch_size 16, seq length 806]\tLoss: 0.094491\n","100it [00:02, 32.26it/s]Train epoch: 88 [batch #100, batch_size 16, seq length 892]\tLoss: 0.082126\n","124it [00:03, 32.80it/s]Train epoch: 88 [batch #125, batch_size 16, seq length 978]\tLoss: 0.080196\n","148it [00:04, 30.84it/s]Train epoch: 88 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.093434\n","172it [00:05, 31.93it/s]Train epoch: 88 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.080638\n","200it [00:06, 30.69it/s]Train epoch: 88 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.090277\n","223it [00:06, 29.26it/s]Train epoch: 88 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.091288\n","250it [00:07, 29.74it/s]Train epoch: 88 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.097255\n","275it [00:08, 28.34it/s]Train epoch: 88 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.095762\n","298it [00:09, 28.34it/s]Train epoch: 88 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.107467\n","325it [00:10, 26.86it/s]Train epoch: 88 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.098723\n","349it [00:11, 26.14it/s]Train epoch: 88 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.103204\n","373it [00:12, 24.85it/s]Train epoch: 88 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.115092\n","400it [00:13, 25.30it/s]Train epoch: 88 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.108968\n","424it [00:14, 23.93it/s]Train epoch: 88 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.105645\n","448it [00:15, 22.98it/s]Train epoch: 88 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.112691\n","475it [00:16, 21.29it/s]Train epoch: 88 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.128417\n","499it [00:17, 19.91it/s]Train epoch: 88 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.134045\n","505it [00:18, 27.70it/s]\n","epoch loss: 0.09661317225361224\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 342.48it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3994, 0.6175, 0.4948, 0.5494, 0.8722\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4357, 0.6815, 0.5470, 0.6069, 0.9038\n","rec_at_5: 0.5739\n","prec_at_5: 0.5856\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 89\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 89 [batch #0, batch_size 16, seq length 212]\tLoss: 0.117093\n","24it [00:00, 35.93it/s]Train epoch: 89 [batch #25, batch_size 16, seq length 571]\tLoss: 0.072913\n","48it [00:01, 35.78it/s]Train epoch: 89 [batch #50, batch_size 16, seq length 709]\tLoss: 0.076079\n","72it [00:02, 34.37it/s]Train epoch: 89 [batch #75, batch_size 16, seq length 806]\tLoss: 0.092109\n","100it [00:02, 33.12it/s]Train epoch: 89 [batch #100, batch_size 16, seq length 892]\tLoss: 0.075443\n","124it [00:03, 32.20it/s]Train epoch: 89 [batch #125, batch_size 16, seq length 978]\tLoss: 0.079060\n","148it [00:04, 33.12it/s]Train epoch: 89 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.093683\n","172it [00:05, 31.21it/s]Train epoch: 89 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.080057\n","200it [00:06, 30.48it/s]Train epoch: 89 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.085755\n","222it [00:06, 27.88it/s]Train epoch: 89 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.086230\n","250it [00:07, 28.73it/s]Train epoch: 89 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.096033\n","275it [00:08, 29.19it/s]Train epoch: 89 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.091766\n","298it [00:09, 28.47it/s]Train epoch: 89 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.103346\n","325it [00:10, 27.85it/s]Train epoch: 89 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.095515\n","349it [00:11, 26.65it/s]Train epoch: 89 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.102286\n","373it [00:12, 25.27it/s]Train epoch: 89 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.111573\n","400it [00:13, 25.60it/s]Train epoch: 89 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.107314\n","424it [00:14, 24.80it/s]Train epoch: 89 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.104349\n","448it [00:15, 23.32it/s]Train epoch: 89 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.110882\n","475it [00:16, 21.88it/s]Train epoch: 89 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.127841\n","499it [00:17, 19.72it/s]Train epoch: 89 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.131836\n","505it [00:18, 28.04it/s]\n","epoch loss: 0.09432708396769986\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.13it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.4001, 0.6157, 0.4966, 0.5498, 0.8718\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4368, 0.6817, 0.5487, 0.6080, 0.9034\n","rec_at_5: 0.5744\n","prec_at_5: 0.5860\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 90\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 90 [batch #0, batch_size 16, seq length 212]\tLoss: 0.131642\n","24it [00:00, 37.22it/s]Train epoch: 90 [batch #25, batch_size 16, seq length 571]\tLoss: 0.070620\n","48it [00:01, 35.90it/s]Train epoch: 90 [batch #50, batch_size 16, seq length 709]\tLoss: 0.071336\n","72it [00:02, 33.84it/s]Train epoch: 90 [batch #75, batch_size 16, seq length 806]\tLoss: 0.090216\n","100it [00:02, 33.81it/s]Train epoch: 90 [batch #100, batch_size 16, seq length 892]\tLoss: 0.076084\n","124it [00:03, 32.59it/s]Train epoch: 90 [batch #125, batch_size 16, seq length 978]\tLoss: 0.076176\n","148it [00:04, 31.94it/s]Train epoch: 90 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.090559\n","172it [00:05, 30.38it/s]Train epoch: 90 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.079294\n","200it [00:06, 29.18it/s]Train epoch: 90 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.083182\n","224it [00:06, 30.32it/s]Train epoch: 90 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.085337\n","250it [00:07, 28.76it/s]Train epoch: 90 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.094665\n","275it [00:08, 28.56it/s]Train epoch: 90 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.089897\n","300it [00:09, 28.43it/s]Train epoch: 90 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.100585\n","325it [00:10, 27.55it/s]Train epoch: 90 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.092597\n","349it [00:11, 25.95it/s]Train epoch: 90 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.098305\n","373it [00:12, 25.16it/s]Train epoch: 90 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.110260\n","400it [00:13, 25.87it/s]Train epoch: 90 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.107630\n","424it [00:14, 23.88it/s]Train epoch: 90 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.103269\n","448it [00:15, 22.29it/s]Train epoch: 90 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.107692\n","475it [00:16, 21.61it/s]Train epoch: 90 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.124725\n","499it [00:17, 20.06it/s]Train epoch: 90 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.129577\n","505it [00:18, 28.01it/s]\n","epoch loss: 0.09208337141219342\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 341.90it/s]\n","Finish save rediction by checkpoint  90\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3983, 0.6148, 0.4951, 0.5485, 0.8715\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4349, 0.6776, 0.5483, 0.6061, 0.9028\n","rec_at_5: 0.5741\n","prec_at_5: 0.5863\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 91\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 91 [batch #0, batch_size 16, seq length 212]\tLoss: 0.111834\n","25it [00:00, 35.84it/s]Train epoch: 91 [batch #25, batch_size 16, seq length 571]\tLoss: 0.072097\n","49it [00:01, 36.60it/s]Train epoch: 91 [batch #50, batch_size 16, seq length 709]\tLoss: 0.070239\n","73it [00:02, 32.90it/s]Train epoch: 91 [batch #75, batch_size 16, seq length 806]\tLoss: 0.091248\n","97it [00:02, 33.37it/s]Train epoch: 91 [batch #100, batch_size 16, seq length 892]\tLoss: 0.075375\n","125it [00:03, 32.32it/s]Train epoch: 91 [batch #125, batch_size 16, seq length 978]\tLoss: 0.076408\n","149it [00:04, 31.29it/s]Train epoch: 91 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.089067\n","173it [00:05, 30.88it/s]Train epoch: 91 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.076314\n","200it [00:06, 29.78it/s]Train epoch: 91 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.084465\n","223it [00:06, 26.97it/s]Train epoch: 91 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.085348\n","248it [00:07, 28.04it/s]Train epoch: 91 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.091532\n","275it [00:08, 28.35it/s]Train epoch: 91 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.092787\n","298it [00:09, 26.96it/s]Train epoch: 91 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.099215\n","325it [00:10, 27.84it/s]Train epoch: 91 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.091447\n","349it [00:11, 26.76it/s]Train epoch: 91 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.097743\n","373it [00:12, 26.27it/s]Train epoch: 91 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.110154\n","400it [00:13, 26.10it/s]Train epoch: 91 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.103718\n","424it [00:14, 24.22it/s]Train epoch: 91 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.100129\n","448it [00:15, 23.78it/s]Train epoch: 91 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.105688\n","475it [00:16, 21.41it/s]Train epoch: 91 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.121189\n","499it [00:17, 19.57it/s]Train epoch: 91 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.125329\n","505it [00:18, 27.73it/s]\n","epoch loss: 0.09055336738134374\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 342.98it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3987, 0.6127, 0.4964, 0.5485, 0.8712\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4339, 0.6778, 0.5467, 0.6052, 0.9025\n","rec_at_5: 0.5720\n","prec_at_5: 0.5844\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 92\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 92 [batch #0, batch_size 16, seq length 212]\tLoss: 0.121993\n","24it [00:00, 36.26it/s]Train epoch: 92 [batch #25, batch_size 16, seq length 571]\tLoss: 0.068625\n","48it [00:01, 35.55it/s]Train epoch: 92 [batch #50, batch_size 16, seq length 709]\tLoss: 0.070858\n","72it [00:02, 33.81it/s]Train epoch: 92 [batch #75, batch_size 16, seq length 806]\tLoss: 0.088549\n","100it [00:02, 33.68it/s]Train epoch: 92 [batch #100, batch_size 16, seq length 892]\tLoss: 0.075347\n","124it [00:03, 31.01it/s]Train epoch: 92 [batch #125, batch_size 16, seq length 978]\tLoss: 0.075240\n","148it [00:04, 31.84it/s]Train epoch: 92 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.086587\n","172it [00:05, 31.48it/s]Train epoch: 92 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.074140\n","200it [00:06, 29.67it/s]Train epoch: 92 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.081691\n","224it [00:06, 30.14it/s]Train epoch: 92 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.081009\n","249it [00:07, 29.57it/s]Train epoch: 92 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.088492\n","275it [00:08, 29.11it/s]Train epoch: 92 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.088534\n","298it [00:09, 28.24it/s]Train epoch: 92 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.099172\n","324it [00:10, 28.09it/s]Train epoch: 92 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.090105\n","348it [00:11, 27.41it/s]Train epoch: 92 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.093334\n","375it [00:12, 26.28it/s]Train epoch: 92 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.105884\n","399it [00:13, 24.88it/s]Train epoch: 92 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.101021\n","423it [00:14, 24.02it/s]Train epoch: 92 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.095304\n","450it [00:15, 24.01it/s]Train epoch: 92 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.101532\n","474it [00:16, 21.45it/s]Train epoch: 92 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.118718\n","498it [00:17, 20.04it/s]Train epoch: 92 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.123994\n","505it [00:18, 28.01it/s]\n","epoch loss: 0.08853703248360664\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 338.04it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3963, 0.6107, 0.4941, 0.5463, 0.8710\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4318, 0.6743, 0.5456, 0.6032, 0.9023\n","rec_at_5: 0.5715\n","prec_at_5: 0.5833\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 93\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 93 [batch #0, batch_size 16, seq length 212]\tLoss: 0.117959\n","24it [00:00, 36.00it/s]Train epoch: 93 [batch #25, batch_size 16, seq length 571]\tLoss: 0.067362\n","48it [00:01, 34.63it/s]Train epoch: 93 [batch #50, batch_size 16, seq length 709]\tLoss: 0.064996\n","72it [00:02, 33.79it/s]Train epoch: 93 [batch #75, batch_size 16, seq length 806]\tLoss: 0.084488\n","100it [00:02, 32.64it/s]Train epoch: 93 [batch #100, batch_size 16, seq length 892]\tLoss: 0.072226\n","124it [00:03, 32.58it/s]Train epoch: 93 [batch #125, batch_size 16, seq length 978]\tLoss: 0.072931\n","148it [00:04, 30.81it/s]Train epoch: 93 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.087272\n","172it [00:05, 30.32it/s]Train epoch: 93 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.073983\n","200it [00:06, 30.34it/s]Train epoch: 93 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.079301\n","224it [00:06, 29.25it/s]Train epoch: 93 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.081925\n","247it [00:07, 28.62it/s]Train epoch: 93 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.086562\n","274it [00:08, 28.50it/s]Train epoch: 93 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.087007\n","298it [00:09, 26.49it/s]Train epoch: 93 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.094502\n","325it [00:10, 26.85it/s]Train epoch: 93 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.089635\n","349it [00:11, 25.93it/s]Train epoch: 93 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.094117\n","373it [00:12, 24.37it/s]Train epoch: 93 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.103955\n","400it [00:13, 22.80it/s]Train epoch: 93 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.099868\n","424it [00:14, 18.48it/s]Train epoch: 93 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.097286\n","450it [00:16, 14.29it/s]Train epoch: 93 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.100987\n","474it [00:17, 19.53it/s]Train epoch: 93 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.118451\n","500it [00:19, 19.24it/s]Train epoch: 93 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.120200\n","505it [00:19, 25.81it/s]\n","epoch loss: 0.08706836168202434\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 331.87it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3977, 0.6122, 0.4958, 0.5479, 0.8707\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4330, 0.6754, 0.5468, 0.6043, 0.9019\n","rec_at_5: 0.5702\n","prec_at_5: 0.5825\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 94\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 94 [batch #0, batch_size 16, seq length 212]\tLoss: 0.102806\n","24it [00:00, 36.58it/s]Train epoch: 94 [batch #25, batch_size 16, seq length 571]\tLoss: 0.065700\n","48it [00:01, 33.76it/s]Train epoch: 94 [batch #50, batch_size 16, seq length 709]\tLoss: 0.067599\n","72it [00:02, 31.00it/s]Train epoch: 94 [batch #75, batch_size 16, seq length 806]\tLoss: 0.089727\n","99it [00:03, 23.54it/s]Train epoch: 94 [batch #100, batch_size 16, seq length 892]\tLoss: 0.073674\n","125it [00:04, 18.19it/s]Train epoch: 94 [batch #125, batch_size 16, seq length 978]\tLoss: 0.071420\n","150it [00:05, 17.69it/s]Train epoch: 94 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.083762\n","175it [00:07, 15.96it/s]Train epoch: 94 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.071675\n","197it [00:08, 21.09it/s]Train epoch: 94 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.076860\n","222it [00:09, 27.53it/s]Train epoch: 94 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.079250\n","248it [00:10, 28.76it/s]Train epoch: 94 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.085497\n","274it [00:11, 28.69it/s]Train epoch: 94 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.086764\n","298it [00:12, 27.26it/s]Train epoch: 94 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.095593\n","325it [00:13, 27.36it/s]Train epoch: 94 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.088230\n","349it [00:13, 26.92it/s]Train epoch: 94 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.091861\n","373it [00:14, 26.32it/s]Train epoch: 94 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.102897\n","400it [00:15, 25.48it/s]Train epoch: 94 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.097357\n","424it [00:16, 24.05it/s]Train epoch: 94 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.093286\n","448it [00:17, 23.51it/s]Train epoch: 94 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.098309\n","475it [00:19, 22.28it/s]Train epoch: 94 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.113005\n","499it [00:20, 20.13it/s]Train epoch: 94 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.119019\n","505it [00:20, 24.37it/s]\n","epoch loss: 0.08552016618265079\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 337.76it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3971, 0.6085, 0.4967, 0.5469, 0.8704\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4318, 0.6719, 0.5472, 0.6032, 0.9015\n","rec_at_5: 0.5691\n","prec_at_5: 0.5821\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 95\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 95 [batch #0, batch_size 16, seq length 212]\tLoss: 0.109344\n","24it [00:00, 36.87it/s]Train epoch: 95 [batch #25, batch_size 16, seq length 571]\tLoss: 0.064910\n","48it [00:01, 34.88it/s]Train epoch: 95 [batch #50, batch_size 16, seq length 709]\tLoss: 0.065100\n","72it [00:02, 33.50it/s]Train epoch: 95 [batch #75, batch_size 16, seq length 806]\tLoss: 0.083700\n","100it [00:02, 32.65it/s]Train epoch: 95 [batch #100, batch_size 16, seq length 892]\tLoss: 0.071746\n","124it [00:03, 31.45it/s]Train epoch: 95 [batch #125, batch_size 16, seq length 978]\tLoss: 0.069638\n","148it [00:04, 32.56it/s]Train epoch: 95 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.082229\n","172it [00:05, 31.34it/s]Train epoch: 95 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.070298\n","200it [00:06, 29.64it/s]Train epoch: 95 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.073648\n","225it [00:07, 29.07it/s]Train epoch: 95 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.076821\n","250it [00:07, 27.69it/s]Train epoch: 95 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.085161\n","274it [00:08, 27.76it/s]Train epoch: 95 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.083881\n","298it [00:09, 25.65it/s]Train epoch: 95 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.092311\n","325it [00:10, 26.96it/s]Train epoch: 95 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.087012\n","349it [00:11, 25.29it/s]Train epoch: 95 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.090619\n","373it [00:12, 24.54it/s]Train epoch: 95 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.097879\n","400it [00:13, 24.22it/s]Train epoch: 95 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.094392\n","424it [00:14, 23.87it/s]Train epoch: 95 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.090236\n","448it [00:15, 22.29it/s]Train epoch: 95 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.097855\n","475it [00:17, 21.08it/s]Train epoch: 95 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.111710\n","499it [00:18, 19.62it/s]Train epoch: 95 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.113297\n","505it [00:18, 27.11it/s]\n","epoch loss: 0.08365375458604038\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 328.51it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3969, 0.6072, 0.4973, 0.5468, 0.8702\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4321, 0.6704, 0.5486, 0.6034, 0.9013\n","rec_at_5: 0.5684\n","prec_at_5: 0.5819\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 96\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 96 [batch #0, batch_size 16, seq length 212]\tLoss: 0.108290\n","22it [00:00, 37.07it/s]Train epoch: 96 [batch #25, batch_size 16, seq length 571]\tLoss: 0.062670\n","50it [00:01, 35.21it/s]Train epoch: 96 [batch #50, batch_size 16, seq length 709]\tLoss: 0.066699\n","74it [00:02, 32.42it/s]Train epoch: 96 [batch #75, batch_size 16, seq length 806]\tLoss: 0.084954\n","98it [00:02, 33.28it/s]Train epoch: 96 [batch #100, batch_size 16, seq length 892]\tLoss: 0.068101\n","122it [00:03, 33.20it/s]Train epoch: 96 [batch #125, batch_size 16, seq length 978]\tLoss: 0.070454\n","150it [00:04, 30.73it/s]Train epoch: 96 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.081051\n","174it [00:05, 30.92it/s]Train epoch: 96 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.068695\n","200it [00:06, 30.28it/s]Train epoch: 96 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.074452\n","224it [00:06, 29.08it/s]Train epoch: 96 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.076254\n","250it [00:07, 27.50it/s]Train epoch: 96 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.084729\n","273it [00:08, 28.84it/s]Train epoch: 96 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.079298\n","300it [00:09, 27.71it/s]Train epoch: 96 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.089712\n","324it [00:10, 27.56it/s]Train epoch: 96 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.081371\n","348it [00:11, 25.42it/s]Train epoch: 96 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.087128\n","375it [00:12, 24.82it/s]Train epoch: 96 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.100310\n","399it [00:13, 24.55it/s]Train epoch: 96 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.092876\n","423it [00:14, 23.50it/s]Train epoch: 96 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.088661\n","450it [00:15, 22.58it/s]Train epoch: 96 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.094919\n","474it [00:16, 21.68it/s]Train epoch: 96 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.109423\n","498it [00:17, 20.26it/s]Train epoch: 96 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.110593\n","505it [00:18, 27.47it/s]\n","epoch loss: 0.08183719532398304\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 331.77it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3986, 0.6059, 0.5012, 0.5486, 0.8699\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4321, 0.6682, 0.5500, 0.6034, 0.9008\n","rec_at_5: 0.5671\n","prec_at_5: 0.5802\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 97\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 97 [batch #0, batch_size 16, seq length 212]\tLoss: 0.108807\n","24it [00:00, 34.70it/s]Train epoch: 97 [batch #25, batch_size 16, seq length 571]\tLoss: 0.060768\n","48it [00:01, 34.58it/s]Train epoch: 97 [batch #50, batch_size 16, seq length 709]\tLoss: 0.061945\n","72it [00:02, 32.68it/s]Train epoch: 97 [batch #75, batch_size 16, seq length 806]\tLoss: 0.083458\n","100it [00:02, 31.29it/s]Train epoch: 97 [batch #100, batch_size 16, seq length 892]\tLoss: 0.067923\n","124it [00:03, 30.67it/s]Train epoch: 97 [batch #125, batch_size 16, seq length 978]\tLoss: 0.066966\n","148it [00:04, 31.29it/s]Train epoch: 97 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.082172\n","172it [00:05, 30.17it/s]Train epoch: 97 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.065771\n","199it [00:06, 28.04it/s]Train epoch: 97 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.073471\n","225it [00:07, 28.18it/s]Train epoch: 97 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.073661\n","249it [00:08, 27.88it/s]Train epoch: 97 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.080806\n","274it [00:09, 26.40it/s]Train epoch: 97 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.080357\n","300it [00:09, 25.95it/s]Train epoch: 97 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.087903\n","324it [00:10, 25.63it/s]Train epoch: 97 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.082684\n","348it [00:11, 25.78it/s]Train epoch: 97 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.088539\n","375it [00:12, 25.31it/s]Train epoch: 97 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.095007\n","399it [00:13, 24.26it/s]Train epoch: 97 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.090194\n","423it [00:14, 24.20it/s]Train epoch: 97 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.086498\n","450it [00:16, 21.79it/s]Train epoch: 97 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.091638\n","474it [00:17, 21.17it/s]Train epoch: 97 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.105924\n","499it [00:18, 18.65it/s]Train epoch: 97 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.106726\n","505it [00:18, 27.02it/s]\n","epoch loss: 0.08017921013812912\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.53it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3958, 0.6055, 0.4961, 0.5454, 0.8698\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4307, 0.6694, 0.5470, 0.6021, 0.9006\n","rec_at_5: 0.5676\n","prec_at_5: 0.5809\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 98\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 98 [batch #0, batch_size 16, seq length 212]\tLoss: 0.097350\n","24it [00:00, 35.58it/s]Train epoch: 98 [batch #25, batch_size 16, seq length 571]\tLoss: 0.060588\n","48it [00:01, 33.64it/s]Train epoch: 98 [batch #50, batch_size 16, seq length 709]\tLoss: 0.060288\n","72it [00:02, 33.17it/s]Train epoch: 98 [batch #75, batch_size 16, seq length 806]\tLoss: 0.079182\n","100it [00:02, 31.79it/s]Train epoch: 98 [batch #100, batch_size 16, seq length 892]\tLoss: 0.067105\n","124it [00:03, 30.75it/s]Train epoch: 98 [batch #125, batch_size 16, seq length 978]\tLoss: 0.066075\n","148it [00:04, 30.62it/s]Train epoch: 98 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.077379\n","172it [00:05, 31.22it/s]Train epoch: 98 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.066539\n","199it [00:06, 29.29it/s]Train epoch: 98 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.069413\n","222it [00:07, 28.97it/s]Train epoch: 98 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.073224\n","248it [00:07, 28.29it/s]Train epoch: 98 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.074903\n","273it [00:08, 27.73it/s]Train epoch: 98 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.075404\n","300it [00:09, 26.28it/s]Train epoch: 98 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.085896\n","324it [00:10, 27.12it/s]Train epoch: 98 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.083328\n","348it [00:11, 26.23it/s]Train epoch: 98 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.084197\n","375it [00:12, 24.53it/s]Train epoch: 98 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.093438\n","399it [00:13, 24.33it/s]Train epoch: 98 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.089235\n","423it [00:14, 23.36it/s]Train epoch: 98 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.085198\n","450it [00:15, 22.22it/s]Train epoch: 98 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.089126\n","474it [00:17, 21.46it/s]Train epoch: 98 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.104247\n","499it [00:18, 18.57it/s]Train epoch: 98 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.107430\n","505it [00:18, 26.95it/s]\n","epoch loss: 0.07826673922746784\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.31it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3954, 0.6053, 0.4952, 0.5447, 0.8695\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4295, 0.6678, 0.5463, 0.6009, 0.9001\n","rec_at_5: 0.5674\n","prec_at_5: 0.5800\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 99\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 99 [batch #0, batch_size 16, seq length 212]\tLoss: 0.108769\n","25it [00:00, 35.98it/s]Train epoch: 99 [batch #25, batch_size 16, seq length 571]\tLoss: 0.059712\n","49it [00:01, 33.57it/s]Train epoch: 99 [batch #50, batch_size 16, seq length 709]\tLoss: 0.061537\n","73it [00:02, 31.80it/s]Train epoch: 99 [batch #75, batch_size 16, seq length 806]\tLoss: 0.078076\n","97it [00:02, 32.62it/s]Train epoch: 99 [batch #100, batch_size 16, seq length 892]\tLoss: 0.067024\n","125it [00:03, 31.90it/s]Train epoch: 99 [batch #125, batch_size 16, seq length 978]\tLoss: 0.063238\n","149it [00:04, 31.59it/s]Train epoch: 99 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.075567\n","173it [00:05, 29.89it/s]Train epoch: 99 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.065123\n","200it [00:06, 27.63it/s]Train epoch: 99 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.067903\n","225it [00:07, 29.06it/s]Train epoch: 99 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.074826\n","248it [00:07, 29.05it/s]Train epoch: 99 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.078294\n","274it [00:08, 27.23it/s]Train epoch: 99 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.075842\n","299it [00:09, 28.36it/s]Train epoch: 99 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.080822\n","323it [00:10, 26.34it/s]Train epoch: 99 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.080389\n","350it [00:11, 26.11it/s]Train epoch: 99 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.082457\n","374it [00:12, 25.18it/s]Train epoch: 99 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.091302\n","398it [00:13, 24.38it/s]Train epoch: 99 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.087677\n","425it [00:14, 23.17it/s]Train epoch: 99 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.081656\n","449it [00:15, 22.76it/s]Train epoch: 99 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.089835\n","473it [00:16, 21.96it/s]Train epoch: 99 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.101707\n","500it [00:18, 19.42it/s]Train epoch: 99 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.102716\n","505it [00:18, 27.27it/s]\n","epoch loss: 0.07648851965544838\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 336.36it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3963, 0.6019, 0.4983, 0.5452, 0.8693\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4300, 0.6658, 0.5483, 0.6014, 0.8999\n","rec_at_5: 0.5666\n","prec_at_5: 0.5799\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 100\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 100 [batch #0, batch_size 16, seq length 212]\tLoss: 0.094099\n","24it [00:00, 35.95it/s]Train epoch: 100 [batch #25, batch_size 16, seq length 571]\tLoss: 0.056308\n","48it [00:01, 34.19it/s]Train epoch: 100 [batch #50, batch_size 16, seq length 709]\tLoss: 0.057333\n","72it [00:02, 32.31it/s]Train epoch: 100 [batch #75, batch_size 16, seq length 806]\tLoss: 0.078067\n","100it [00:02, 32.58it/s]Train epoch: 100 [batch #100, batch_size 16, seq length 892]\tLoss: 0.064035\n","124it [00:03, 31.43it/s]Train epoch: 100 [batch #125, batch_size 16, seq length 978]\tLoss: 0.063750\n","148it [00:04, 30.64it/s]Train epoch: 100 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.076601\n","172it [00:05, 30.13it/s]Train epoch: 100 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.063368\n","200it [00:06, 27.97it/s]Train epoch: 100 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.069514\n","224it [00:07, 26.42it/s]Train epoch: 100 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.069357\n","248it [00:08, 27.04it/s]Train epoch: 100 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.075051\n","273it [00:09, 26.77it/s]Train epoch: 100 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.075480\n","299it [00:09, 26.92it/s]Train epoch: 100 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.083207\n","323it [00:10, 26.91it/s]Train epoch: 100 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.075402\n","350it [00:11, 26.44it/s]Train epoch: 100 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.080414\n","374it [00:12, 24.00it/s]Train epoch: 100 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.091773\n","398it [00:13, 24.13it/s]Train epoch: 100 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.084049\n","425it [00:15, 22.30it/s]Train epoch: 100 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.081075\n","449it [00:16, 22.39it/s]Train epoch: 100 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.088570\n","473it [00:17, 20.99it/s]Train epoch: 100 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.098938\n","500it [00:18, 18.28it/s]Train epoch: 100 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.100812\n","505it [00:18, 26.60it/s]\n","epoch loss: 0.07496246208464451\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 321.06it/s]\n","Finish save rediction by checkpoint  100\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3951, 0.6012, 0.4967, 0.5440, 0.8689\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4290, 0.6653, 0.5470, 0.6004, 0.8995\n","rec_at_5: 0.5642\n","prec_at_5: 0.5786\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 101\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 101 [batch #0, batch_size 16, seq length 212]\tLoss: 0.095422\n","24it [00:00, 36.04it/s]Train epoch: 101 [batch #25, batch_size 16, seq length 571]\tLoss: 0.055352\n","48it [00:01, 34.25it/s]Train epoch: 101 [batch #50, batch_size 16, seq length 709]\tLoss: 0.057294\n","72it [00:02, 32.58it/s]Train epoch: 101 [batch #75, batch_size 16, seq length 806]\tLoss: 0.076592\n","100it [00:02, 32.98it/s]Train epoch: 101 [batch #100, batch_size 16, seq length 892]\tLoss: 0.064183\n","124it [00:03, 31.44it/s]Train epoch: 101 [batch #125, batch_size 16, seq length 978]\tLoss: 0.061883\n","148it [00:04, 30.54it/s]Train epoch: 101 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.069808\n","172it [00:05, 29.79it/s]Train epoch: 101 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.060685\n","198it [00:06, 29.93it/s]Train epoch: 101 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.066996\n","222it [00:07, 28.80it/s]Train epoch: 101 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.067315\n","250it [00:08, 28.12it/s]Train epoch: 101 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.073197\n","274it [00:08, 27.81it/s]Train epoch: 101 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.075196\n","299it [00:09, 26.95it/s]Train epoch: 101 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.082560\n","323it [00:10, 26.33it/s]Train epoch: 101 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.074949\n","350it [00:11, 26.64it/s]Train epoch: 101 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.080069\n","374it [00:12, 25.98it/s]Train epoch: 101 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.088609\n","398it [00:13, 26.08it/s]Train epoch: 101 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.084052\n","425it [00:14, 24.53it/s]Train epoch: 101 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.077989\n","449it [00:15, 22.69it/s]Train epoch: 101 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.084173\n","473it [00:16, 21.16it/s]Train epoch: 101 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.095218\n","500it [00:18, 19.69it/s]Train epoch: 101 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.097011\n","505it [00:18, 27.31it/s]\n","epoch loss: 0.07309548387137971\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 339.95it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3970, 0.6007, 0.5013, 0.5465, 0.8686\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4299, 0.6611, 0.5514, 0.6013, 0.8992\n","rec_at_5: 0.5644\n","prec_at_5: 0.5788\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 102\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 102 [batch #0, batch_size 16, seq length 212]\tLoss: 0.098486\n","24it [00:00, 35.93it/s]Train epoch: 102 [batch #25, batch_size 16, seq length 571]\tLoss: 0.056622\n","48it [00:01, 35.15it/s]Train epoch: 102 [batch #50, batch_size 16, seq length 709]\tLoss: 0.057071\n","72it [00:02, 34.49it/s]Train epoch: 102 [batch #75, batch_size 16, seq length 806]\tLoss: 0.074552\n","100it [00:02, 34.00it/s]Train epoch: 102 [batch #100, batch_size 16, seq length 892]\tLoss: 0.063194\n","124it [00:03, 31.33it/s]Train epoch: 102 [batch #125, batch_size 16, seq length 978]\tLoss: 0.063134\n","148it [00:04, 31.08it/s]Train epoch: 102 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.070558\n","172it [00:05, 30.05it/s]Train epoch: 102 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.058357\n","200it [00:06, 29.12it/s]Train epoch: 102 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.066402\n","225it [00:07, 28.53it/s]Train epoch: 102 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.068387\n","247it [00:07, 27.05it/s]Train epoch: 102 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.071793\n","275it [00:08, 28.84it/s]Train epoch: 102 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.071025\n","300it [00:09, 26.68it/s]Train epoch: 102 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.077704\n","325it [00:10, 26.65it/s]Train epoch: 102 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.071723\n","349it [00:11, 26.80it/s]Train epoch: 102 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.075577\n","373it [00:12, 25.74it/s]Train epoch: 102 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.086247\n","400it [00:13, 24.23it/s]Train epoch: 102 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.081868\n","424it [00:14, 24.13it/s]Train epoch: 102 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.076279\n","448it [00:15, 22.99it/s]Train epoch: 102 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.080898\n","475it [00:16, 21.78it/s]Train epoch: 102 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.095821\n","499it [00:18, 19.84it/s]Train epoch: 102 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.094441\n","505it [00:18, 27.52it/s]\n","epoch loss: 0.07181127332519777\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:05, 302.21it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3942, 0.6006, 0.4972, 0.5441, 0.8682\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4281, 0.6622, 0.5477, 0.5995, 0.8988\n","rec_at_5: 0.5636\n","prec_at_5: 0.5784\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 103\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 103 [batch #0, batch_size 16, seq length 212]\tLoss: 0.095875\n","24it [00:01, 23.18it/s]Train epoch: 103 [batch #25, batch_size 16, seq length 571]\tLoss: 0.055626\n","48it [00:02, 20.38it/s]Train epoch: 103 [batch #50, batch_size 16, seq length 709]\tLoss: 0.052797\n","74it [00:03, 25.09it/s]Train epoch: 103 [batch #75, batch_size 16, seq length 806]\tLoss: 0.073501\n","98it [00:04, 32.55it/s]Train epoch: 103 [batch #100, batch_size 16, seq length 892]\tLoss: 0.061610\n","122it [00:04, 32.22it/s]Train epoch: 103 [batch #125, batch_size 16, seq length 978]\tLoss: 0.059628\n","149it [00:05, 29.59it/s]Train epoch: 103 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.070842\n","173it [00:06, 30.18it/s]Train epoch: 103 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.058268\n","199it [00:07, 29.88it/s]Train epoch: 103 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.062390\n","225it [00:08, 28.11it/s]Train epoch: 103 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.069370\n","250it [00:09, 29.32it/s]Train epoch: 103 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.073751\n","273it [00:10, 27.98it/s]Train epoch: 103 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.068752\n","300it [00:11, 26.90it/s]Train epoch: 103 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.075326\n","324it [00:12, 26.40it/s]Train epoch: 103 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.071401\n","348it [00:12, 26.19it/s]Train epoch: 103 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.076302\n","375it [00:13, 25.71it/s]Train epoch: 103 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.081113\n","399it [00:14, 25.25it/s]Train epoch: 103 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.078416\n","423it [00:15, 23.16it/s]Train epoch: 103 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.073483\n","450it [00:17, 22.72it/s]Train epoch: 103 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.081400\n","474it [00:18, 21.62it/s]Train epoch: 103 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.094732\n","499it [00:19, 18.44it/s]Train epoch: 103 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.094616\n","505it [00:19, 25.50it/s]\n","epoch loss: 0.07007888214349156\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 336.40it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3949, 0.5964, 0.5004, 0.5442, 0.8679\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4268, 0.6573, 0.5490, 0.5983, 0.8983\n","rec_at_5: 0.5630\n","prec_at_5: 0.5771\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 104\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 104 [batch #0, batch_size 16, seq length 212]\tLoss: 0.097104\n","23it [00:00, 28.25it/s]Train epoch: 104 [batch #25, batch_size 16, seq length 571]\tLoss: 0.053679\n","47it [00:01, 31.69it/s]Train epoch: 104 [batch #50, batch_size 16, seq length 709]\tLoss: 0.055462\n","75it [00:02, 31.86it/s]Train epoch: 104 [batch #75, batch_size 16, seq length 806]\tLoss: 0.071904\n","99it [00:03, 31.00it/s]Train epoch: 104 [batch #100, batch_size 16, seq length 892]\tLoss: 0.060171\n","123it [00:03, 31.59it/s]Train epoch: 104 [batch #125, batch_size 16, seq length 978]\tLoss: 0.060232\n","147it [00:04, 30.56it/s]Train epoch: 104 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.068036\n","175it [00:05, 30.85it/s]Train epoch: 104 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.056539\n","197it [00:06, 28.59it/s]Train epoch: 104 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.064170\n","224it [00:07, 27.58it/s]Train epoch: 104 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.064857\n","250it [00:08, 27.30it/s]Train epoch: 104 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.069738\n","273it [00:09, 27.86it/s]Train epoch: 104 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.066163\n","300it [00:10, 26.42it/s]Train epoch: 104 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.075572\n","324it [00:10, 27.01it/s]Train epoch: 104 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.070448\n","348it [00:11, 26.87it/s]Train epoch: 104 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.073427\n","375it [00:12, 25.67it/s]Train epoch: 104 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.082527\n","399it [00:13, 25.27it/s]Train epoch: 104 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.079941\n","423it [00:14, 22.99it/s]Train epoch: 104 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.072563\n","450it [00:16, 22.90it/s]Train epoch: 104 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.079526\n","474it [00:17, 21.60it/s]Train epoch: 104 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.090048\n","498it [00:18, 19.37it/s]Train epoch: 104 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.091507\n","505it [00:18, 26.94it/s]\n","epoch loss: 0.06885472546598993\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 330.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3946, 0.5968, 0.4985, 0.5432, 0.8677\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4270, 0.6589, 0.5482, 0.5985, 0.8981\n","rec_at_5: 0.5603\n","prec_at_5: 0.5758\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 105\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 105 [batch #0, batch_size 16, seq length 212]\tLoss: 0.084970\n","24it [00:00, 32.89it/s]Train epoch: 105 [batch #25, batch_size 16, seq length 571]\tLoss: 0.051937\n","48it [00:01, 33.95it/s]Train epoch: 105 [batch #50, batch_size 16, seq length 709]\tLoss: 0.053516\n","72it [00:02, 33.04it/s]Train epoch: 105 [batch #75, batch_size 16, seq length 806]\tLoss: 0.070090\n","100it [00:03, 32.03it/s]Train epoch: 105 [batch #100, batch_size 16, seq length 892]\tLoss: 0.059992\n","124it [00:03, 30.50it/s]Train epoch: 105 [batch #125, batch_size 16, seq length 978]\tLoss: 0.057267\n","148it [00:04, 29.86it/s]Train epoch: 105 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.064472\n","174it [00:05, 29.11it/s]Train epoch: 105 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.057847\n","198it [00:06, 29.22it/s]Train epoch: 105 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.060468\n","224it [00:07, 28.18it/s]Train epoch: 105 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.063151\n","247it [00:08, 27.67it/s]Train epoch: 105 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.066104\n","274it [00:09, 27.56it/s]Train epoch: 105 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.065323\n","298it [00:09, 24.73it/s]Train epoch: 105 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.074343\n","325it [00:11, 24.90it/s]Train epoch: 105 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.067888\n","349it [00:11, 26.18it/s]Train epoch: 105 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.075377\n","373it [00:12, 23.93it/s]Train epoch: 105 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.080576\n","400it [00:13, 24.11it/s]Train epoch: 105 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.078466\n","424it [00:15, 22.27it/s]Train epoch: 105 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.071065\n","448it [00:16, 22.47it/s]Train epoch: 105 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.077216\n","475it [00:17, 20.63it/s]Train epoch: 105 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.088121\n","499it [00:18, 17.79it/s]Train epoch: 105 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.088941\n","505it [00:19, 26.56it/s]\n","epoch loss: 0.06721473263742606\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:05, 303.73it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3949, 0.5947, 0.5008, 0.5437, 0.8674\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4264, 0.6554, 0.5496, 0.5979, 0.8978\n","rec_at_5: 0.5590\n","prec_at_5: 0.5751\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 106\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 106 [batch #0, batch_size 16, seq length 212]\tLoss: 0.096331\n","24it [00:00, 24.70it/s]Train epoch: 106 [batch #25, batch_size 16, seq length 571]\tLoss: 0.051525\n","48it [00:02, 21.90it/s]Train epoch: 106 [batch #50, batch_size 16, seq length 709]\tLoss: 0.049502\n","74it [00:02, 29.95it/s]Train epoch: 106 [batch #75, batch_size 16, seq length 806]\tLoss: 0.067157\n","98it [00:03, 29.87it/s]Train epoch: 106 [batch #100, batch_size 16, seq length 892]\tLoss: 0.054471\n","125it [00:04, 28.94it/s]Train epoch: 106 [batch #125, batch_size 16, seq length 978]\tLoss: 0.058642\n","149it [00:05, 29.58it/s]Train epoch: 106 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.065061\n","173it [00:06, 29.01it/s]Train epoch: 106 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.054483\n","198it [00:07, 27.13it/s]Train epoch: 106 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.059026\n","225it [00:08, 27.47it/s]Train epoch: 106 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.062609\n","249it [00:09, 25.50it/s]Train epoch: 106 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.068797\n","273it [00:10, 26.53it/s]Train epoch: 106 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.063759\n","300it [00:11, 25.00it/s]Train epoch: 106 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.071210\n","324it [00:12, 25.07it/s]Train epoch: 106 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.067826\n","348it [00:12, 24.85it/s]Train epoch: 106 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.072940\n","375it [00:14, 23.71it/s]Train epoch: 106 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.078753\n","399it [00:15, 23.70it/s]Train epoch: 106 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.075320\n","423it [00:16, 22.00it/s]Train epoch: 106 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.069170\n","450it [00:17, 22.51it/s]Train epoch: 106 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.074023\n","474it [00:18, 20.38it/s]Train epoch: 106 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.087817\n","500it [00:19, 17.76it/s]Train epoch: 106 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.085374\n","505it [00:20, 25.08it/s]\n","epoch loss: 0.06543330063877424\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 315.83it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3948, 0.5960, 0.5002, 0.5439, 0.8673\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4271, 0.6572, 0.5495, 0.5985, 0.8976\n","rec_at_5: 0.5613\n","prec_at_5: 0.5770\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 107\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 107 [batch #0, batch_size 16, seq length 212]\tLoss: 0.083616\n","24it [00:00, 34.43it/s]Train epoch: 107 [batch #25, batch_size 16, seq length 571]\tLoss: 0.052030\n","48it [00:01, 31.81it/s]Train epoch: 107 [batch #50, batch_size 16, seq length 709]\tLoss: 0.049790\n","72it [00:02, 31.60it/s]Train epoch: 107 [batch #75, batch_size 16, seq length 806]\tLoss: 0.072421\n","100it [00:03, 31.73it/s]Train epoch: 107 [batch #100, batch_size 16, seq length 892]\tLoss: 0.055474\n","124it [00:03, 30.72it/s]Train epoch: 107 [batch #125, batch_size 16, seq length 978]\tLoss: 0.056316\n","148it [00:04, 29.47it/s]Train epoch: 107 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.062223\n","172it [00:05, 28.39it/s]Train epoch: 107 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.053095\n","199it [00:06, 26.64it/s]Train epoch: 107 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.057600\n","223it [00:07, 27.66it/s]Train epoch: 107 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.060007\n","248it [00:08, 26.51it/s]Train epoch: 107 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.062097\n","275it [00:09, 26.40it/s]Train epoch: 107 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.061141\n","299it [00:10, 25.72it/s]Train epoch: 107 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.070499\n","323it [00:11, 24.91it/s]Train epoch: 107 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.062813\n","350it [00:12, 25.88it/s]Train epoch: 107 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.068473\n","374it [00:13, 25.31it/s]Train epoch: 107 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.079530\n","398it [00:14, 23.40it/s]Train epoch: 107 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.071444\n","425it [00:15, 22.27it/s]Train epoch: 107 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.070782\n","449it [00:16, 20.96it/s]Train epoch: 107 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.072153\n","473it [00:17, 21.16it/s]Train epoch: 107 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.085026\n","499it [00:19, 17.56it/s]Train epoch: 107 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.083224\n","505it [00:19, 25.96it/s]\n","epoch loss: 0.06385504675125427\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 318.18it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3948, 0.5958, 0.5007, 0.5441, 0.8667\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4262, 0.6554, 0.5494, 0.5977, 0.8971\n","rec_at_5: 0.5592\n","prec_at_5: 0.5756\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 108\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 108 [batch #0, batch_size 16, seq length 212]\tLoss: 0.081196\n","24it [00:00, 35.12it/s]Train epoch: 108 [batch #25, batch_size 16, seq length 571]\tLoss: 0.049841\n","48it [00:01, 32.15it/s]Train epoch: 108 [batch #50, batch_size 16, seq length 709]\tLoss: 0.048887\n","72it [00:02, 30.73it/s]Train epoch: 108 [batch #75, batch_size 16, seq length 806]\tLoss: 0.069463\n","100it [00:03, 31.99it/s]Train epoch: 108 [batch #100, batch_size 16, seq length 892]\tLoss: 0.054196\n","124it [00:03, 31.48it/s]Train epoch: 108 [batch #125, batch_size 16, seq length 978]\tLoss: 0.055409\n","148it [00:04, 29.23it/s]Train epoch: 108 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.060831\n","173it [00:05, 29.73it/s]Train epoch: 108 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.052800\n","199it [00:06, 27.81it/s]Train epoch: 108 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.056071\n","224it [00:07, 28.83it/s]Train epoch: 108 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.059556\n","250it [00:08, 27.85it/s]Train epoch: 108 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.066472\n","274it [00:09, 26.91it/s]Train epoch: 108 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.059476\n","298it [00:10, 24.89it/s]Train epoch: 108 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.069750\n","325it [00:11, 27.10it/s]Train epoch: 108 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.063218\n","349it [00:11, 26.30it/s]Train epoch: 108 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.070105\n","373it [00:12, 25.43it/s]Train epoch: 108 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.074813\n","400it [00:14, 24.29it/s]Train epoch: 108 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.071961\n","424it [00:15, 22.71it/s]Train epoch: 108 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.067589\n","448it [00:16, 22.79it/s]Train epoch: 108 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.072172\n","475it [00:17, 20.98it/s]Train epoch: 108 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.082831\n","499it [00:18, 18.42it/s]Train epoch: 108 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.083227\n","505it [00:18, 26.64it/s]\n","epoch loss: 0.06296099397067977\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 320.38it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3948, 0.5957, 0.5008, 0.5441, 0.8664\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4259, 0.6544, 0.5495, 0.5974, 0.8966\n","rec_at_5: 0.5574\n","prec_at_5: 0.5739\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 109\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 109 [batch #0, batch_size 16, seq length 212]\tLoss: 0.076153\n","24it [00:00, 33.21it/s]Train epoch: 109 [batch #25, batch_size 16, seq length 571]\tLoss: 0.048385\n","48it [00:01, 32.91it/s]Train epoch: 109 [batch #50, batch_size 16, seq length 709]\tLoss: 0.049088\n","72it [00:02, 31.58it/s]Train epoch: 109 [batch #75, batch_size 16, seq length 806]\tLoss: 0.063949\n","100it [00:03, 31.45it/s]Train epoch: 109 [batch #100, batch_size 16, seq length 892]\tLoss: 0.053251\n","123it [00:03, 27.99it/s]Train epoch: 109 [batch #125, batch_size 16, seq length 978]\tLoss: 0.050956\n","148it [00:04, 27.51it/s]Train epoch: 109 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.059057\n","173it [00:05, 28.62it/s]Train epoch: 109 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.052200\n","200it [00:06, 28.93it/s]Train epoch: 109 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.056413\n","224it [00:07, 24.83it/s]Train epoch: 109 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.060459\n","250it [00:08, 26.78it/s]Train epoch: 109 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.060027\n","274it [00:09, 23.87it/s]Train epoch: 109 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.059702\n","298it [00:10, 25.70it/s]Train epoch: 109 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.064837\n","325it [00:11, 24.98it/s]Train epoch: 109 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.062725\n","349it [00:12, 24.84it/s]Train epoch: 109 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.066761\n","373it [00:13, 23.14it/s]Train epoch: 109 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.074823\n","400it [00:14, 23.20it/s]Train epoch: 109 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.071879\n","424it [00:15, 23.34it/s]Train epoch: 109 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.064633\n","448it [00:16, 21.96it/s]Train epoch: 109 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.071609\n","475it [00:18, 20.24it/s]Train epoch: 109 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.081815\n","500it [00:19, 18.33it/s]Train epoch: 109 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.077342\n","505it [00:19, 25.49it/s]\n","epoch loss: 0.061248284614285324\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 318.22it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3945, 0.5956, 0.5001, 0.5436, 0.8662\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4255, 0.6545, 0.5487, 0.5970, 0.8966\n","rec_at_5: 0.5563\n","prec_at_5: 0.5734\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 110\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 110 [batch #0, batch_size 16, seq length 212]\tLoss: 0.075258\n","24it [00:00, 34.15it/s]Train epoch: 110 [batch #25, batch_size 16, seq length 571]\tLoss: 0.047728\n","48it [00:01, 31.48it/s]Train epoch: 110 [batch #50, batch_size 16, seq length 709]\tLoss: 0.049795\n","72it [00:02, 31.97it/s]Train epoch: 110 [batch #75, batch_size 16, seq length 806]\tLoss: 0.063440\n","100it [00:03, 30.30it/s]Train epoch: 110 [batch #100, batch_size 16, seq length 892]\tLoss: 0.051821\n","123it [00:03, 29.18it/s]Train epoch: 110 [batch #125, batch_size 16, seq length 978]\tLoss: 0.052460\n","148it [00:04, 29.63it/s]Train epoch: 110 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.058965\n","175it [00:05, 27.68it/s]Train epoch: 110 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.051345\n","199it [00:06, 27.15it/s]Train epoch: 110 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.052778\n","224it [00:07, 26.62it/s]Train epoch: 110 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.056613\n","248it [00:08, 26.34it/s]Train epoch: 110 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.059750\n","275it [00:09, 25.61it/s]Train epoch: 110 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.061781\n","299it [00:10, 25.87it/s]Train epoch: 110 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.066955\n","323it [00:11, 25.89it/s]Train epoch: 110 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.058654\n","350it [00:12, 23.08it/s]Train epoch: 110 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.064993\n","374it [00:13, 23.63it/s]Train epoch: 110 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.070956\n","398it [00:14, 23.05it/s]Train epoch: 110 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.065600\n","425it [00:15, 22.69it/s]Train epoch: 110 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.065044\n","449it [00:16, 22.59it/s]Train epoch: 110 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.066992\n","473it [00:18, 21.04it/s]Train epoch: 110 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.078179\n","499it [00:19, 17.91it/s]Train epoch: 110 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.077516\n","505it [00:19, 25.52it/s]\n","epoch loss: 0.05992381291851254\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 318.81it/s]\n","Finish save rediction by checkpoint  110\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3934, 0.5923, 0.4996, 0.5420, 0.8661\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4251, 0.6523, 0.5497, 0.5966, 0.8966\n","rec_at_5: 0.5592\n","prec_at_5: 0.5755\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 111\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 111 [batch #0, batch_size 16, seq length 212]\tLoss: 0.081155\n","24it [00:00, 34.14it/s]Train epoch: 111 [batch #25, batch_size 16, seq length 571]\tLoss: 0.044207\n","48it [00:01, 31.85it/s]Train epoch: 111 [batch #50, batch_size 16, seq length 709]\tLoss: 0.050461\n","72it [00:02, 32.09it/s]Train epoch: 111 [batch #75, batch_size 16, seq length 806]\tLoss: 0.061435\n","100it [00:03, 30.26it/s]Train epoch: 111 [batch #100, batch_size 16, seq length 892]\tLoss: 0.050268\n","124it [00:03, 30.28it/s]Train epoch: 111 [batch #125, batch_size 16, seq length 978]\tLoss: 0.050192\n","149it [00:05, 19.20it/s]Train epoch: 111 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.059449\n","175it [00:06, 19.79it/s]Train epoch: 111 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.049008\n","200it [00:08, 14.05it/s]Train epoch: 111 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.053402\n","224it [00:09, 16.13it/s]Train epoch: 111 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.057475\n","249it [00:10, 26.78it/s]Train epoch: 111 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.056661\n","274it [00:11, 27.10it/s]Train epoch: 111 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.056254\n","300it [00:12, 27.82it/s]Train epoch: 111 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.060334\n","324it [00:13, 26.29it/s]Train epoch: 111 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.059831\n","348it [00:14, 26.36it/s]Train epoch: 111 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.061497\n","375it [00:15, 25.20it/s]Train epoch: 111 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.070475\n","399it [00:16, 23.46it/s]Train epoch: 111 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.066250\n","423it [00:17, 23.31it/s]Train epoch: 111 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.061568\n","450it [00:18, 22.07it/s]Train epoch: 111 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.068642\n","474it [00:19, 21.47it/s]Train epoch: 111 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.076146\n","500it [00:21, 17.70it/s]Train epoch: 111 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.075361\n","505it [00:21, 23.45it/s]\n","epoch loss: 0.058030388423121804\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 314.91it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3928, 0.5912, 0.4998, 0.5417, 0.8659\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4239, 0.6507, 0.5487, 0.5954, 0.8960\n","rec_at_5: 0.5583\n","prec_at_5: 0.5748\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 112\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 112 [batch #0, batch_size 16, seq length 212]\tLoss: 0.090006\n","24it [00:00, 35.49it/s]Train epoch: 112 [batch #25, batch_size 16, seq length 571]\tLoss: 0.046406\n","48it [00:01, 32.92it/s]Train epoch: 112 [batch #50, batch_size 16, seq length 709]\tLoss: 0.048255\n","72it [00:02, 32.73it/s]Train epoch: 112 [batch #75, batch_size 16, seq length 806]\tLoss: 0.060829\n","100it [00:03, 31.89it/s]Train epoch: 112 [batch #100, batch_size 16, seq length 892]\tLoss: 0.048464\n","124it [00:03, 31.02it/s]Train epoch: 112 [batch #125, batch_size 16, seq length 978]\tLoss: 0.050749\n","148it [00:04, 30.99it/s]Train epoch: 112 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.053988\n","174it [00:05, 28.74it/s]Train epoch: 112 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.046706\n","200it [00:06, 27.67it/s]Train epoch: 112 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.051467\n","223it [00:07, 28.85it/s]Train epoch: 112 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.053488\n","249it [00:08, 28.04it/s]Train epoch: 112 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.057264\n","274it [00:09, 27.65it/s]Train epoch: 112 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.054273\n","298it [00:09, 26.50it/s]Train epoch: 112 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.060431\n","325it [00:10, 25.78it/s]Train epoch: 112 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.057212\n","349it [00:11, 25.04it/s]Train epoch: 112 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.064310\n","373it [00:12, 25.60it/s]Train epoch: 112 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.071663\n","400it [00:14, 23.85it/s]Train epoch: 112 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.068492\n","424it [00:15, 22.78it/s]Train epoch: 112 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.057786\n","448it [00:16, 22.01it/s]Train epoch: 112 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.066428\n","475it [00:17, 20.23it/s]Train epoch: 112 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.072515\n","499it [00:18, 17.44it/s]Train epoch: 112 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.071401\n","505it [00:19, 26.49it/s]\n","epoch loss: 0.056725235944521606\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 318.41it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3937, 0.5897, 0.5016, 0.5421, 0.8655\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4229, 0.6481, 0.5490, 0.5944, 0.8958\n","rec_at_5: 0.5555\n","prec_at_5: 0.5723\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 113\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 113 [batch #0, batch_size 16, seq length 212]\tLoss: 0.092771\n","24it [00:00, 34.87it/s]Train epoch: 113 [batch #25, batch_size 16, seq length 571]\tLoss: 0.045438\n","48it [00:01, 33.94it/s]Train epoch: 113 [batch #50, batch_size 16, seq length 709]\tLoss: 0.044793\n","72it [00:02, 30.92it/s]Train epoch: 113 [batch #75, batch_size 16, seq length 806]\tLoss: 0.058624\n","100it [00:03, 30.92it/s]Train epoch: 113 [batch #100, batch_size 16, seq length 892]\tLoss: 0.046978\n","124it [00:03, 31.33it/s]Train epoch: 113 [batch #125, batch_size 16, seq length 978]\tLoss: 0.050552\n","148it [00:04, 30.42it/s]Train epoch: 113 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.054161\n","172it [00:05, 29.97it/s]Train epoch: 113 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.046242\n","199it [00:06, 28.41it/s]Train epoch: 113 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.048648\n","225it [00:07, 26.96it/s]Train epoch: 113 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.055650\n","250it [00:08, 28.46it/s]Train epoch: 113 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.055265\n","274it [00:09, 27.04it/s]Train epoch: 113 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.053332\n","298it [00:09, 26.38it/s]Train epoch: 113 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.057233\n","325it [00:11, 26.30it/s]Train epoch: 113 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.054644\n","349it [00:12, 24.47it/s]Train epoch: 113 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.061564\n","373it [00:12, 24.96it/s]Train epoch: 113 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.067680\n","400it [00:14, 24.19it/s]Train epoch: 113 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.064395\n","424it [00:15, 23.01it/s]Train epoch: 113 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.061657\n","448it [00:16, 22.74it/s]Train epoch: 113 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.062516\n","475it [00:17, 20.39it/s]Train epoch: 113 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.071803\n","499it [00:18, 17.84it/s]Train epoch: 113 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.069754\n","505it [00:19, 26.41it/s]\n","epoch loss: 0.05573020519246117\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.63it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3925, 0.5904, 0.4981, 0.5403, 0.8654\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4226, 0.6506, 0.5467, 0.5942, 0.8955\n","rec_at_5: 0.5579\n","prec_at_5: 0.5753\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 114\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 114 [batch #0, batch_size 16, seq length 212]\tLoss: 0.088516\n","25it [00:00, 35.02it/s]Train epoch: 114 [batch #25, batch_size 16, seq length 571]\tLoss: 0.043749\n","49it [00:01, 34.13it/s]Train epoch: 114 [batch #50, batch_size 16, seq length 709]\tLoss: 0.043141\n","73it [00:02, 32.92it/s]Train epoch: 114 [batch #75, batch_size 16, seq length 806]\tLoss: 0.058796\n","97it [00:02, 31.51it/s]Train epoch: 114 [batch #100, batch_size 16, seq length 892]\tLoss: 0.047648\n","125it [00:03, 30.90it/s]Train epoch: 114 [batch #125, batch_size 16, seq length 978]\tLoss: 0.048687\n","149it [00:04, 31.34it/s]Train epoch: 114 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.052671\n","173it [00:05, 31.40it/s]Train epoch: 114 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.045597\n","200it [00:06, 29.46it/s]Train epoch: 114 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.048570\n","225it [00:07, 28.98it/s]Train epoch: 114 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.054805\n","249it [00:07, 27.69it/s]Train epoch: 114 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.052299\n","273it [00:08, 28.08it/s]Train epoch: 114 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.053270\n","298it [00:09, 26.92it/s]Train epoch: 114 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.058341\n","325it [00:10, 26.55it/s]Train epoch: 114 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.053374\n","349it [00:11, 26.94it/s]Train epoch: 114 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.057763\n","373it [00:12, 25.06it/s]Train epoch: 114 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.067073\n","400it [00:13, 24.43it/s]Train epoch: 114 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.062519\n","424it [00:14, 24.20it/s]Train epoch: 114 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.059138\n","448it [00:15, 23.00it/s]Train epoch: 114 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.059273\n","475it [00:16, 21.41it/s]Train epoch: 114 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.068347\n","499it [00:18, 18.44it/s]Train epoch: 114 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.070022\n","505it [00:18, 27.24it/s]\n","epoch loss: 0.054167473397798614\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 328.86it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3906, 0.5884, 0.4974, 0.5391, 0.8651\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4202, 0.6459, 0.5461, 0.5918, 0.8952\n","rec_at_5: 0.5542\n","prec_at_5: 0.5715\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 115\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 115 [batch #0, batch_size 16, seq length 212]\tLoss: 0.079015\n","24it [00:00, 35.50it/s]Train epoch: 115 [batch #25, batch_size 16, seq length 571]\tLoss: 0.042762\n","48it [00:01, 34.35it/s]Train epoch: 115 [batch #50, batch_size 16, seq length 709]\tLoss: 0.042914\n","72it [00:02, 33.35it/s]Train epoch: 115 [batch #75, batch_size 16, seq length 806]\tLoss: 0.056725\n","100it [00:02, 32.41it/s]Train epoch: 115 [batch #100, batch_size 16, seq length 892]\tLoss: 0.047137\n","124it [00:03, 32.21it/s]Train epoch: 115 [batch #125, batch_size 16, seq length 978]\tLoss: 0.046431\n","148it [00:04, 31.16it/s]Train epoch: 115 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.052091\n","172it [00:05, 30.86it/s]Train epoch: 115 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.045342\n","200it [00:06, 29.98it/s]Train epoch: 115 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.049148\n","224it [00:07, 29.99it/s]Train epoch: 115 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.051055\n","249it [00:07, 28.45it/s]Train epoch: 115 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.054945\n","275it [00:08, 27.20it/s]Train epoch: 115 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.052148\n","300it [00:09, 27.05it/s]Train epoch: 115 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.056864\n","324it [00:10, 26.57it/s]Train epoch: 115 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.054652\n","348it [00:11, 26.13it/s]Train epoch: 115 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.056454\n","375it [00:12, 25.97it/s]Train epoch: 115 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.061065\n","399it [00:13, 25.10it/s]Train epoch: 115 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.059676\n","423it [00:14, 23.48it/s]Train epoch: 115 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.058098\n","450it [00:15, 21.87it/s]Train epoch: 115 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.059990\n","474it [00:16, 21.45it/s]Train epoch: 115 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.067775\n","500it [00:18, 19.77it/s]Train epoch: 115 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.065651\n","505it [00:18, 27.30it/s]\n","epoch loss: 0.05289469646234619\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 329.05it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3917, 0.5876, 0.4994, 0.5400, 0.8650\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4208, 0.6454, 0.5473, 0.5923, 0.8949\n","rec_at_5: 0.5543\n","prec_at_5: 0.5715\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 116\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 116 [batch #0, batch_size 16, seq length 212]\tLoss: 0.072988\n","25it [00:00, 34.87it/s]Train epoch: 116 [batch #25, batch_size 16, seq length 571]\tLoss: 0.041421\n","49it [00:01, 33.84it/s]Train epoch: 116 [batch #50, batch_size 16, seq length 709]\tLoss: 0.043133\n","73it [00:02, 33.03it/s]Train epoch: 116 [batch #75, batch_size 16, seq length 806]\tLoss: 0.056829\n","97it [00:02, 31.87it/s]Train epoch: 116 [batch #100, batch_size 16, seq length 892]\tLoss: 0.044409\n","125it [00:03, 31.30it/s]Train epoch: 116 [batch #125, batch_size 16, seq length 978]\tLoss: 0.048621\n","149it [00:04, 31.10it/s]Train epoch: 116 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.052406\n","173it [00:05, 30.41it/s]Train epoch: 116 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.044983\n","200it [00:06, 29.36it/s]Train epoch: 116 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.045775\n","225it [00:07, 28.71it/s]Train epoch: 116 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.051020\n","249it [00:07, 29.37it/s]Train epoch: 116 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.054259\n","273it [00:08, 27.95it/s]Train epoch: 116 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.048797\n","298it [00:09, 27.39it/s]Train epoch: 116 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.056015\n","325it [00:10, 27.36it/s]Train epoch: 116 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.055593\n","349it [00:11, 25.26it/s]Train epoch: 116 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.056018\n","373it [00:12, 26.22it/s]Train epoch: 116 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.064393\n","400it [00:13, 18.20it/s]Train epoch: 116 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.058493\n","424it [00:15, 16.47it/s]Train epoch: 116 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.054443\n","448it [00:16, 19.62it/s]Train epoch: 116 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.059772\n","475it [00:17, 21.60it/s]Train epoch: 116 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.068961\n","499it [00:19, 19.35it/s]Train epoch: 116 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.065254\n","505it [00:19, 26.06it/s]\n","epoch loss: 0.051752840065498754\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 340.17it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3914, 0.5856, 0.4998, 0.5393, 0.8645\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4205, 0.6428, 0.5486, 0.5920, 0.8947\n","rec_at_5: 0.5527\n","prec_at_5: 0.5697\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 117\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 117 [batch #0, batch_size 16, seq length 212]\tLoss: 0.066247\n","25it [00:00, 35.37it/s]Train epoch: 117 [batch #25, batch_size 16, seq length 571]\tLoss: 0.043598\n","49it [00:01, 33.47it/s]Train epoch: 117 [batch #50, batch_size 16, seq length 709]\tLoss: 0.043293\n","73it [00:02, 33.97it/s]Train epoch: 117 [batch #75, batch_size 16, seq length 806]\tLoss: 0.055883\n","97it [00:02, 33.02it/s]Train epoch: 117 [batch #100, batch_size 16, seq length 892]\tLoss: 0.043170\n","125it [00:03, 32.06it/s]Train epoch: 117 [batch #125, batch_size 16, seq length 978]\tLoss: 0.044370\n","149it [00:04, 31.25it/s]Train epoch: 117 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.046068\n","173it [00:05, 30.81it/s]Train epoch: 117 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040805\n","200it [00:06, 28.90it/s]Train epoch: 117 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.044647\n","223it [00:07, 28.69it/s]Train epoch: 117 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.047019\n","249it [00:07, 28.45it/s]Train epoch: 117 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.052843\n","275it [00:08, 28.03it/s]Train epoch: 117 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.050112\n","299it [00:09, 26.58it/s]Train epoch: 117 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.050908\n","324it [00:10, 26.60it/s]Train epoch: 117 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.048588\n","348it [00:11, 27.14it/s]Train epoch: 117 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.053898\n","375it [00:12, 25.58it/s]Train epoch: 117 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.063752\n","399it [00:13, 24.71it/s]Train epoch: 117 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.058986\n","423it [00:14, 24.39it/s]Train epoch: 117 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.052227\n","450it [00:15, 23.45it/s]Train epoch: 117 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.057321\n","474it [00:16, 21.73it/s]Train epoch: 117 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.063910\n","500it [00:18, 19.33it/s]Train epoch: 117 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.061759\n","505it [00:18, 27.24it/s]\n","epoch loss: 0.05022683513562868\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.29it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3906, 0.5834, 0.5002, 0.5386, 0.8642\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4202, 0.6417, 0.5490, 0.5917, 0.8943\n","rec_at_5: 0.5522\n","prec_at_5: 0.5685\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 118\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 118 [batch #0, batch_size 16, seq length 212]\tLoss: 0.085375\n","24it [00:00, 35.07it/s]Train epoch: 118 [batch #25, batch_size 16, seq length 571]\tLoss: 0.039663\n","48it [00:01, 34.64it/s]Train epoch: 118 [batch #50, batch_size 16, seq length 709]\tLoss: 0.041037\n","72it [00:02, 33.97it/s]Train epoch: 118 [batch #75, batch_size 16, seq length 806]\tLoss: 0.050122\n","100it [00:02, 32.00it/s]Train epoch: 118 [batch #100, batch_size 16, seq length 892]\tLoss: 0.044391\n","124it [00:03, 30.98it/s]Train epoch: 118 [batch #125, batch_size 16, seq length 978]\tLoss: 0.043653\n","148it [00:04, 32.04it/s]Train epoch: 118 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.049246\n","172it [00:05, 30.66it/s]Train epoch: 118 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040391\n","197it [00:06, 29.07it/s]Train epoch: 118 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.044466\n","225it [00:07, 27.74it/s]Train epoch: 118 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.047296\n","249it [00:07, 28.15it/s]Train epoch: 118 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.049246\n","273it [00:08, 28.46it/s]Train epoch: 118 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.047947\n","298it [00:09, 27.02it/s]Train epoch: 118 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.052579\n","325it [00:10, 25.54it/s]Train epoch: 118 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.050105\n","349it [00:11, 25.47it/s]Train epoch: 118 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.054468\n","373it [00:12, 24.04it/s]Train epoch: 118 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.057985\n","400it [00:13, 24.81it/s]Train epoch: 118 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.056049\n","424it [00:14, 23.54it/s]Train epoch: 118 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.051837\n","448it [00:15, 22.08it/s]Train epoch: 118 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.055049\n","475it [00:17, 21.00it/s]Train epoch: 118 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.061071\n","499it [00:18, 19.45it/s]Train epoch: 118 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.061736\n","505it [00:18, 27.18it/s]\n","epoch loss: 0.04913021775958414\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.96it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3912, 0.5832, 0.5014, 0.5392, 0.8640\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4198, 0.6412, 0.5487, 0.5914, 0.8942\n","rec_at_5: 0.5533\n","prec_at_5: 0.5696\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 119\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 119 [batch #0, batch_size 16, seq length 212]\tLoss: 0.073280\n","25it [00:00, 36.90it/s]Train epoch: 119 [batch #25, batch_size 16, seq length 571]\tLoss: 0.040494\n","49it [00:01, 33.94it/s]Train epoch: 119 [batch #50, batch_size 16, seq length 709]\tLoss: 0.036155\n","73it [00:02, 33.31it/s]Train epoch: 119 [batch #75, batch_size 16, seq length 806]\tLoss: 0.053905\n","97it [00:02, 32.80it/s]Train epoch: 119 [batch #100, batch_size 16, seq length 892]\tLoss: 0.042278\n","125it [00:03, 31.86it/s]Train epoch: 119 [batch #125, batch_size 16, seq length 978]\tLoss: 0.042201\n","149it [00:04, 30.17it/s]Train epoch: 119 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.050354\n","173it [00:05, 30.12it/s]Train epoch: 119 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.042081\n","200it [00:06, 29.60it/s]Train epoch: 119 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.041262\n","223it [00:07, 30.05it/s]Train epoch: 119 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.046398\n","247it [00:07, 27.82it/s]Train epoch: 119 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.051486\n","274it [00:08, 27.12it/s]Train epoch: 119 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.046998\n","298it [00:09, 27.44it/s]Train epoch: 119 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.049874\n","325it [00:10, 26.26it/s]Train epoch: 119 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.047957\n","349it [00:11, 25.55it/s]Train epoch: 119 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.049642\n","375it [00:13, 12.98it/s]Train epoch: 119 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.057880\n","399it [00:14, 15.44it/s]Train epoch: 119 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.052067\n","425it [00:16, 16.00it/s]Train epoch: 119 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.049129\n","450it [00:18, 17.43it/s]Train epoch: 119 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.054358\n","474it [00:19, 20.28it/s]Train epoch: 119 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.059614\n","500it [00:20, 18.59it/s]Train epoch: 119 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.060171\n","505it [00:20, 24.27it/s]\n","epoch loss: 0.047793968647976616\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.74it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3898, 0.5806, 0.5010, 0.5379, 0.8640\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4188, 0.6394, 0.5483, 0.5904, 0.8940\n","rec_at_5: 0.5548\n","prec_at_5: 0.5709\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 120\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 120 [batch #0, batch_size 16, seq length 212]\tLoss: 0.080043\n","24it [00:00, 35.52it/s]Train epoch: 120 [batch #25, batch_size 16, seq length 571]\tLoss: 0.039143\n","48it [00:01, 34.55it/s]Train epoch: 120 [batch #50, batch_size 16, seq length 709]\tLoss: 0.037404\n","72it [00:02, 32.66it/s]Train epoch: 120 [batch #75, batch_size 16, seq length 806]\tLoss: 0.051602\n","100it [00:02, 31.98it/s]Train epoch: 120 [batch #100, batch_size 16, seq length 892]\tLoss: 0.044160\n","124it [00:03, 31.16it/s]Train epoch: 120 [batch #125, batch_size 16, seq length 978]\tLoss: 0.041396\n","148it [00:04, 30.20it/s]Train epoch: 120 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.046236\n","174it [00:05, 30.29it/s]Train epoch: 120 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.040451\n","200it [00:06, 29.77it/s]Train epoch: 120 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.041345\n","223it [00:07, 27.77it/s]Train epoch: 120 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.043255\n","250it [00:08, 28.86it/s]Train epoch: 120 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.047000\n","275it [00:08, 28.30it/s]Train epoch: 120 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.046517\n","299it [00:09, 25.90it/s]Train epoch: 120 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.048637\n","323it [00:10, 25.93it/s]Train epoch: 120 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.046651\n","350it [00:11, 25.87it/s]Train epoch: 120 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.049799\n","374it [00:12, 25.76it/s]Train epoch: 120 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.055666\n","398it [00:13, 24.08it/s]Train epoch: 120 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.055934\n","425it [00:14, 23.93it/s]Train epoch: 120 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.050878\n","449it [00:16, 22.61it/s]Train epoch: 120 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.053109\n","473it [00:17, 21.44it/s]Train epoch: 120 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.060784\n","500it [00:18, 19.30it/s]Train epoch: 120 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.057575\n","505it [00:18, 26.93it/s]\n","epoch loss: 0.04679143296573127\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.02it/s]\n","Finish save rediction by checkpoint  120\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3894, 0.5824, 0.5002, 0.5382, 0.8636\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4176, 0.6374, 0.5477, 0.5891, 0.8937\n","rec_at_5: 0.5523\n","prec_at_5: 0.5682\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 121\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 121 [batch #0, batch_size 16, seq length 212]\tLoss: 0.062755\n","25it [00:00, 35.10it/s]Train epoch: 121 [batch #25, batch_size 16, seq length 571]\tLoss: 0.038047\n","49it [00:01, 33.74it/s]Train epoch: 121 [batch #50, batch_size 16, seq length 709]\tLoss: 0.039903\n","73it [00:02, 33.27it/s]Train epoch: 121 [batch #75, batch_size 16, seq length 806]\tLoss: 0.052294\n","97it [00:02, 30.86it/s]Train epoch: 121 [batch #100, batch_size 16, seq length 892]\tLoss: 0.042676\n","125it [00:03, 30.76it/s]Train epoch: 121 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039445\n","149it [00:04, 31.23it/s]Train epoch: 121 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.044910\n","173it [00:05, 29.99it/s]Train epoch: 121 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.039151\n","197it [00:06, 30.24it/s]Train epoch: 121 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.042268\n","224it [00:07, 29.06it/s]Train epoch: 121 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.044133\n","249it [00:07, 28.47it/s]Train epoch: 121 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.044187\n","275it [00:08, 26.90it/s]Train epoch: 121 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.045414\n","300it [00:09, 25.39it/s]Train epoch: 121 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.047483\n","324it [00:10, 25.64it/s]Train epoch: 121 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.047241\n","348it [00:11, 25.78it/s]Train epoch: 121 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.048602\n","375it [00:12, 25.04it/s]Train epoch: 121 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.052282\n","399it [00:13, 24.53it/s]Train epoch: 121 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.051708\n","423it [00:14, 23.93it/s]Train epoch: 121 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.050341\n","450it [00:16, 22.27it/s]Train epoch: 121 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.051736\n","474it [00:17, 21.66it/s]Train epoch: 121 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.058879\n","500it [00:18, 19.60it/s]Train epoch: 121 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.056072\n","505it [00:18, 26.95it/s]\n","epoch loss: 0.04579678014759896\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 331.60it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3905, 0.5830, 0.5009, 0.5388, 0.8636\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4187, 0.6388, 0.5485, 0.5902, 0.8935\n","rec_at_5: 0.5521\n","prec_at_5: 0.5671\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 122\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 122 [batch #0, batch_size 16, seq length 212]\tLoss: 0.069172\n","24it [00:00, 35.96it/s]Train epoch: 122 [batch #25, batch_size 16, seq length 571]\tLoss: 0.037248\n","48it [00:01, 32.96it/s]Train epoch: 122 [batch #50, batch_size 16, seq length 709]\tLoss: 0.036173\n","72it [00:02, 33.18it/s]Train epoch: 122 [batch #75, batch_size 16, seq length 806]\tLoss: 0.049705\n","100it [00:02, 32.74it/s]Train epoch: 122 [batch #100, batch_size 16, seq length 892]\tLoss: 0.039231\n","124it [00:03, 32.41it/s]Train epoch: 122 [batch #125, batch_size 16, seq length 978]\tLoss: 0.039516\n","148it [00:04, 30.75it/s]Train epoch: 122 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.042944\n","175it [00:05, 28.87it/s]Train epoch: 122 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.041376\n","199it [00:06, 29.25it/s]Train epoch: 122 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.039861\n","225it [00:07, 28.04it/s]Train epoch: 122 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.043501\n","248it [00:07, 28.40it/s]Train epoch: 122 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.045272\n","273it [00:08, 28.72it/s]Train epoch: 122 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.042910\n","300it [00:09, 27.01it/s]Train epoch: 122 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.045888\n","324it [00:10, 27.21it/s]Train epoch: 122 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.044460\n","348it [00:11, 25.55it/s]Train epoch: 122 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.048909\n","375it [00:12, 23.29it/s]Train epoch: 122 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.053774\n","399it [00:13, 24.39it/s]Train epoch: 122 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.048452\n","423it [00:14, 23.68it/s]Train epoch: 122 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.044523\n","450it [00:16, 21.75it/s]Train epoch: 122 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.050536\n","474it [00:17, 21.11it/s]Train epoch: 122 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.056898\n","500it [00:18, 18.95it/s]Train epoch: 122 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.055731\n","505it [00:18, 26.90it/s]\n","epoch loss: 0.04416714436835655\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.36it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3899, 0.5813, 0.5013, 0.5383, 0.8632\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4192, 0.6389, 0.5493, 0.5907, 0.8932\n","rec_at_5: 0.5553\n","prec_at_5: 0.5700\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 123\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 123 [batch #0, batch_size 16, seq length 212]\tLoss: 0.062365\n","24it [00:00, 36.31it/s]Train epoch: 123 [batch #25, batch_size 16, seq length 571]\tLoss: 0.034692\n","48it [00:01, 32.59it/s]Train epoch: 123 [batch #50, batch_size 16, seq length 709]\tLoss: 0.037022\n","72it [00:02, 33.59it/s]Train epoch: 123 [batch #75, batch_size 16, seq length 806]\tLoss: 0.049920\n","100it [00:02, 32.79it/s]Train epoch: 123 [batch #100, batch_size 16, seq length 892]\tLoss: 0.039705\n","124it [00:03, 32.49it/s]Train epoch: 123 [batch #125, batch_size 16, seq length 978]\tLoss: 0.040700\n","148it [00:04, 32.02it/s]Train epoch: 123 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.038944\n","172it [00:05, 30.91it/s]Train epoch: 123 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.037063\n","200it [00:06, 30.85it/s]Train epoch: 123 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.039399\n","224it [00:07, 29.06it/s]Train epoch: 123 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.043902\n","249it [00:07, 29.02it/s]Train epoch: 123 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.045088\n","273it [00:08, 27.81it/s]Train epoch: 123 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.039911\n","299it [00:09, 26.74it/s]Train epoch: 123 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.046659\n","324it [00:10, 26.36it/s]Train epoch: 123 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.043444\n","348it [00:11, 26.44it/s]Train epoch: 123 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.046955\n","375it [00:12, 25.42it/s]Train epoch: 123 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.052629\n","399it [00:13, 25.10it/s]Train epoch: 123 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.049031\n","423it [00:14, 23.40it/s]Train epoch: 123 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.042624\n","450it [00:15, 23.06it/s]Train epoch: 123 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.048713\n","474it [00:16, 20.98it/s]Train epoch: 123 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.054399\n","500it [00:18, 19.30it/s]Train epoch: 123 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.052856\n","505it [00:18, 27.43it/s]\n","epoch loss: 0.04305940525753811\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3873, 0.5780, 0.4982, 0.5352, 0.8631\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4164, 0.6378, 0.5454, 0.5880, 0.8931\n","rec_at_5: 0.5539\n","prec_at_5: 0.5688\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 124\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 124 [batch #0, batch_size 16, seq length 212]\tLoss: 0.072345\n","25it [00:00, 37.23it/s]Train epoch: 124 [batch #25, batch_size 16, seq length 571]\tLoss: 0.036238\n","49it [00:01, 34.59it/s]Train epoch: 124 [batch #50, batch_size 16, seq length 709]\tLoss: 0.033166\n","73it [00:02, 32.68it/s]Train epoch: 124 [batch #75, batch_size 16, seq length 806]\tLoss: 0.049509\n","97it [00:02, 31.69it/s]Train epoch: 124 [batch #100, batch_size 16, seq length 892]\tLoss: 0.040578\n","125it [00:03, 31.47it/s]Train epoch: 124 [batch #125, batch_size 16, seq length 978]\tLoss: 0.036085\n","149it [00:04, 31.71it/s]Train epoch: 124 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.041801\n","173it [00:05, 30.87it/s]Train epoch: 124 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.035553\n","197it [00:06, 30.53it/s]Train epoch: 124 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.038462\n","225it [00:07, 28.21it/s]Train epoch: 124 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.043110\n","247it [00:07, 28.73it/s]Train epoch: 124 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.041824\n","274it [00:08, 27.60it/s]Train epoch: 124 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.040777\n","300it [00:09, 27.29it/s]Train epoch: 124 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.046673\n","324it [00:10, 27.63it/s]Train epoch: 124 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.043647\n","348it [00:11, 27.24it/s]Train epoch: 124 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.044530\n","375it [00:12, 24.41it/s]Train epoch: 124 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.050377\n","399it [00:13, 25.32it/s]Train epoch: 124 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.047389\n","423it [00:14, 23.14it/s]Train epoch: 124 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.044285\n","450it [00:15, 23.21it/s]Train epoch: 124 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.047317\n","474it [00:16, 22.27it/s]Train epoch: 124 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.051422\n","498it [00:17, 19.55it/s]Train epoch: 124 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.051661\n","505it [00:18, 27.51it/s]\n","epoch loss: 0.04196621767835378\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 337.57it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3866, 0.5799, 0.4969, 0.5352, 0.8629\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4161, 0.6374, 0.5452, 0.5877, 0.8928\n","rec_at_5: 0.5543\n","prec_at_5: 0.5682\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 125\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 125 [batch #0, batch_size 16, seq length 212]\tLoss: 0.079569\n","25it [00:00, 36.23it/s]Train epoch: 125 [batch #25, batch_size 16, seq length 571]\tLoss: 0.032250\n","49it [00:01, 34.71it/s]Train epoch: 125 [batch #50, batch_size 16, seq length 709]\tLoss: 0.034411\n","73it [00:02, 32.24it/s]Train epoch: 125 [batch #75, batch_size 16, seq length 806]\tLoss: 0.046751\n","97it [00:02, 33.66it/s]Train epoch: 125 [batch #100, batch_size 16, seq length 892]\tLoss: 0.038141\n","125it [00:03, 32.43it/s]Train epoch: 125 [batch #125, batch_size 16, seq length 978]\tLoss: 0.040583\n","149it [00:04, 30.85it/s]Train epoch: 125 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.041824\n","173it [00:05, 30.18it/s]Train epoch: 125 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.035299\n","197it [00:06, 29.79it/s]Train epoch: 125 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.036663\n","223it [00:06, 29.72it/s]Train epoch: 125 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.039450\n","249it [00:07, 28.93it/s]Train epoch: 125 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.042008\n","275it [00:08, 26.89it/s]Train epoch: 125 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.040932\n","299it [00:09, 26.51it/s]Train epoch: 125 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.041352\n","323it [00:10, 27.71it/s]Train epoch: 125 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.041046\n","350it [00:11, 26.34it/s]Train epoch: 125 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.045657\n","374it [00:12, 24.34it/s]Train epoch: 125 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.047260\n","398it [00:13, 24.65it/s]Train epoch: 125 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.048600\n","425it [00:14, 23.97it/s]Train epoch: 125 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.044019\n","449it [00:15, 22.24it/s]Train epoch: 125 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.047014\n","473it [00:16, 22.69it/s]Train epoch: 125 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.052615\n","500it [00:18, 20.01it/s]Train epoch: 125 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.050731\n","505it [00:18, 27.47it/s]\n","epoch loss: 0.04132834329998287\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 339.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3886, 0.5803, 0.4993, 0.5368, 0.8628\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4177, 0.6378, 0.5476, 0.5893, 0.8928\n","rec_at_5: 0.5507\n","prec_at_5: 0.5659\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 126\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 126 [batch #0, batch_size 16, seq length 212]\tLoss: 0.064943\n","24it [00:00, 36.05it/s]Train epoch: 126 [batch #25, batch_size 16, seq length 571]\tLoss: 0.036031\n","48it [00:01, 33.28it/s]Train epoch: 126 [batch #50, batch_size 16, seq length 709]\tLoss: 0.031585\n","72it [00:02, 32.78it/s]Train epoch: 126 [batch #75, batch_size 16, seq length 806]\tLoss: 0.044473\n","100it [00:02, 33.50it/s]Train epoch: 126 [batch #100, batch_size 16, seq length 892]\tLoss: 0.039403\n","124it [00:03, 32.90it/s]Train epoch: 126 [batch #125, batch_size 16, seq length 978]\tLoss: 0.038088\n","148it [00:04, 31.12it/s]Train epoch: 126 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.037332\n","172it [00:05, 30.11it/s]Train epoch: 126 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.033286\n","200it [00:06, 29.50it/s]Train epoch: 126 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.037390\n","224it [00:07, 29.87it/s]Train epoch: 126 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.041730\n","248it [00:07, 29.56it/s]Train epoch: 126 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.042186\n","275it [00:08, 27.91it/s]Train epoch: 126 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.040563\n","299it [00:09, 26.65it/s]Train epoch: 126 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.042443\n","324it [00:10, 27.64it/s]Train epoch: 126 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.037679\n","348it [00:11, 26.82it/s]Train epoch: 126 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.041641\n","375it [00:12, 25.29it/s]Train epoch: 126 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.045984\n","399it [00:13, 25.34it/s]Train epoch: 126 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.045327\n","423it [00:14, 23.66it/s]Train epoch: 126 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.041125\n","450it [00:15, 22.19it/s]Train epoch: 126 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.046571\n","474it [00:16, 20.90it/s]Train epoch: 126 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.051146\n","498it [00:17, 19.71it/s]Train epoch: 126 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.047525\n","505it [00:18, 27.57it/s]\n","epoch loss: 0.04003182501246286\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.12it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3877, 0.5828, 0.4970, 0.5365, 0.8626\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4168, 0.6389, 0.5453, 0.5884, 0.8925\n","rec_at_5: 0.5509\n","prec_at_5: 0.5669\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 127\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 127 [batch #0, batch_size 16, seq length 212]\tLoss: 0.054238\n","22it [00:00, 35.87it/s]Train epoch: 127 [batch #25, batch_size 16, seq length 571]\tLoss: 0.032360\n","50it [00:01, 35.18it/s]Train epoch: 127 [batch #50, batch_size 16, seq length 709]\tLoss: 0.034565\n","74it [00:02, 33.83it/s]Train epoch: 127 [batch #75, batch_size 16, seq length 806]\tLoss: 0.045753\n","98it [00:02, 32.50it/s]Train epoch: 127 [batch #100, batch_size 16, seq length 892]\tLoss: 0.036707\n","122it [00:03, 31.26it/s]Train epoch: 127 [batch #125, batch_size 16, seq length 978]\tLoss: 0.035469\n","150it [00:04, 31.95it/s]Train epoch: 127 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.038435\n","174it [00:05, 31.25it/s]Train epoch: 127 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.032618\n","198it [00:05, 30.62it/s]Train epoch: 127 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.034531\n","225it [00:06, 30.04it/s]Train epoch: 127 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.041131\n","247it [00:07, 29.26it/s]Train epoch: 127 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.042505\n","272it [00:08, 27.10it/s]Train epoch: 127 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.038060\n","299it [00:09, 27.83it/s]Train epoch: 127 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.041823\n","324it [00:10, 27.81it/s]Train epoch: 127 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.040543\n","348it [00:11, 27.52it/s]Train epoch: 127 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.044264\n","375it [00:12, 26.99it/s]Train epoch: 127 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.044261\n","399it [00:13, 25.05it/s]Train epoch: 127 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.044390\n","423it [00:14, 24.32it/s]Train epoch: 127 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.040186\n","450it [00:15, 20.93it/s]Train epoch: 127 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.046225\n","475it [00:17, 14.34it/s]Train epoch: 127 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.049392\n","499it [00:18, 17.30it/s]Train epoch: 127 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.047543\n","505it [00:19, 26.34it/s]\n","epoch loss: 0.03935528141400307\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:05, 289.05it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3876, 0.5785, 0.4982, 0.5354, 0.8622\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4161, 0.6358, 0.5464, 0.5877, 0.8923\n","rec_at_5: 0.5513\n","prec_at_5: 0.5668\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 128\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 128 [batch #0, batch_size 16, seq length 212]\tLoss: 0.068479\n","24it [00:01, 20.13it/s]Train epoch: 128 [batch #25, batch_size 16, seq length 571]\tLoss: 0.033533\n","49it [00:02, 20.50it/s]Train epoch: 128 [batch #50, batch_size 16, seq length 709]\tLoss: 0.028841\n","73it [00:03, 30.43it/s]Train epoch: 128 [batch #75, batch_size 16, seq length 806]\tLoss: 0.046781\n","97it [00:03, 32.53it/s]Train epoch: 128 [batch #100, batch_size 16, seq length 892]\tLoss: 0.035634\n","125it [00:04, 31.78it/s]Train epoch: 128 [batch #125, batch_size 16, seq length 978]\tLoss: 0.036502\n","149it [00:05, 30.93it/s]Train epoch: 128 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.040184\n","173it [00:06, 30.42it/s]Train epoch: 128 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.032713\n","198it [00:07, 29.05it/s]Train epoch: 128 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.033260\n","223it [00:08, 29.35it/s]Train epoch: 128 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.037613\n","249it [00:08, 27.89it/s]Train epoch: 128 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.037172\n","274it [00:09, 28.06it/s]Train epoch: 128 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.036272\n","298it [00:10, 28.08it/s]Train epoch: 128 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.041143\n","323it [00:11, 27.58it/s]Train epoch: 128 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.038191\n","350it [00:12, 25.94it/s]Train epoch: 128 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.042307\n","374it [00:13, 25.55it/s]Train epoch: 128 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.044461\n","398it [00:14, 24.96it/s]Train epoch: 128 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.042924\n","425it [00:15, 23.94it/s]Train epoch: 128 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.038857\n","449it [00:16, 23.31it/s]Train epoch: 128 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.043740\n","473it [00:17, 21.21it/s]Train epoch: 128 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.046761\n","500it [00:19, 19.44it/s]Train epoch: 128 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.045938\n","505it [00:19, 26.05it/s]\n","epoch loss: 0.03818834191142111\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 342.62it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3878, 0.5793, 0.4982, 0.5357, 0.8621\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4166, 0.6350, 0.5478, 0.5882, 0.8921\n","rec_at_5: 0.5503\n","prec_at_5: 0.5663\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 129\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 129 [batch #0, batch_size 16, seq length 212]\tLoss: 0.059641\n","23it [00:00, 38.73it/s]Train epoch: 129 [batch #25, batch_size 16, seq length 571]\tLoss: 0.029988\n","47it [00:01, 34.80it/s]Train epoch: 129 [batch #50, batch_size 16, seq length 709]\tLoss: 0.030631\n","75it [00:02, 33.23it/s]Train epoch: 129 [batch #75, batch_size 16, seq length 806]\tLoss: 0.041875\n","99it [00:02, 32.13it/s]Train epoch: 129 [batch #100, batch_size 16, seq length 892]\tLoss: 0.034639\n","123it [00:03, 32.02it/s]Train epoch: 129 [batch #125, batch_size 16, seq length 978]\tLoss: 0.035432\n","147it [00:04, 31.40it/s]Train epoch: 129 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.035399\n","175it [00:05, 31.04it/s]Train epoch: 129 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.029524\n","199it [00:06, 29.69it/s]Train epoch: 129 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.033110\n","223it [00:06, 29.89it/s]Train epoch: 129 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.034169\n","249it [00:07, 28.44it/s]Train epoch: 129 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.038941\n","273it [00:08, 28.81it/s]Train epoch: 129 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.037213\n","299it [00:09, 27.81it/s]Train epoch: 129 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.037035\n","323it [00:10, 27.70it/s]Train epoch: 129 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.036301\n","350it [00:11, 26.11it/s]Train epoch: 129 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.038060\n","374it [00:12, 26.22it/s]Train epoch: 129 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.045132\n","398it [00:13, 24.29it/s]Train epoch: 129 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.040867\n","425it [00:14, 24.22it/s]Train epoch: 129 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.038112\n","449it [00:15, 21.91it/s]Train epoch: 129 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.039629\n","473it [00:16, 20.76it/s]Train epoch: 129 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.049263\n","498it [00:17, 18.44it/s]Train epoch: 129 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.046308\n","505it [00:18, 27.56it/s]\n","epoch loss: 0.03717218278884187\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.47it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3884, 0.5778, 0.4994, 0.5358, 0.8618\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4168, 0.6335, 0.5492, 0.5883, 0.8919\n","rec_at_5: 0.5494\n","prec_at_5: 0.5662\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 130\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 130 [batch #0, batch_size 16, seq length 212]\tLoss: 0.060874\n","22it [00:00, 37.14it/s]Train epoch: 130 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030408\n","50it [00:01, 34.07it/s]Train epoch: 130 [batch #50, batch_size 16, seq length 709]\tLoss: 0.028958\n","74it [00:02, 33.38it/s]Train epoch: 130 [batch #75, batch_size 16, seq length 806]\tLoss: 0.042898\n","98it [00:02, 32.91it/s]Train epoch: 130 [batch #100, batch_size 16, seq length 892]\tLoss: 0.035262\n","122it [00:03, 32.91it/s]Train epoch: 130 [batch #125, batch_size 16, seq length 978]\tLoss: 0.034425\n","150it [00:04, 31.82it/s]Train epoch: 130 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.037768\n","174it [00:05, 30.61it/s]Train epoch: 130 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.032017\n","198it [00:06, 29.73it/s]Train epoch: 130 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.030619\n","222it [00:06, 30.17it/s]Train epoch: 130 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.037406\n","249it [00:07, 28.17it/s]Train epoch: 130 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.039366\n","273it [00:08, 28.41it/s]Train epoch: 130 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.033703\n","300it [00:09, 27.99it/s]Train epoch: 130 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.034713\n","324it [00:10, 25.37it/s]Train epoch: 130 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.033847\n","348it [00:11, 22.63it/s]Train epoch: 130 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.039146\n","375it [00:12, 25.91it/s]Train epoch: 130 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.042168\n","399it [00:13, 22.06it/s]Train epoch: 130 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.042802\n","423it [00:14, 22.95it/s]Train epoch: 130 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.036105\n","450it [00:16, 21.66it/s]Train epoch: 130 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.041230\n","474it [00:17, 20.22it/s]Train epoch: 130 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.045068\n","498it [00:18, 19.43it/s]Train epoch: 130 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.042882\n","505it [00:18, 26.82it/s]\n","epoch loss: 0.03633814688592153\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 315.63it/s]\n","Finish save rediction by checkpoint  130\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3880, 0.5779, 0.5000, 0.5361, 0.8616\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4161, 0.6326, 0.5486, 0.5876, 0.8917\n","rec_at_5: 0.5503\n","prec_at_5: 0.5666\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 131\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 131 [batch #0, batch_size 16, seq length 212]\tLoss: 0.067687\n","24it [00:00, 34.66it/s]Train epoch: 131 [batch #25, batch_size 16, seq length 571]\tLoss: 0.031344\n","48it [00:01, 32.17it/s]Train epoch: 131 [batch #50, batch_size 16, seq length 709]\tLoss: 0.027023\n","72it [00:02, 30.54it/s]Train epoch: 131 [batch #75, batch_size 16, seq length 806]\tLoss: 0.041762\n","100it [00:03, 31.44it/s]Train epoch: 131 [batch #100, batch_size 16, seq length 892]\tLoss: 0.032143\n","124it [00:03, 30.26it/s]Train epoch: 131 [batch #125, batch_size 16, seq length 978]\tLoss: 0.033231\n","147it [00:04, 29.74it/s]Train epoch: 131 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.032844\n","174it [00:05, 28.11it/s]Train epoch: 131 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.028622\n","199it [00:06, 27.96it/s]Train epoch: 131 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.030173\n","223it [00:07, 27.35it/s]Train epoch: 131 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.035868\n","250it [00:08, 26.82it/s]Train epoch: 131 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.038031\n","274it [00:09, 27.78it/s]Train epoch: 131 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.034520\n","298it [00:10, 25.48it/s]Train epoch: 131 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.035793\n","325it [00:11, 25.73it/s]Train epoch: 131 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.037333\n","349it [00:12, 25.20it/s]Train epoch: 131 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.038275\n","373it [00:13, 23.54it/s]Train epoch: 131 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.041149\n","400it [00:14, 23.51it/s]Train epoch: 131 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.040506\n","424it [00:15, 23.49it/s]Train epoch: 131 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.036418\n","448it [00:16, 22.32it/s]Train epoch: 131 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.038124\n","475it [00:17, 19.83it/s]Train epoch: 131 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.045469\n","499it [00:18, 18.40it/s]Train epoch: 131 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.043993\n","505it [00:19, 26.17it/s]\n","epoch loss: 0.03505912540955377\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 320.54it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3869, 0.5746, 0.5000, 0.5347, 0.8612\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4155, 0.6304, 0.5494, 0.5871, 0.8914\n","rec_at_5: 0.5488\n","prec_at_5: 0.5659\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 132\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 132 [batch #0, batch_size 16, seq length 212]\tLoss: 0.065112\n","24it [00:00, 35.03it/s]Train epoch: 132 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030763\n","48it [00:01, 33.35it/s]Train epoch: 132 [batch #50, batch_size 16, seq length 709]\tLoss: 0.029646\n","72it [00:02, 31.60it/s]Train epoch: 132 [batch #75, batch_size 16, seq length 806]\tLoss: 0.038281\n","100it [00:03, 31.14it/s]Train epoch: 132 [batch #100, batch_size 16, seq length 892]\tLoss: 0.034607\n","124it [00:03, 29.99it/s]Train epoch: 132 [batch #125, batch_size 16, seq length 978]\tLoss: 0.033010\n","148it [00:04, 28.90it/s]Train epoch: 132 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.034609\n","175it [00:05, 27.43it/s]Train epoch: 132 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.026939\n","199it [00:06, 27.29it/s]Train epoch: 132 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.031710\n","224it [00:07, 26.41it/s]Train epoch: 132 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.034409\n","248it [00:08, 24.76it/s]Train epoch: 132 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.036209\n","275it [00:09, 24.98it/s]Train epoch: 132 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.033438\n","299it [00:10, 24.43it/s]Train epoch: 132 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.034101\n","323it [00:11, 25.05it/s]Train epoch: 132 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.032315\n","350it [00:12, 24.16it/s]Train epoch: 132 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.036020\n","374it [00:13, 22.61it/s]Train epoch: 132 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.039092\n","398it [00:14, 23.35it/s]Train epoch: 132 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.036141\n","425it [00:15, 21.78it/s]Train epoch: 132 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.037620\n","449it [00:17, 20.81it/s]Train epoch: 132 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.036136\n","473it [00:18, 19.45it/s]Train epoch: 132 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.041426\n","500it [00:19, 16.98it/s]Train epoch: 132 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.039378\n","505it [00:20, 25.11it/s]\n","epoch loss: 0.03409210689052349\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:05, 309.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3860, 0.5738, 0.4992, 0.5339, 0.8610\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4145, 0.6301, 0.5479, 0.5861, 0.8912\n","rec_at_5: 0.5496\n","prec_at_5: 0.5668\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 133\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 133 [batch #0, batch_size 16, seq length 212]\tLoss: 0.047537\n","24it [00:00, 32.35it/s]Train epoch: 133 [batch #25, batch_size 16, seq length 571]\tLoss: 0.025805\n","48it [00:01, 30.67it/s]Train epoch: 133 [batch #50, batch_size 16, seq length 709]\tLoss: 0.029706\n","72it [00:02, 30.40it/s]Train epoch: 133 [batch #75, batch_size 16, seq length 806]\tLoss: 0.042901\n","100it [00:03, 29.77it/s]Train epoch: 133 [batch #100, batch_size 16, seq length 892]\tLoss: 0.033018\n","124it [00:04, 30.41it/s]Train epoch: 133 [batch #125, batch_size 16, seq length 978]\tLoss: 0.032696\n","150it [00:04, 28.61it/s]Train epoch: 133 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.035679\n","175it [00:05, 28.32it/s]Train epoch: 133 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.028472\n","198it [00:06, 28.34it/s]Train epoch: 133 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.029520\n","225it [00:07, 26.34it/s]Train epoch: 133 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.033463\n","249it [00:08, 26.51it/s]Train epoch: 133 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.033760\n","273it [00:09, 25.27it/s]Train epoch: 133 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.036078\n","300it [00:10, 24.89it/s]Train epoch: 133 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.034409\n","324it [00:11, 25.13it/s]Train epoch: 133 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.035797\n","348it [00:12, 23.21it/s]Train epoch: 133 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.034488\n","375it [00:13, 23.99it/s]Train epoch: 133 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.038789\n","399it [00:14, 23.04it/s]Train epoch: 133 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.035283\n","423it [00:15, 22.18it/s]Train epoch: 133 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.033624\n","450it [00:16, 21.92it/s]Train epoch: 133 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.034683\n","474it [00:18, 20.44it/s]Train epoch: 133 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.040078\n","499it [00:19, 18.48it/s]Train epoch: 133 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.038157\n","505it [00:19, 25.52it/s]\n","epoch loss: 0.03342488119102056\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 321.66it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3866, 0.5743, 0.5003, 0.5348, 0.8609\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4160, 0.6295, 0.5509, 0.5876, 0.8911\n","rec_at_5: 0.5489\n","prec_at_5: 0.5671\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 134\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 134 [batch #0, batch_size 16, seq length 212]\tLoss: 0.051466\n","24it [00:00, 33.41it/s]Train epoch: 134 [batch #25, batch_size 16, seq length 571]\tLoss: 0.033282\n","48it [00:01, 33.15it/s]Train epoch: 134 [batch #50, batch_size 16, seq length 709]\tLoss: 0.027706\n","72it [00:02, 30.24it/s]Train epoch: 134 [batch #75, batch_size 16, seq length 806]\tLoss: 0.036371\n","100it [00:03, 29.90it/s]Train epoch: 134 [batch #100, batch_size 16, seq length 892]\tLoss: 0.034062\n","124it [00:03, 30.15it/s]Train epoch: 134 [batch #125, batch_size 16, seq length 978]\tLoss: 0.029421\n","150it [00:04, 28.24it/s]Train epoch: 134 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.032675\n","174it [00:05, 29.28it/s]Train epoch: 134 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.028323\n","199it [00:06, 27.15it/s]Train epoch: 134 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.029619\n","223it [00:07, 26.90it/s]Train epoch: 134 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.033334\n","250it [00:08, 27.00it/s]Train epoch: 134 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.033161\n","274it [00:09, 25.73it/s]Train epoch: 134 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.030972\n","298it [00:10, 25.08it/s]Train epoch: 134 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.034828\n","325it [00:11, 24.55it/s]Train epoch: 134 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.030443\n","349it [00:12, 23.33it/s]Train epoch: 134 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.035287\n","373it [00:13, 23.74it/s]Train epoch: 134 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.035754\n","400it [00:14, 24.08it/s]Train epoch: 134 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.035432\n","424it [00:15, 21.90it/s]Train epoch: 134 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.033348\n","448it [00:16, 21.05it/s]Train epoch: 134 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.034208\n","475it [00:18, 20.36it/s]Train epoch: 134 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.039029\n","500it [00:19, 18.19it/s]Train epoch: 134 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.039274\n","505it [00:19, 25.67it/s]\n","epoch loss: 0.0327734798829228\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 326.06it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3902, 0.5749, 0.5069, 0.5388, 0.8607\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4165, 0.6284, 0.5525, 0.5880, 0.8909\n","rec_at_5: 0.5492\n","prec_at_5: 0.5658\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 135\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 135 [batch #0, batch_size 16, seq length 212]\tLoss: 0.076867\n","24it [00:00, 34.80it/s]Train epoch: 135 [batch #25, batch_size 16, seq length 571]\tLoss: 0.027452\n","48it [00:01, 32.21it/s]Train epoch: 135 [batch #50, batch_size 16, seq length 709]\tLoss: 0.029502\n","72it [00:02, 32.59it/s]Train epoch: 135 [batch #75, batch_size 16, seq length 806]\tLoss: 0.034409\n","100it [00:03, 31.48it/s]Train epoch: 135 [batch #100, batch_size 16, seq length 892]\tLoss: 0.029729\n","124it [00:03, 29.33it/s]Train epoch: 135 [batch #125, batch_size 16, seq length 978]\tLoss: 0.028236\n","147it [00:04, 29.50it/s]Train epoch: 135 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.032471\n","172it [00:05, 28.65it/s]Train epoch: 135 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.024063\n","198it [00:06, 27.91it/s]Train epoch: 135 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.030392\n","225it [00:07, 27.81it/s]Train epoch: 135 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.031463\n","250it [00:08, 27.33it/s]Train epoch: 135 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.034236\n","274it [00:09, 25.15it/s]Train epoch: 135 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.031906\n","298it [00:10, 25.38it/s]Train epoch: 135 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.029529\n","325it [00:11, 25.40it/s]Train epoch: 135 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.029993\n","349it [00:12, 25.21it/s]Train epoch: 135 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.033680\n","373it [00:13, 23.73it/s]Train epoch: 135 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.034530\n","400it [00:14, 23.05it/s]Train epoch: 135 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.034820\n","424it [00:15, 22.00it/s]Train epoch: 135 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.031509\n","448it [00:16, 21.77it/s]Train epoch: 135 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.034348\n","474it [00:17, 19.24it/s]Train epoch: 135 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.040585\n","500it [00:19, 19.02it/s]Train epoch: 135 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.037308\n","505it [00:19, 25.88it/s]\n","epoch loss: 0.03173625757802499\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 324.13it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3868, 0.5741, 0.5000, 0.5345, 0.8608\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4145, 0.6297, 0.5482, 0.5861, 0.8909\n","rec_at_5: 0.5486\n","prec_at_5: 0.5649\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 136\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 136 [batch #0, batch_size 16, seq length 212]\tLoss: 0.051711\n","24it [00:00, 33.85it/s]Train epoch: 136 [batch #25, batch_size 16, seq length 571]\tLoss: 0.029467\n","48it [00:01, 34.61it/s]Train epoch: 136 [batch #50, batch_size 16, seq length 709]\tLoss: 0.025413\n","72it [00:02, 30.92it/s]Train epoch: 136 [batch #75, batch_size 16, seq length 806]\tLoss: 0.035870\n","100it [00:03, 32.02it/s]Train epoch: 136 [batch #100, batch_size 16, seq length 892]\tLoss: 0.030937\n","124it [00:03, 31.31it/s]Train epoch: 136 [batch #125, batch_size 16, seq length 978]\tLoss: 0.028093\n","148it [00:04, 30.26it/s]Train epoch: 136 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.031206\n","175it [00:05, 29.75it/s]Train epoch: 136 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.024079\n","200it [00:06, 28.65it/s]Train epoch: 136 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.025456\n","223it [00:07, 27.46it/s]Train epoch: 136 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.029870\n","250it [00:08, 27.36it/s]Train epoch: 136 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.031137\n","274it [00:09, 26.55it/s]Train epoch: 136 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.029532\n","298it [00:09, 26.47it/s]Train epoch: 136 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.029163\n","325it [00:11, 24.56it/s]Train epoch: 136 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.031744\n","349it [00:12, 24.62it/s]Train epoch: 136 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.030663\n","375it [00:13, 16.13it/s]Train epoch: 136 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.035525\n","399it [00:15, 13.30it/s]Train epoch: 136 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.033957\n","425it [00:17, 10.32it/s]Train epoch: 136 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.031341\n","448it [00:18, 19.90it/s]Train epoch: 136 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.032591\n","475it [00:20, 20.10it/s]Train epoch: 136 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.038601\n","500it [00:21, 17.98it/s]Train epoch: 136 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.036983\n","505it [00:21, 23.31it/s]\n","epoch loss: 0.03057908649731538\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 330.04it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3872, 0.5754, 0.5004, 0.5353, 0.8606\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4146, 0.6300, 0.5480, 0.5862, 0.8906\n","rec_at_5: 0.5491\n","prec_at_5: 0.5661\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 137\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 137 [batch #0, batch_size 16, seq length 212]\tLoss: 0.059212\n","24it [00:00, 33.58it/s]Train epoch: 137 [batch #25, batch_size 16, seq length 571]\tLoss: 0.025137\n","48it [00:01, 32.70it/s]Train epoch: 137 [batch #50, batch_size 16, seq length 709]\tLoss: 0.026354\n","72it [00:02, 30.92it/s]Train epoch: 137 [batch #75, batch_size 16, seq length 806]\tLoss: 0.033892\n","100it [00:03, 31.55it/s]Train epoch: 137 [batch #100, batch_size 16, seq length 892]\tLoss: 0.028770\n","124it [00:03, 31.26it/s]Train epoch: 137 [batch #125, batch_size 16, seq length 978]\tLoss: 0.031139\n","148it [00:04, 31.16it/s]Train epoch: 137 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.030544\n","172it [00:05, 29.36it/s]Train epoch: 137 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.023447\n","199it [00:06, 27.56it/s]Train epoch: 137 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.025542\n","225it [00:07, 28.34it/s]Train epoch: 137 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.029075\n","250it [00:08, 26.32it/s]Train epoch: 137 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.032666\n","274it [00:09, 27.25it/s]Train epoch: 137 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.029622\n","298it [00:10, 26.74it/s]Train epoch: 137 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.030704\n","325it [00:11, 27.10it/s]Train epoch: 137 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.027171\n","349it [00:12, 26.02it/s]Train epoch: 137 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.033809\n","373it [00:13, 23.79it/s]Train epoch: 137 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.034145\n","400it [00:14, 22.85it/s]Train epoch: 137 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.031571\n","424it [00:15, 22.13it/s]Train epoch: 137 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.029853\n","448it [00:16, 20.78it/s]Train epoch: 137 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.032919\n","475it [00:17, 20.36it/s]Train epoch: 137 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.035736\n","500it [00:19, 17.72it/s]Train epoch: 137 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.035383\n","505it [00:19, 25.98it/s]\n","epoch loss: 0.030147567458247933\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 320.92it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3862, 0.5739, 0.5002, 0.5345, 0.8602\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4137, 0.6263, 0.5493, 0.5853, 0.8903\n","rec_at_5: 0.5479\n","prec_at_5: 0.5648\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 138\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 138 [batch #0, batch_size 16, seq length 212]\tLoss: 0.057178\n","24it [00:00, 34.56it/s]Train epoch: 138 [batch #25, batch_size 16, seq length 571]\tLoss: 0.027083\n","48it [00:01, 32.03it/s]Train epoch: 138 [batch #50, batch_size 16, seq length 709]\tLoss: 0.022893\n","72it [00:02, 32.24it/s]Train epoch: 138 [batch #75, batch_size 16, seq length 806]\tLoss: 0.035819\n","100it [00:03, 30.10it/s]Train epoch: 138 [batch #100, batch_size 16, seq length 892]\tLoss: 0.026314\n","124it [00:03, 29.77it/s]Train epoch: 138 [batch #125, batch_size 16, seq length 978]\tLoss: 0.025902\n","147it [00:04, 29.27it/s]Train epoch: 138 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.029373\n","175it [00:05, 29.70it/s]Train epoch: 138 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.023933\n","198it [00:06, 28.77it/s]Train epoch: 138 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.024643\n","225it [00:07, 27.09it/s]Train epoch: 138 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.029334\n","250it [00:08, 26.62it/s]Train epoch: 138 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.032230\n","274it [00:09, 26.83it/s]Train epoch: 138 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.030216\n","298it [00:10, 25.79it/s]Train epoch: 138 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.030208\n","325it [00:11, 26.32it/s]Train epoch: 138 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.030224\n","349it [00:12, 24.89it/s]Train epoch: 138 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.029840\n","373it [00:13, 25.66it/s]Train epoch: 138 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.032597\n","400it [00:14, 23.96it/s]Train epoch: 138 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.031157\n","424it [00:15, 21.99it/s]Train epoch: 138 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.030519\n","448it [00:16, 21.76it/s]Train epoch: 138 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.031454\n","475it [00:17, 20.43it/s]Train epoch: 138 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.035194\n","500it [00:18, 18.43it/s]Train epoch: 138 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.035527\n","505it [00:19, 26.27it/s]\n","epoch loss: 0.029025688102367418\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:05, 313.52it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3856, 0.5743, 0.4974, 0.5331, 0.8602\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4128, 0.6287, 0.5459, 0.5844, 0.8902\n","rec_at_5: 0.5480\n","prec_at_5: 0.5644\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 139\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 139 [batch #0, batch_size 16, seq length 212]\tLoss: 0.055378\n","24it [00:00, 33.52it/s]Train epoch: 139 [batch #25, batch_size 16, seq length 571]\tLoss: 0.026738\n","48it [00:01, 32.44it/s]Train epoch: 139 [batch #50, batch_size 16, seq length 709]\tLoss: 0.024599\n","72it [00:02, 30.39it/s]Train epoch: 139 [batch #75, batch_size 16, seq length 806]\tLoss: 0.032916\n","100it [00:03, 30.38it/s]Train epoch: 139 [batch #100, batch_size 16, seq length 892]\tLoss: 0.027013\n","124it [00:03, 29.06it/s]Train epoch: 139 [batch #125, batch_size 16, seq length 978]\tLoss: 0.025537\n","148it [00:04, 29.83it/s]Train epoch: 139 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.027391\n","174it [00:05, 28.76it/s]Train epoch: 139 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.024954\n","198it [00:06, 28.25it/s]Train epoch: 139 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.023199\n","224it [00:07, 28.17it/s]Train epoch: 139 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.029221\n","249it [00:08, 28.76it/s]Train epoch: 139 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.031835\n","275it [00:09, 26.95it/s]Train epoch: 139 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.025788\n","299it [00:10, 26.73it/s]Train epoch: 139 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.028352\n","323it [00:11, 25.45it/s]Train epoch: 139 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.028191\n","350it [00:12, 25.63it/s]Train epoch: 139 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.030009\n","374it [00:13, 23.57it/s]Train epoch: 139 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.034597\n","398it [00:14, 24.00it/s]Train epoch: 139 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.027000\n","425it [00:15, 24.18it/s]Train epoch: 139 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.026570\n","449it [00:16, 21.59it/s]Train epoch: 139 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.031133\n","475it [00:17, 16.43it/s]Train epoch: 139 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.034461\n","499it [00:19, 11.77it/s]Train epoch: 139 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.032326\n","505it [00:20, 25.04it/s]\n","epoch loss: 0.02790467088908363\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 314.90it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3860, 0.5740, 0.4987, 0.5337, 0.8602\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4147, 0.6285, 0.5494, 0.5863, 0.8904\n","rec_at_5: 0.5496\n","prec_at_5: 0.5657\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 140\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 140 [batch #0, batch_size 16, seq length 212]\tLoss: 0.055308\n","24it [00:00, 36.97it/s]Train epoch: 140 [batch #25, batch_size 16, seq length 571]\tLoss: 0.030243\n","48it [00:01, 35.13it/s]Train epoch: 140 [batch #50, batch_size 16, seq length 709]\tLoss: 0.022171\n","72it [00:02, 32.29it/s]Train epoch: 140 [batch #75, batch_size 16, seq length 806]\tLoss: 0.032543\n","100it [00:02, 31.23it/s]Train epoch: 140 [batch #100, batch_size 16, seq length 892]\tLoss: 0.026214\n","124it [00:03, 31.40it/s]Train epoch: 140 [batch #125, batch_size 16, seq length 978]\tLoss: 0.026639\n","148it [00:04, 32.16it/s]Train epoch: 140 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.025436\n","175it [00:05, 29.61it/s]Train epoch: 140 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.024431\n","198it [00:06, 28.13it/s]Train epoch: 140 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.024330\n","222it [00:07, 28.23it/s]Train epoch: 140 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.028962\n","248it [00:07, 28.13it/s]Train epoch: 140 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.030392\n","274it [00:08, 28.67it/s]Train epoch: 140 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.026842\n","299it [00:09, 28.17it/s]Train epoch: 140 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.027081\n","323it [00:10, 24.58it/s]Train epoch: 140 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.027352\n","350it [00:11, 26.45it/s]Train epoch: 140 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.025581\n","374it [00:12, 26.29it/s]Train epoch: 140 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.032621\n","398it [00:13, 25.33it/s]Train epoch: 140 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.027413\n","425it [00:14, 24.05it/s]Train epoch: 140 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.028638\n","449it [00:15, 23.32it/s]Train epoch: 140 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.030911\n","473it [00:16, 21.64it/s]Train epoch: 140 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.034232\n","500it [00:18, 19.09it/s]Train epoch: 140 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.034387\n","505it [00:18, 27.21it/s]\n","epoch loss: 0.02768369601938025\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 337.79it/s]\n","Finish save rediction by checkpoint  140\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3837, 0.5713, 0.4968, 0.5315, 0.8597\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4127, 0.6279, 0.5463, 0.5843, 0.8899\n","rec_at_5: 0.5485\n","prec_at_5: 0.5645\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 141\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 141 [batch #0, batch_size 16, seq length 212]\tLoss: 0.048890\n","24it [00:00, 35.63it/s]Train epoch: 141 [batch #25, batch_size 16, seq length 571]\tLoss: 0.025170\n","48it [00:01, 35.46it/s]Train epoch: 141 [batch #50, batch_size 16, seq length 709]\tLoss: 0.025247\n","72it [00:02, 33.27it/s]Train epoch: 141 [batch #75, batch_size 16, seq length 806]\tLoss: 0.034429\n","100it [00:02, 32.77it/s]Train epoch: 141 [batch #100, batch_size 16, seq length 892]\tLoss: 0.028145\n","124it [00:03, 32.96it/s]Train epoch: 141 [batch #125, batch_size 16, seq length 978]\tLoss: 0.025975\n","148it [00:04, 31.03it/s]Train epoch: 141 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.028782\n","172it [00:05, 30.83it/s]Train epoch: 141 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.020727\n","200it [00:06, 30.25it/s]Train epoch: 141 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.020381\n","223it [00:06, 28.21it/s]Train epoch: 141 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.026671\n","250it [00:07, 28.19it/s]Train epoch: 141 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.026864\n","275it [00:08, 25.31it/s]Train epoch: 141 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.027061\n","300it [00:09, 27.35it/s]Train epoch: 141 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.025048\n","324it [00:10, 25.47it/s]Train epoch: 141 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.023995\n","348it [00:11, 25.55it/s]Train epoch: 141 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.029272\n","375it [00:12, 26.20it/s]Train epoch: 141 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.029803\n","399it [00:13, 24.94it/s]Train epoch: 141 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.030276\n","423it [00:14, 24.04it/s]Train epoch: 141 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.029163\n","450it [00:15, 23.13it/s]Train epoch: 141 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.028226\n","474it [00:16, 21.61it/s]Train epoch: 141 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.030429\n","500it [00:18, 18.77it/s]Train epoch: 141 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.031027\n","505it [00:18, 27.34it/s]\n","epoch loss: 0.02712838817341714\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.07it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3823, 0.5695, 0.4952, 0.5298, 0.8594\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4109, 0.6246, 0.5456, 0.5825, 0.8897\n","rec_at_5: 0.5488\n","prec_at_5: 0.5649\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 142\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 142 [batch #0, batch_size 16, seq length 212]\tLoss: 0.042292\n","24it [00:00, 35.61it/s]Train epoch: 142 [batch #25, batch_size 16, seq length 571]\tLoss: 0.026949\n","48it [00:01, 33.41it/s]Train epoch: 142 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021655\n","72it [00:02, 33.92it/s]Train epoch: 142 [batch #75, batch_size 16, seq length 806]\tLoss: 0.032377\n","100it [00:02, 31.93it/s]Train epoch: 142 [batch #100, batch_size 16, seq length 892]\tLoss: 0.027381\n","124it [00:03, 32.03it/s]Train epoch: 142 [batch #125, batch_size 16, seq length 978]\tLoss: 0.025184\n","148it [00:04, 30.82it/s]Train epoch: 142 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.027508\n","172it [00:05, 30.32it/s]Train epoch: 142 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.021531\n","200it [00:06, 29.70it/s]Train epoch: 142 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.023662\n","225it [00:07, 28.58it/s]Train epoch: 142 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.025654\n","249it [00:07, 28.32it/s]Train epoch: 142 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.024051\n","272it [00:08, 27.35it/s]Train epoch: 142 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.025763\n","300it [00:09, 27.18it/s]Train epoch: 142 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.027150\n","324it [00:10, 27.08it/s]Train epoch: 142 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.025560\n","348it [00:11, 26.09it/s]Train epoch: 142 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.026861\n","375it [00:12, 25.15it/s]Train epoch: 142 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.028545\n","399it [00:13, 24.92it/s]Train epoch: 142 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.028865\n","423it [00:14, 24.46it/s]Train epoch: 142 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.026270\n","450it [00:15, 22.43it/s]Train epoch: 142 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.027871\n","474it [00:16, 21.20it/s]Train epoch: 142 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.031749\n","499it [00:18, 18.06it/s]Train epoch: 142 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.030459\n","505it [00:18, 27.27it/s]\n","epoch loss: 0.026218160028145233\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.34it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3841, 0.5724, 0.4958, 0.5314, 0.8596\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4126, 0.6278, 0.5463, 0.5842, 0.8897\n","rec_at_5: 0.5477\n","prec_at_5: 0.5640\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 143\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 143 [batch #0, batch_size 16, seq length 212]\tLoss: 0.042412\n","24it [00:00, 35.91it/s]Train epoch: 143 [batch #25, batch_size 16, seq length 571]\tLoss: 0.025715\n","48it [00:01, 35.39it/s]Train epoch: 143 [batch #50, batch_size 16, seq length 709]\tLoss: 0.024039\n","72it [00:02, 33.29it/s]Train epoch: 143 [batch #75, batch_size 16, seq length 806]\tLoss: 0.029335\n","100it [00:02, 31.63it/s]Train epoch: 143 [batch #100, batch_size 16, seq length 892]\tLoss: 0.026423\n","124it [00:03, 32.82it/s]Train epoch: 143 [batch #125, batch_size 16, seq length 978]\tLoss: 0.024521\n","148it [00:04, 31.09it/s]Train epoch: 143 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.025122\n","172it [00:05, 31.38it/s]Train epoch: 143 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.024648\n","199it [00:06, 30.10it/s]Train epoch: 143 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.020915\n","225it [00:07, 28.40it/s]Train epoch: 143 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.028332\n","248it [00:07, 28.34it/s]Train epoch: 143 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.026565\n","274it [00:08, 27.66it/s]Train epoch: 143 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.024134\n","300it [00:09, 26.65it/s]Train epoch: 143 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.025389\n","324it [00:10, 27.23it/s]Train epoch: 143 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.025025\n","348it [00:11, 26.08it/s]Train epoch: 143 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.025993\n","375it [00:12, 25.85it/s]Train epoch: 143 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.025673\n","399it [00:13, 25.28it/s]Train epoch: 143 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.024859\n","423it [00:14, 24.48it/s]Train epoch: 143 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.025424\n","450it [00:15, 21.85it/s]Train epoch: 143 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.026484\n","474it [00:16, 20.93it/s]Train epoch: 143 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.030056\n","498it [00:17, 20.13it/s]Train epoch: 143 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.030626\n","505it [00:18, 27.55it/s]\n","epoch loss: 0.02544398857741663\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 340.04it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3851, 0.5698, 0.4990, 0.5320, 0.8593\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4122, 0.6239, 0.5485, 0.5838, 0.8895\n","rec_at_5: 0.5490\n","prec_at_5: 0.5654\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 144\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 144 [batch #0, batch_size 16, seq length 212]\tLoss: 0.051769\n","22it [00:00, 37.03it/s]Train epoch: 144 [batch #25, batch_size 16, seq length 571]\tLoss: 0.024783\n","50it [00:01, 34.50it/s]Train epoch: 144 [batch #50, batch_size 16, seq length 709]\tLoss: 0.020164\n","74it [00:02, 33.92it/s]Train epoch: 144 [batch #75, batch_size 16, seq length 806]\tLoss: 0.031062\n","98it [00:02, 32.65it/s]Train epoch: 144 [batch #100, batch_size 16, seq length 892]\tLoss: 0.024611\n","122it [00:03, 31.17it/s]Train epoch: 144 [batch #125, batch_size 16, seq length 978]\tLoss: 0.024839\n","150it [00:04, 30.48it/s]Train epoch: 144 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.026020\n","173it [00:05, 30.72it/s]Train epoch: 144 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.020874\n","197it [00:06, 29.69it/s]Train epoch: 144 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019333\n","223it [00:06, 28.78it/s]Train epoch: 144 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.025421\n","249it [00:07, 27.40it/s]Train epoch: 144 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.023630\n","273it [00:08, 27.64it/s]Train epoch: 144 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.025719\n","300it [00:09, 27.56it/s]Train epoch: 144 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.024374\n","324it [00:10, 26.80it/s]Train epoch: 144 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.026516\n","348it [00:11, 25.76it/s]Train epoch: 144 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.026904\n","375it [00:12, 25.41it/s]Train epoch: 144 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.029694\n","399it [00:13, 25.50it/s]Train epoch: 144 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.028229\n","423it [00:14, 23.78it/s]Train epoch: 144 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.024766\n","450it [00:15, 23.64it/s]Train epoch: 144 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.024887\n","474it [00:16, 22.04it/s]Train epoch: 144 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.028610\n","498it [00:17, 19.88it/s]Train epoch: 144 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.027581\n","505it [00:18, 27.52it/s]\n","epoch loss: 0.025090949682030125\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:05, 295.46it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3815, 0.5667, 0.4959, 0.5290, 0.8592\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4094, 0.6224, 0.5447, 0.5809, 0.8894\n","rec_at_5: 0.5476\n","prec_at_5: 0.5639\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 145\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 145 [batch #0, batch_size 16, seq length 212]\tLoss: 0.058072\n","24it [00:01, 19.57it/s]Train epoch: 145 [batch #25, batch_size 16, seq length 571]\tLoss: 0.022786\n","49it [00:02, 16.44it/s]Train epoch: 145 [batch #50, batch_size 16, seq length 709]\tLoss: 0.022499\n","72it [00:03, 28.56it/s]Train epoch: 145 [batch #75, batch_size 16, seq length 806]\tLoss: 0.029854\n","100it [00:04, 32.80it/s]Train epoch: 145 [batch #100, batch_size 16, seq length 892]\tLoss: 0.022588\n","124it [00:05, 32.95it/s]Train epoch: 145 [batch #125, batch_size 16, seq length 978]\tLoss: 0.023500\n","148it [00:05, 31.96it/s]Train epoch: 145 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.023605\n","172it [00:06, 31.43it/s]Train epoch: 145 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.023507\n","200it [00:07, 30.23it/s]Train epoch: 145 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.021622\n","225it [00:08, 28.66it/s]Train epoch: 145 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.023189\n","247it [00:09, 29.54it/s]Train epoch: 145 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.024169\n","272it [00:10, 28.07it/s]Train epoch: 145 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.021826\n","298it [00:10, 27.82it/s]Train epoch: 145 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.023437\n","323it [00:11, 27.46it/s]Train epoch: 145 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.023226\n","350it [00:12, 24.55it/s]Train epoch: 145 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022791\n","374it [00:13, 26.01it/s]Train epoch: 145 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.026545\n","398it [00:14, 24.92it/s]Train epoch: 145 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.024778\n","425it [00:15, 24.99it/s]Train epoch: 145 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.025298\n","449it [00:16, 22.56it/s]Train epoch: 145 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.027277\n","473it [00:18, 22.09it/s]Train epoch: 145 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.029228\n","500it [00:19, 19.06it/s]Train epoch: 145 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026488\n","505it [00:19, 25.73it/s]\n","epoch loss: 0.023955216719353994\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.41it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3838, 0.5673, 0.4997, 0.5314, 0.8592\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4119, 0.6219, 0.5496, 0.5835, 0.8894\n","rec_at_5: 0.5458\n","prec_at_5: 0.5624\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 146\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 146 [batch #0, batch_size 16, seq length 212]\tLoss: 0.051285\n","25it [00:00, 35.91it/s]Train epoch: 146 [batch #25, batch_size 16, seq length 571]\tLoss: 0.022644\n","49it [00:01, 36.40it/s]Train epoch: 146 [batch #50, batch_size 16, seq length 709]\tLoss: 0.021444\n","73it [00:02, 33.50it/s]Train epoch: 146 [batch #75, batch_size 16, seq length 806]\tLoss: 0.028523\n","97it [00:02, 32.68it/s]Train epoch: 146 [batch #100, batch_size 16, seq length 892]\tLoss: 0.023922\n","125it [00:03, 31.96it/s]Train epoch: 146 [batch #125, batch_size 16, seq length 978]\tLoss: 0.025963\n","149it [00:04, 30.92it/s]Train epoch: 146 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.021671\n","173it [00:05, 30.74it/s]Train epoch: 146 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.019094\n","197it [00:06, 30.35it/s]Train epoch: 146 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.020373\n","225it [00:07, 29.04it/s]Train epoch: 146 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.024453\n","250it [00:07, 28.79it/s]Train epoch: 146 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.026648\n","274it [00:08, 28.46it/s]Train epoch: 146 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.025976\n","299it [00:09, 25.80it/s]Train epoch: 146 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.021905\n","323it [00:10, 27.41it/s]Train epoch: 146 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.021843\n","350it [00:11, 26.14it/s]Train epoch: 146 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.024791\n","374it [00:12, 25.81it/s]Train epoch: 146 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.027819\n","398it [00:13, 24.71it/s]Train epoch: 146 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.023538\n","425it [00:14, 24.67it/s]Train epoch: 146 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.022524\n","449it [00:15, 22.43it/s]Train epoch: 146 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.023228\n","473it [00:16, 22.65it/s]Train epoch: 146 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.027195\n","500it [00:18, 19.78it/s]Train epoch: 146 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026224\n","505it [00:18, 27.51it/s]\n","epoch loss: 0.02363925777112899\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3832, 0.5698, 0.4972, 0.5310, 0.8590\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4112, 0.6232, 0.5472, 0.5828, 0.8891\n","rec_at_5: 0.5457\n","prec_at_5: 0.5624\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 147\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 147 [batch #0, batch_size 16, seq length 212]\tLoss: 0.037142\n","25it [00:00, 34.86it/s]Train epoch: 147 [batch #25, batch_size 16, seq length 571]\tLoss: 0.023117\n","49it [00:01, 33.48it/s]Train epoch: 147 [batch #50, batch_size 16, seq length 709]\tLoss: 0.019080\n","73it [00:02, 32.27it/s]Train epoch: 147 [batch #75, batch_size 16, seq length 806]\tLoss: 0.028149\n","97it [00:02, 31.72it/s]Train epoch: 147 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021532\n","125it [00:03, 31.90it/s]Train epoch: 147 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020988\n","149it [00:04, 30.98it/s]Train epoch: 147 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.022606\n","173it [00:05, 29.64it/s]Train epoch: 147 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.019649\n","197it [00:06, 30.38it/s]Train epoch: 147 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019099\n","224it [00:07, 28.56it/s]Train epoch: 147 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.021514\n","249it [00:08, 28.57it/s]Train epoch: 147 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.027067\n","275it [00:08, 28.51it/s]Train epoch: 147 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.020650\n","299it [00:09, 28.31it/s]Train epoch: 147 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.020404\n","323it [00:10, 26.75it/s]Train epoch: 147 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.019298\n","350it [00:11, 25.87it/s]Train epoch: 147 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.024551\n","374it [00:12, 24.72it/s]Train epoch: 147 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.024262\n","398it [00:13, 25.17it/s]Train epoch: 147 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.021819\n","425it [00:14, 23.66it/s]Train epoch: 147 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.022486\n","449it [00:15, 23.24it/s]Train epoch: 147 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.023315\n","473it [00:16, 20.67it/s]Train epoch: 147 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.027355\n","499it [00:18, 19.55it/s]Train epoch: 147 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.028114\n","505it [00:18, 27.22it/s]\n","epoch loss: 0.022535109360665453\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 339.37it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3825, 0.5666, 0.4969, 0.5294, 0.8588\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4112, 0.6226, 0.5478, 0.5828, 0.8891\n","rec_at_5: 0.5478\n","prec_at_5: 0.5640\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 148\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 148 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035039\n","25it [00:00, 35.72it/s]Train epoch: 148 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021912\n","49it [00:01, 33.79it/s]Train epoch: 148 [batch #50, batch_size 16, seq length 709]\tLoss: 0.020219\n","73it [00:02, 34.36it/s]Train epoch: 148 [batch #75, batch_size 16, seq length 806]\tLoss: 0.027784\n","97it [00:02, 31.86it/s]Train epoch: 148 [batch #100, batch_size 16, seq length 892]\tLoss: 0.022544\n","125it [00:03, 31.68it/s]Train epoch: 148 [batch #125, batch_size 16, seq length 978]\tLoss: 0.019311\n","149it [00:04, 30.66it/s]Train epoch: 148 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.022382\n","173it [00:05, 29.98it/s]Train epoch: 148 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.020640\n","199it [00:06, 29.96it/s]Train epoch: 148 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019714\n","223it [00:07, 26.18it/s]Train epoch: 148 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.023289\n","247it [00:07, 27.82it/s]Train epoch: 148 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.023130\n","275it [00:08, 27.01it/s]Train epoch: 148 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.020710\n","298it [00:09, 27.03it/s]Train epoch: 148 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.024740\n","323it [00:10, 27.66it/s]Train epoch: 148 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.019508\n","350it [00:11, 25.46it/s]Train epoch: 148 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.027162\n","374it [00:12, 25.70it/s]Train epoch: 148 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.024751\n","398it [00:13, 24.51it/s]Train epoch: 148 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.023841\n","425it [00:14, 24.54it/s]Train epoch: 148 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.022248\n","449it [00:15, 22.16it/s]Train epoch: 148 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.021582\n","473it [00:16, 22.27it/s]Train epoch: 148 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.026074\n","500it [00:18, 19.80it/s]Train epoch: 148 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.026790\n","505it [00:18, 27.31it/s]\n","epoch loss: 0.022450416000665968\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 335.81it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3797, 0.5656, 0.4927, 0.5266, 0.8585\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4088, 0.6211, 0.5447, 0.5803, 0.8888\n","rec_at_5: 0.5478\n","prec_at_5: 0.5633\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 149\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 149 [batch #0, batch_size 16, seq length 212]\tLoss: 0.052525\n","24it [00:00, 35.77it/s]Train epoch: 149 [batch #25, batch_size 16, seq length 571]\tLoss: 0.020182\n","48it [00:01, 34.00it/s]Train epoch: 149 [batch #50, batch_size 16, seq length 709]\tLoss: 0.017627\n","72it [00:02, 32.38it/s]Train epoch: 149 [batch #75, batch_size 16, seq length 806]\tLoss: 0.025121\n","100it [00:02, 32.98it/s]Train epoch: 149 [batch #100, batch_size 16, seq length 892]\tLoss: 0.023074\n","124it [00:03, 31.97it/s]Train epoch: 149 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020779\n","148it [00:04, 31.68it/s]Train epoch: 149 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.022980\n","172it [00:05, 30.82it/s]Train epoch: 149 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.016368\n","200it [00:06, 29.23it/s]Train epoch: 149 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.017679\n","223it [00:06, 29.85it/s]Train epoch: 149 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.024393\n","249it [00:07, 28.45it/s]Train epoch: 149 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.023745\n","273it [00:08, 28.13it/s]Train epoch: 149 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.019955\n","299it [00:09, 27.61it/s]Train epoch: 149 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.019514\n","325it [00:10, 28.03it/s]Train epoch: 149 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.022813\n","349it [00:11, 26.88it/s]Train epoch: 149 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.020760\n","373it [00:12, 25.35it/s]Train epoch: 149 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.022678\n","400it [00:13, 24.45it/s]Train epoch: 149 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.022061\n","424it [00:14, 23.46it/s]Train epoch: 149 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.020697\n","448it [00:15, 23.07it/s]Train epoch: 149 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.022273\n","475it [00:16, 21.33it/s]Train epoch: 149 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.024301\n","500it [00:18, 17.77it/s]Train epoch: 149 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.022216\n","505it [00:18, 27.66it/s]\n","epoch loss: 0.021786528926614746\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 337.21it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3797, 0.5637, 0.4939, 0.5265, 0.8584\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4091, 0.6196, 0.5463, 0.5806, 0.8889\n","rec_at_5: 0.5493\n","prec_at_5: 0.5650\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 150\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 150 [batch #0, batch_size 16, seq length 212]\tLoss: 0.041091\n","22it [00:00, 37.97it/s]Train epoch: 150 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021282\n","50it [00:01, 34.89it/s]Train epoch: 150 [batch #50, batch_size 16, seq length 709]\tLoss: 0.018501\n","74it [00:02, 33.91it/s]Train epoch: 150 [batch #75, batch_size 16, seq length 806]\tLoss: 0.023037\n","98it [00:02, 32.29it/s]Train epoch: 150 [batch #100, batch_size 16, seq length 892]\tLoss: 0.019731\n","122it [00:03, 33.58it/s]Train epoch: 150 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020457\n","150it [00:04, 31.38it/s]Train epoch: 150 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.024336\n","174it [00:05, 30.96it/s]Train epoch: 150 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017986\n","198it [00:06, 29.36it/s]Train epoch: 150 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.019236\n","222it [00:06, 29.92it/s]Train epoch: 150 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.020907\n","250it [00:07, 28.36it/s]Train epoch: 150 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.023211\n","275it [00:08, 27.49it/s]Train epoch: 150 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.022070\n","300it [00:09, 27.18it/s]Train epoch: 150 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.019904\n","324it [00:10, 27.08it/s]Train epoch: 150 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.019508\n","348it [00:11, 27.41it/s]Train epoch: 150 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.022938\n","375it [00:12, 25.93it/s]Train epoch: 150 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.023894\n","399it [00:13, 24.38it/s]Train epoch: 150 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.021966\n","423it [00:14, 23.42it/s]Train epoch: 150 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.021411\n","450it [00:15, 23.98it/s]Train epoch: 150 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.020446\n","474it [00:16, 22.00it/s]Train epoch: 150 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.023912\n","498it [00:17, 20.69it/s]Train epoch: 150 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.025819\n","505it [00:18, 27.66it/s]\n","epoch loss: 0.021128035190291612\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:05, 283.09it/s]\n","Finish save rediction by checkpoint  150\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3826, 0.5650, 0.4990, 0.5300, 0.8582\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4092, 0.6179, 0.5478, 0.5807, 0.8887\n","rec_at_5: 0.5453\n","prec_at_5: 0.5624\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 151\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 151 [batch #0, batch_size 16, seq length 212]\tLoss: 0.059417\n","25it [00:00, 35.85it/s]Train epoch: 151 [batch #25, batch_size 16, seq length 571]\tLoss: 0.021093\n","49it [00:01, 34.45it/s]Train epoch: 151 [batch #50, batch_size 16, seq length 709]\tLoss: 0.018133\n","73it [00:02, 33.00it/s]Train epoch: 151 [batch #75, batch_size 16, seq length 806]\tLoss: 0.024122\n","97it [00:02, 32.71it/s]Train epoch: 151 [batch #100, batch_size 16, seq length 892]\tLoss: 0.019677\n","125it [00:03, 31.35it/s]Train epoch: 151 [batch #125, batch_size 16, seq length 978]\tLoss: 0.020709\n","149it [00:04, 29.35it/s]Train epoch: 151 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017902\n","173it [00:05, 30.50it/s]Train epoch: 151 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015597\n","199it [00:06, 29.46it/s]Train epoch: 151 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.016357\n","223it [00:07, 28.91it/s]Train epoch: 151 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.020678\n","247it [00:07, 28.61it/s]Train epoch: 151 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.020150\n","275it [00:08, 28.91it/s]Train epoch: 151 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.020408\n","299it [00:09, 26.81it/s]Train epoch: 151 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.021276\n","323it [00:10, 26.92it/s]Train epoch: 151 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.020278\n","350it [00:11, 26.75it/s]Train epoch: 151 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.021461\n","374it [00:12, 25.02it/s]Train epoch: 151 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.020122\n","398it [00:13, 25.92it/s]Train epoch: 151 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.020781\n","425it [00:14, 24.32it/s]Train epoch: 151 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.022259\n","449it [00:15, 23.50it/s]Train epoch: 151 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.021562\n","473it [00:16, 21.85it/s]Train epoch: 151 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.022482\n","500it [00:18, 20.01it/s]Train epoch: 151 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.023910\n","505it [00:18, 27.53it/s]\n","epoch loss: 0.020525161672261368\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 335.83it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3812, 0.5640, 0.4975, 0.5287, 0.8581\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4091, 0.6182, 0.5473, 0.5806, 0.8884\n","rec_at_5: 0.5447\n","prec_at_5: 0.5615\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 152\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 152 [batch #0, batch_size 16, seq length 212]\tLoss: 0.031979\n","24it [00:00, 35.50it/s]Train epoch: 152 [batch #25, batch_size 16, seq length 571]\tLoss: 0.020213\n","48it [00:01, 33.86it/s]Train epoch: 152 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016120\n","72it [00:02, 32.24it/s]Train epoch: 152 [batch #75, batch_size 16, seq length 806]\tLoss: 0.021597\n","100it [00:02, 32.53it/s]Train epoch: 152 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021121\n","124it [00:03, 31.41it/s]Train epoch: 152 [batch #125, batch_size 16, seq length 978]\tLoss: 0.019146\n","148it [00:04, 31.41it/s]Train epoch: 152 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.020748\n","172it [00:05, 30.19it/s]Train epoch: 152 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017051\n","199it [00:06, 28.83it/s]Train epoch: 152 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015159\n","223it [00:06, 29.81it/s]Train epoch: 152 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.021295\n","248it [00:07, 28.78it/s]Train epoch: 152 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.020894\n","275it [00:08, 28.46it/s]Train epoch: 152 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.020105\n","298it [00:09, 27.76it/s]Train epoch: 152 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.016952\n","322it [00:10, 26.74it/s]Train epoch: 152 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.018492\n","350it [00:11, 24.55it/s]Train epoch: 152 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.020291\n","374it [00:12, 25.04it/s]Train epoch: 152 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.020833\n","398it [00:13, 24.24it/s]Train epoch: 152 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.020150\n","425it [00:14, 24.59it/s]Train epoch: 152 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.018420\n","449it [00:15, 23.14it/s]Train epoch: 152 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.021102\n","473it [00:16, 22.19it/s]Train epoch: 152 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.022738\n","500it [00:18, 19.16it/s]Train epoch: 152 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.023599\n","505it [00:18, 27.49it/s]\n","epoch loss: 0.019730464950443644\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 335.16it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3812, 0.5658, 0.4960, 0.5286, 0.8578\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4088, 0.6198, 0.5456, 0.5803, 0.8882\n","rec_at_5: 0.5443\n","prec_at_5: 0.5603\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 153\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 153 [batch #0, batch_size 16, seq length 212]\tLoss: 0.039457\n","25it [00:00, 36.45it/s]Train epoch: 153 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017623\n","49it [00:01, 33.93it/s]Train epoch: 153 [batch #50, batch_size 16, seq length 709]\tLoss: 0.015813\n","73it [00:02, 31.77it/s]Train epoch: 153 [batch #75, batch_size 16, seq length 806]\tLoss: 0.025106\n","97it [00:02, 33.14it/s]Train epoch: 153 [batch #100, batch_size 16, seq length 892]\tLoss: 0.021574\n","125it [00:03, 32.01it/s]Train epoch: 153 [batch #125, batch_size 16, seq length 978]\tLoss: 0.019622\n","149it [00:04, 30.80it/s]Train epoch: 153 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.020619\n","173it [00:05, 30.44it/s]Train epoch: 153 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015745\n","197it [00:06, 29.54it/s]Train epoch: 153 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015666\n","222it [00:06, 28.49it/s]Train epoch: 153 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.020321\n","249it [00:07, 27.99it/s]Train epoch: 153 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.022154\n","272it [00:08, 27.34it/s]Train epoch: 153 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.018628\n","298it [00:09, 27.36it/s]Train epoch: 153 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.020578\n","325it [00:10, 27.79it/s]Train epoch: 153 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.016387\n","349it [00:11, 26.39it/s]Train epoch: 153 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.021350\n","373it [00:12, 24.62it/s]Train epoch: 153 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.019336\n","399it [00:13, 16.56it/s]Train epoch: 153 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.019864\n","425it [00:15, 12.86it/s]Train epoch: 153 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.019742\n","449it [00:17, 15.03it/s]Train epoch: 153 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.020577\n","475it [00:19, 18.47it/s]Train epoch: 153 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.023023\n","500it [00:20, 19.48it/s]Train epoch: 153 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.022283\n","505it [00:20, 24.37it/s]\n","epoch loss: 0.019325725488263265\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.96it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3792, 0.5652, 0.4932, 0.5268, 0.8577\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4071, 0.6216, 0.5412, 0.5786, 0.8879\n","rec_at_5: 0.5452\n","prec_at_5: 0.5621\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 154\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 154 [batch #0, batch_size 16, seq length 212]\tLoss: 0.039241\n","24it [00:00, 36.88it/s]Train epoch: 154 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017176\n","48it [00:01, 34.66it/s]Train epoch: 154 [batch #50, batch_size 16, seq length 709]\tLoss: 0.016316\n","72it [00:02, 34.01it/s]Train epoch: 154 [batch #75, batch_size 16, seq length 806]\tLoss: 0.023344\n","100it [00:02, 32.45it/s]Train epoch: 154 [batch #100, batch_size 16, seq length 892]\tLoss: 0.020794\n","124it [00:03, 31.49it/s]Train epoch: 154 [batch #125, batch_size 16, seq length 978]\tLoss: 0.018162\n","148it [00:04, 31.32it/s]Train epoch: 154 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.019618\n","172it [00:05, 31.32it/s]Train epoch: 154 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015482\n","198it [00:06, 28.76it/s]Train epoch: 154 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.013860\n","225it [00:07, 28.46it/s]Train epoch: 154 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.020117\n","247it [00:07, 29.24it/s]Train epoch: 154 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.021545\n","275it [00:08, 28.79it/s]Train epoch: 154 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016450\n","298it [00:09, 27.50it/s]Train epoch: 154 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.017240\n","325it [00:10, 27.14it/s]Train epoch: 154 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.017233\n","349it [00:11, 25.70it/s]Train epoch: 154 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.017506\n","373it [00:12, 25.91it/s]Train epoch: 154 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.020112\n","400it [00:13, 23.52it/s]Train epoch: 154 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.019467\n","424it [00:14, 24.53it/s]Train epoch: 154 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.016586\n","448it [00:15, 22.50it/s]Train epoch: 154 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.020292\n","475it [00:16, 20.79it/s]Train epoch: 154 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.022339\n","499it [00:18, 19.46it/s]Train epoch: 154 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.022005\n","505it [00:18, 27.54it/s]\n","epoch loss: 0.018811710922757515\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 337.10it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3806, 0.5655, 0.4959, 0.5284, 0.8580\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4087, 0.6206, 0.5448, 0.5802, 0.8885\n","rec_at_5: 0.5466\n","prec_at_5: 0.5626\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 155\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 155 [batch #0, batch_size 16, seq length 212]\tLoss: 0.042486\n","24it [00:00, 36.12it/s]Train epoch: 155 [batch #25, batch_size 16, seq length 571]\tLoss: 0.018798\n","48it [00:01, 34.02it/s]Train epoch: 155 [batch #50, batch_size 16, seq length 709]\tLoss: 0.017602\n","72it [00:02, 33.24it/s]Train epoch: 155 [batch #75, batch_size 16, seq length 806]\tLoss: 0.023770\n","100it [00:02, 32.61it/s]Train epoch: 155 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017980\n","124it [00:03, 32.03it/s]Train epoch: 155 [batch #125, batch_size 16, seq length 978]\tLoss: 0.021686\n","148it [00:04, 31.09it/s]Train epoch: 155 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.019145\n","174it [00:05, 29.65it/s]Train epoch: 155 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.017342\n","198it [00:06, 28.35it/s]Train epoch: 155 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015872\n","222it [00:07, 29.11it/s]Train epoch: 155 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.019393\n","248it [00:07, 29.31it/s]Train epoch: 155 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.018728\n","274it [00:08, 27.89it/s]Train epoch: 155 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.017910\n","299it [00:09, 27.54it/s]Train epoch: 155 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.017701\n","324it [00:10, 27.65it/s]Train epoch: 155 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.016329\n","348it [00:11, 26.89it/s]Train epoch: 155 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.017541\n","375it [00:12, 25.62it/s]Train epoch: 155 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.018579\n","399it [00:13, 25.22it/s]Train epoch: 155 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.018553\n","423it [00:14, 23.26it/s]Train epoch: 155 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.019503\n","450it [00:15, 22.85it/s]Train epoch: 155 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.019483\n","474it [00:16, 21.74it/s]Train epoch: 155 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.019243\n","498it [00:18, 20.29it/s]Train epoch: 155 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.021309\n","505it [00:18, 27.46it/s]\n","epoch loss: 0.018662301038442602\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 329.20it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3812, 0.5643, 0.4982, 0.5292, 0.8581\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4084, 0.6172, 0.5470, 0.5800, 0.8887\n","rec_at_5: 0.5459\n","prec_at_5: 0.5617\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 156\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 156 [batch #0, batch_size 16, seq length 212]\tLoss: 0.053201\n","24it [00:00, 35.45it/s]Train epoch: 156 [batch #25, batch_size 16, seq length 571]\tLoss: 0.019800\n","48it [00:01, 33.46it/s]Train epoch: 156 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014804\n","72it [00:02, 31.38it/s]Train epoch: 156 [batch #75, batch_size 16, seq length 806]\tLoss: 0.020559\n","100it [00:03, 29.04it/s]Train epoch: 156 [batch #100, batch_size 16, seq length 892]\tLoss: 0.018236\n","124it [00:03, 30.86it/s]Train epoch: 156 [batch #125, batch_size 16, seq length 978]\tLoss: 0.018254\n","148it [00:04, 29.14it/s]Train epoch: 156 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.018766\n","175it [00:05, 30.46it/s]Train epoch: 156 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015137\n","199it [00:06, 28.71it/s]Train epoch: 156 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014147\n","225it [00:07, 28.50it/s]Train epoch: 156 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.021390\n","250it [00:08, 27.54it/s]Train epoch: 156 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.020124\n","274it [00:09, 28.88it/s]Train epoch: 156 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016308\n","300it [00:10, 26.81it/s]Train epoch: 156 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.018046\n","324it [00:10, 27.64it/s]Train epoch: 156 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.020074\n","348it [00:11, 26.47it/s]Train epoch: 156 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016822\n","375it [00:12, 26.51it/s]Train epoch: 156 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.018682\n","399it [00:13, 24.77it/s]Train epoch: 156 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.019719\n","423it [00:14, 22.88it/s]Train epoch: 156 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.017585\n","450it [00:16, 22.72it/s]Train epoch: 156 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.020553\n","474it [00:17, 21.80it/s]Train epoch: 156 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.020339\n","500it [00:18, 19.21it/s]Train epoch: 156 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.022289\n","505it [00:18, 26.90it/s]\n","epoch loss: 0.018209531989713926\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.40it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3822, 0.5655, 0.4988, 0.5301, 0.8582\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4088, 0.6201, 0.5453, 0.5803, 0.8885\n","rec_at_5: 0.5463\n","prec_at_5: 0.5616\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 157\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 157 [batch #0, batch_size 16, seq length 212]\tLoss: 0.029602\n","25it [00:00, 34.27it/s]Train epoch: 157 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017789\n","49it [00:01, 33.89it/s]Train epoch: 157 [batch #50, batch_size 16, seq length 709]\tLoss: 0.019776\n","73it [00:02, 33.96it/s]Train epoch: 157 [batch #75, batch_size 16, seq length 806]\tLoss: 0.019717\n","97it [00:02, 31.67it/s]Train epoch: 157 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017069\n","125it [00:03, 30.95it/s]Train epoch: 157 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017728\n","149it [00:04, 31.35it/s]Train epoch: 157 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017430\n","173it [00:05, 30.69it/s]Train epoch: 157 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.013525\n","200it [00:06, 30.37it/s]Train epoch: 157 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014866\n","224it [00:07, 29.07it/s]Train epoch: 157 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.017828\n","247it [00:07, 27.67it/s]Train epoch: 157 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.017581\n","275it [00:08, 26.26it/s]Train epoch: 157 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016712\n","298it [00:09, 27.37it/s]Train epoch: 157 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.014704\n","325it [00:10, 26.56it/s]Train epoch: 157 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.017771\n","349it [00:11, 25.36it/s]Train epoch: 157 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016904\n","373it [00:12, 25.52it/s]Train epoch: 157 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.018319\n","400it [00:13, 24.47it/s]Train epoch: 157 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.017950\n","424it [00:14, 23.88it/s]Train epoch: 157 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.017369\n","448it [00:15, 23.33it/s]Train epoch: 157 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.017736\n","475it [00:17, 20.52it/s]Train epoch: 157 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.019628\n","500it [00:18, 19.30it/s]Train epoch: 157 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.021425\n","505it [00:18, 27.15it/s]\n","epoch loss: 0.01768751729367356\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.44it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3808, 0.5660, 0.4952, 0.5282, 0.8582\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4079, 0.6200, 0.5439, 0.5795, 0.8884\n","rec_at_5: 0.5465\n","prec_at_5: 0.5625\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 158\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 158 [batch #0, batch_size 16, seq length 212]\tLoss: 0.037581\n","25it [00:00, 35.69it/s]Train epoch: 158 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017413\n","49it [00:01, 33.85it/s]Train epoch: 158 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014934\n","73it [00:02, 32.59it/s]Train epoch: 158 [batch #75, batch_size 16, seq length 806]\tLoss: 0.021980\n","97it [00:02, 33.09it/s]Train epoch: 158 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017494\n","125it [00:03, 31.74it/s]Train epoch: 158 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016264\n","149it [00:04, 30.94it/s]Train epoch: 158 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.018104\n","173it [00:05, 30.29it/s]Train epoch: 158 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014187\n","197it [00:06, 29.93it/s]Train epoch: 158 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.015030\n","225it [00:07, 28.67it/s]Train epoch: 158 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.018311\n","247it [00:07, 27.60it/s]Train epoch: 158 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.017696\n","274it [00:08, 28.06it/s]Train epoch: 158 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016918\n","299it [00:09, 25.68it/s]Train epoch: 158 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.016214\n","325it [00:10, 27.31it/s]Train epoch: 158 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015084\n","349it [00:11, 26.03it/s]Train epoch: 158 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.018013\n","373it [00:12, 25.76it/s]Train epoch: 158 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.019298\n","400it [00:13, 24.57it/s]Train epoch: 158 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.018205\n","424it [00:14, 22.54it/s]Train epoch: 158 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.018195\n","448it [00:15, 22.62it/s]Train epoch: 158 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015430\n","475it [00:16, 20.91it/s]Train epoch: 158 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.020849\n","500it [00:18, 19.38it/s]Train epoch: 158 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.018404\n","505it [00:18, 27.26it/s]\n","epoch loss: 0.016823981682963597\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.97it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3799, 0.5623, 0.4954, 0.5267, 0.8578\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4075, 0.6177, 0.5449, 0.5790, 0.8882\n","rec_at_5: 0.5477\n","prec_at_5: 0.5641\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 159\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 159 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035183\n","24it [00:00, 35.34it/s]Train epoch: 159 [batch #25, batch_size 16, seq length 571]\tLoss: 0.017687\n","48it [00:01, 33.79it/s]Train epoch: 159 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013652\n","72it [00:02, 34.93it/s]Train epoch: 159 [batch #75, batch_size 16, seq length 806]\tLoss: 0.019256\n","100it [00:02, 33.34it/s]Train epoch: 159 [batch #100, batch_size 16, seq length 892]\tLoss: 0.018855\n","124it [00:03, 31.48it/s]Train epoch: 159 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016358\n","148it [00:04, 31.60it/s]Train epoch: 159 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.019312\n","172it [00:05, 30.81it/s]Train epoch: 159 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.015138\n","198it [00:06, 29.28it/s]Train epoch: 159 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.012784\n","224it [00:07, 28.32it/s]Train epoch: 159 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.017484\n","248it [00:07, 27.76it/s]Train epoch: 159 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.018464\n","275it [00:08, 28.14it/s]Train epoch: 159 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016662\n","299it [00:09, 26.21it/s]Train epoch: 159 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.016073\n","323it [00:10, 27.50it/s]Train epoch: 159 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015919\n","350it [00:11, 26.44it/s]Train epoch: 159 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016946\n","374it [00:12, 25.50it/s]Train epoch: 159 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.018033\n","398it [00:13, 24.20it/s]Train epoch: 159 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.016568\n","425it [00:14, 23.20it/s]Train epoch: 159 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.014863\n","449it [00:15, 22.35it/s]Train epoch: 159 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014735\n","473it [00:16, 21.62it/s]Train epoch: 159 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.016872\n","500it [00:18, 19.67it/s]Train epoch: 159 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.018823\n","505it [00:18, 27.25it/s]\n","epoch loss: 0.01640187319169872\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.39it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3815, 0.5635, 0.4985, 0.5290, 0.8576\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4091, 0.6176, 0.5478, 0.5806, 0.8879\n","rec_at_5: 0.5464\n","prec_at_5: 0.5622\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 160\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 160 [batch #0, batch_size 16, seq length 212]\tLoss: 0.033213\n","24it [00:00, 33.83it/s]Train epoch: 160 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015907\n","48it [00:01, 33.25it/s]Train epoch: 160 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011046\n","72it [00:02, 32.96it/s]Train epoch: 160 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018254\n","100it [00:02, 33.99it/s]Train epoch: 160 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016049\n","124it [00:03, 32.31it/s]Train epoch: 160 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017628\n","148it [00:04, 31.30it/s]Train epoch: 160 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015858\n","172it [00:05, 29.46it/s]Train epoch: 160 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014369\n","200it [00:06, 30.18it/s]Train epoch: 160 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.016161\n","223it [00:07, 28.65it/s]Train epoch: 160 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016974\n","250it [00:07, 27.83it/s]Train epoch: 160 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.021382\n","273it [00:08, 28.47it/s]Train epoch: 160 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.015956\n","298it [00:09, 27.79it/s]Train epoch: 160 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013520\n","325it [00:10, 26.02it/s]Train epoch: 160 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013619\n","349it [00:11, 26.47it/s]Train epoch: 160 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.017862\n","373it [00:12, 25.01it/s]Train epoch: 160 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.017578\n","400it [00:13, 24.27it/s]Train epoch: 160 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.016891\n","424it [00:14, 24.65it/s]Train epoch: 160 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.016072\n","448it [00:15, 22.43it/s]Train epoch: 160 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.015828\n","475it [00:16, 20.99it/s]Train epoch: 160 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.018409\n","500it [00:18, 18.59it/s]Train epoch: 160 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.018032\n","505it [00:18, 27.32it/s]\n","epoch loss: 0.015987726018618847\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 328.10it/s]\n","Finish save rediction by checkpoint  160\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3800, 0.5636, 0.4954, 0.5273, 0.8575\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4081, 0.6180, 0.5457, 0.5796, 0.8879\n","rec_at_5: 0.5472\n","prec_at_5: 0.5620\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 161\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 161 [batch #0, batch_size 16, seq length 212]\tLoss: 0.061835\n","24it [00:00, 36.46it/s]Train epoch: 161 [batch #25, batch_size 16, seq length 571]\tLoss: 0.012699\n","48it [00:01, 34.67it/s]Train epoch: 161 [batch #50, batch_size 16, seq length 709]\tLoss: 0.014987\n","72it [00:02, 33.20it/s]Train epoch: 161 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018282\n","100it [00:02, 32.50it/s]Train epoch: 161 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014668\n","124it [00:03, 32.56it/s]Train epoch: 161 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017715\n","148it [00:04, 30.60it/s]Train epoch: 161 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017969\n","172it [00:05, 29.79it/s]Train epoch: 161 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.012827\n","198it [00:06, 29.84it/s]Train epoch: 161 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.013370\n","224it [00:07, 27.61it/s]Train epoch: 161 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.019433\n","249it [00:07, 27.78it/s]Train epoch: 161 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.018337\n","273it [00:08, 28.94it/s]Train epoch: 161 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.013839\n","300it [00:09, 27.75it/s]Train epoch: 161 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.014437\n","324it [00:10, 25.35it/s]Train epoch: 161 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.015185\n","348it [00:11, 26.39it/s]Train epoch: 161 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.014969\n","375it [00:12, 25.20it/s]Train epoch: 161 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.019282\n","399it [00:13, 24.39it/s]Train epoch: 161 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.014923\n","423it [00:14, 23.30it/s]Train epoch: 161 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.016131\n","450it [00:15, 22.42it/s]Train epoch: 161 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.016560\n","474it [00:17, 21.46it/s]Train epoch: 161 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.018618\n","498it [00:18, 20.19it/s]Train epoch: 161 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017652\n","505it [00:18, 27.17it/s]\n","epoch loss: 0.015755996502714583\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.90it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3790, 0.5624, 0.4954, 0.5268, 0.8572\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4074, 0.6173, 0.5451, 0.5789, 0.8877\n","rec_at_5: 0.5447\n","prec_at_5: 0.5608\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 162\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 162 [batch #0, batch_size 16, seq length 212]\tLoss: 0.046987\n","24it [00:00, 35.55it/s]Train epoch: 162 [batch #25, batch_size 16, seq length 571]\tLoss: 0.015862\n","48it [00:01, 33.44it/s]Train epoch: 162 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013768\n","72it [00:02, 32.70it/s]Train epoch: 162 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018233\n","100it [00:03, 32.03it/s]Train epoch: 162 [batch #100, batch_size 16, seq length 892]\tLoss: 0.017266\n","124it [00:03, 29.22it/s]Train epoch: 162 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016142\n","148it [00:04, 29.74it/s]Train epoch: 162 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017412\n","174it [00:05, 30.17it/s]Train epoch: 162 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.014012\n","199it [00:06, 28.46it/s]Train epoch: 162 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.012390\n","223it [00:07, 23.34it/s]Train epoch: 162 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016191\n","249it [00:09, 15.09it/s]Train epoch: 162 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.017175\n","275it [00:10, 14.30it/s]Train epoch: 162 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.016550\n","299it [00:12, 15.05it/s]Train epoch: 162 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.014971\n","323it [00:13, 23.67it/s]Train epoch: 162 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013118\n","350it [00:14, 26.08it/s]Train epoch: 162 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.014794\n","374it [00:15, 24.49it/s]Train epoch: 162 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.014863\n","398it [00:16, 23.60it/s]Train epoch: 162 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.014643\n","425it [00:17, 23.67it/s]Train epoch: 162 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013820\n","449it [00:19, 13.90it/s]Train epoch: 162 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014736\n","475it [00:21, 13.13it/s]Train epoch: 162 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.017486\n","499it [00:22, 19.18it/s]Train epoch: 162 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.015893\n","505it [00:22, 22.12it/s]\n","epoch loss: 0.014859129825150502\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 325.79it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3793, 0.5638, 0.4945, 0.5269, 0.8569\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4080, 0.6183, 0.5454, 0.5796, 0.8875\n","rec_at_5: 0.5438\n","prec_at_5: 0.5589\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 163\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 163 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035248\n","24it [00:00, 35.03it/s]Train epoch: 163 [batch #25, batch_size 16, seq length 571]\tLoss: 0.014656\n","48it [00:01, 34.33it/s]Train epoch: 163 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012742\n","72it [00:02, 32.22it/s]Train epoch: 163 [batch #75, batch_size 16, seq length 806]\tLoss: 0.017919\n","100it [00:03, 32.43it/s]Train epoch: 163 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014603\n","124it [00:03, 30.99it/s]Train epoch: 163 [batch #125, batch_size 16, seq length 978]\tLoss: 0.017497\n","149it [00:04, 27.86it/s]Train epoch: 163 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.017264\n","174it [00:05, 28.36it/s]Train epoch: 163 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011794\n","198it [00:06, 29.50it/s]Train epoch: 163 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.012796\n","225it [00:07, 27.65it/s]Train epoch: 163 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016125\n","250it [00:08, 27.69it/s]Train epoch: 163 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015308\n","274it [00:09, 26.84it/s]Train epoch: 163 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.014140\n","298it [00:10, 26.85it/s]Train epoch: 163 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.014211\n","325it [00:11, 25.98it/s]Train epoch: 163 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.016131\n","349it [00:12, 25.74it/s]Train epoch: 163 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.016671\n","373it [00:13, 24.20it/s]Train epoch: 163 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.015417\n","400it [00:14, 23.16it/s]Train epoch: 163 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.014693\n","424it [00:15, 22.45it/s]Train epoch: 163 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013760\n","448it [00:16, 21.93it/s]Train epoch: 163 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014717\n","475it [00:17, 20.50it/s]Train epoch: 163 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.017416\n","500it [00:18, 18.71it/s]Train epoch: 163 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017304\n","505it [00:19, 26.24it/s]\n","epoch loss: 0.014960867585326167\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.38it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3802, 0.5628, 0.4958, 0.5272, 0.8569\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4080, 0.6187, 0.5451, 0.5795, 0.8874\n","rec_at_5: 0.5443\n","prec_at_5: 0.5598\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 164\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 164 [batch #0, batch_size 16, seq length 212]\tLoss: 0.041887\n","24it [00:00, 35.43it/s]Train epoch: 164 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013681\n","48it [00:01, 34.60it/s]Train epoch: 164 [batch #50, batch_size 16, seq length 709]\tLoss: 0.013404\n","72it [00:02, 34.15it/s]Train epoch: 164 [batch #75, batch_size 16, seq length 806]\tLoss: 0.017023\n","100it [00:02, 32.96it/s]Train epoch: 164 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016214\n","124it [00:03, 31.21it/s]Train epoch: 164 [batch #125, batch_size 16, seq length 978]\tLoss: 0.016137\n","148it [00:04, 31.18it/s]Train epoch: 164 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.016870\n","172it [00:05, 31.13it/s]Train epoch: 164 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011689\n","198it [00:06, 29.15it/s]Train epoch: 164 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.014556\n","224it [00:07, 28.31it/s]Train epoch: 164 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.016474\n","249it [00:07, 28.72it/s]Train epoch: 164 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015826\n","273it [00:08, 26.77it/s]Train epoch: 164 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.013514\n","299it [00:09, 26.52it/s]Train epoch: 164 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.015707\n","323it [00:10, 26.41it/s]Train epoch: 164 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013343\n","350it [00:11, 25.73it/s]Train epoch: 164 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013716\n","374it [00:12, 26.38it/s]Train epoch: 164 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012397\n","398it [00:13, 24.05it/s]Train epoch: 164 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013978\n","425it [00:14, 23.98it/s]Train epoch: 164 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013386\n","449it [00:15, 22.38it/s]Train epoch: 164 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.013528\n","473it [00:16, 21.17it/s]Train epoch: 164 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014929\n","500it [00:18, 19.85it/s]Train epoch: 164 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.017091\n","505it [00:18, 27.16it/s]\n","epoch loss: 0.014372373269934272\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.87it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3787, 0.5639, 0.4929, 0.5260, 0.8569\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4078, 0.6204, 0.5435, 0.5794, 0.8875\n","rec_at_5: 0.5452\n","prec_at_5: 0.5617\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 165\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 165 [batch #0, batch_size 16, seq length 212]\tLoss: 0.032215\n","25it [00:00, 37.06it/s]Train epoch: 165 [batch #25, batch_size 16, seq length 571]\tLoss: 0.012149\n","49it [00:01, 34.87it/s]Train epoch: 165 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012766\n","73it [00:02, 32.60it/s]Train epoch: 165 [batch #75, batch_size 16, seq length 806]\tLoss: 0.016090\n","97it [00:02, 32.27it/s]Train epoch: 165 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012650\n","125it [00:03, 30.49it/s]Train epoch: 165 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015382\n","149it [00:04, 31.41it/s]Train epoch: 165 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015638\n","173it [00:05, 29.93it/s]Train epoch: 165 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.012797\n","198it [00:06, 29.13it/s]Train epoch: 165 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011605\n","225it [00:07, 28.24it/s]Train epoch: 165 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.017145\n","249it [00:07, 28.83it/s]Train epoch: 165 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.017172\n","273it [00:08, 28.15it/s]Train epoch: 165 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.014443\n","300it [00:09, 25.78it/s]Train epoch: 165 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013926\n","325it [00:10, 25.47it/s]Train epoch: 165 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.014531\n","349it [00:11, 26.26it/s]Train epoch: 165 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.015269\n","373it [00:12, 26.36it/s]Train epoch: 165 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.014908\n","400it [00:13, 25.54it/s]Train epoch: 165 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.015489\n","424it [00:14, 23.67it/s]Train epoch: 165 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.012347\n","448it [00:15, 23.95it/s]Train epoch: 165 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014978\n","475it [00:16, 20.96it/s]Train epoch: 165 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.013928\n","500it [00:18, 18.96it/s]Train epoch: 165 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.015132\n","505it [00:18, 27.30it/s]\n","epoch loss: 0.014037310508757572\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 330.62it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3791, 0.5633, 0.4935, 0.5261, 0.8570\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4068, 0.6178, 0.5436, 0.5783, 0.8876\n","rec_at_5: 0.5450\n","prec_at_5: 0.5613\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 166\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 166 [batch #0, batch_size 16, seq length 212]\tLoss: 0.033287\n","25it [00:00, 36.77it/s]Train epoch: 166 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013784\n","49it [00:01, 35.52it/s]Train epoch: 166 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011137\n","73it [00:02, 34.27it/s]Train epoch: 166 [batch #75, batch_size 16, seq length 806]\tLoss: 0.018145\n","97it [00:02, 32.68it/s]Train epoch: 166 [batch #100, batch_size 16, seq length 892]\tLoss: 0.016332\n","125it [00:03, 31.61it/s]Train epoch: 166 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015118\n","149it [00:04, 30.43it/s]Train epoch: 166 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.016134\n","173it [00:05, 31.68it/s]Train epoch: 166 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011991\n","198it [00:06, 28.74it/s]Train epoch: 166 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011351\n","224it [00:06, 29.07it/s]Train epoch: 166 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.017271\n","249it [00:07, 27.99it/s]Train epoch: 166 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.014957\n","274it [00:08, 27.51it/s]Train epoch: 166 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.013270\n","299it [00:09, 25.98it/s]Train epoch: 166 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012758\n","323it [00:10, 26.01it/s]Train epoch: 166 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013307\n","350it [00:11, 25.91it/s]Train epoch: 166 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.013779\n","374it [00:12, 24.46it/s]Train epoch: 166 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.016108\n","398it [00:13, 24.83it/s]Train epoch: 166 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013533\n","425it [00:14, 23.85it/s]Train epoch: 166 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.015955\n","449it [00:15, 22.46it/s]Train epoch: 166 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.013339\n","473it [00:16, 21.72it/s]Train epoch: 166 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.015234\n","500it [00:18, 19.33it/s]Train epoch: 166 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.015831\n","505it [00:18, 27.31it/s]\n","epoch loss: 0.013859495440138375\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.60it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3785, 0.5629, 0.4936, 0.5260, 0.8565\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4080, 0.6169, 0.5465, 0.5796, 0.8873\n","rec_at_5: 0.5433\n","prec_at_5: 0.5599\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 167\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 167 [batch #0, batch_size 16, seq length 212]\tLoss: 0.037336\n","24it [00:00, 35.98it/s]Train epoch: 167 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013725\n","48it [00:01, 36.27it/s]Train epoch: 167 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011976\n","72it [00:02, 34.61it/s]Train epoch: 167 [batch #75, batch_size 16, seq length 806]\tLoss: 0.014239\n","100it [00:02, 32.66it/s]Train epoch: 167 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013592\n","124it [00:03, 33.33it/s]Train epoch: 167 [batch #125, batch_size 16, seq length 978]\tLoss: 0.014328\n","148it [00:04, 32.08it/s]Train epoch: 167 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014421\n","175it [00:05, 28.58it/s]Train epoch: 167 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011762\n","197it [00:06, 28.57it/s]Train epoch: 167 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010441\n","222it [00:06, 29.18it/s]Train epoch: 167 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.014588\n","249it [00:07, 27.93it/s]Train epoch: 167 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.015250\n","274it [00:08, 28.30it/s]Train epoch: 167 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.012711\n","298it [00:09, 27.02it/s]Train epoch: 167 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012714\n","325it [00:10, 26.03it/s]Train epoch: 167 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.013125\n","349it [00:11, 26.24it/s]Train epoch: 167 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.012872\n","373it [00:12, 24.44it/s]Train epoch: 167 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.014614\n","400it [00:13, 25.04it/s]Train epoch: 167 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013031\n","424it [00:14, 23.10it/s]Train epoch: 167 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011673\n","448it [00:15, 22.04it/s]Train epoch: 167 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.014504\n","475it [00:16, 21.31it/s]Train epoch: 167 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.013584\n","499it [00:17, 19.88it/s]Train epoch: 167 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012519\n","505it [00:18, 27.61it/s]\n","epoch loss: 0.013168759415350338\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 339.38it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3792, 0.5621, 0.4953, 0.5266, 0.8568\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4086, 0.6174, 0.5471, 0.5802, 0.8874\n","rec_at_5: 0.5433\n","prec_at_5: 0.5599\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 168\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 168 [batch #0, batch_size 16, seq length 212]\tLoss: 0.033577\n","25it [00:00, 36.37it/s]Train epoch: 168 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013502\n","49it [00:01, 33.99it/s]Train epoch: 168 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012809\n","73it [00:02, 34.48it/s]Train epoch: 168 [batch #75, batch_size 16, seq length 806]\tLoss: 0.017046\n","97it [00:02, 32.55it/s]Train epoch: 168 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013975\n","125it [00:03, 32.40it/s]Train epoch: 168 [batch #125, batch_size 16, seq length 978]\tLoss: 0.013890\n","149it [00:04, 31.30it/s]Train epoch: 168 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013145\n","173it [00:05, 32.39it/s]Train epoch: 168 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.011378\n","200it [00:06, 29.44it/s]Train epoch: 168 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.011783\n","223it [00:06, 29.38it/s]Train epoch: 168 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012917\n","247it [00:07, 28.77it/s]Train epoch: 168 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.013537\n","272it [00:08, 27.75it/s]Train epoch: 168 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010512\n","299it [00:09, 28.12it/s]Train epoch: 168 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013058\n","325it [00:10, 27.97it/s]Train epoch: 168 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010520\n","350it [00:11, 27.31it/s]Train epoch: 168 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011927\n","374it [00:12, 25.14it/s]Train epoch: 168 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012791\n","398it [00:13, 24.76it/s]Train epoch: 168 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.013711\n","425it [00:14, 23.76it/s]Train epoch: 168 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011470\n","449it [00:15, 22.42it/s]Train epoch: 168 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011970\n","473it [00:16, 21.31it/s]Train epoch: 168 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.012865\n","499it [00:17, 19.02it/s]Train epoch: 168 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013841\n","505it [00:18, 27.72it/s]\n","epoch loss: 0.012792329670628533\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.23it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3784, 0.5607, 0.4948, 0.5257, 0.8564\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4068, 0.6174, 0.5439, 0.5783, 0.8870\n","rec_at_5: 0.5455\n","prec_at_5: 0.5603\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 169\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 169 [batch #0, batch_size 16, seq length 212]\tLoss: 0.028006\n","25it [00:00, 35.84it/s]Train epoch: 169 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011581\n","49it [00:01, 36.14it/s]Train epoch: 169 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012435\n","73it [00:02, 32.20it/s]Train epoch: 169 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013967\n","97it [00:02, 33.00it/s]Train epoch: 169 [batch #100, batch_size 16, seq length 892]\tLoss: 0.014256\n","125it [00:03, 30.96it/s]Train epoch: 169 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012834\n","149it [00:04, 31.79it/s]Train epoch: 169 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014602\n","173it [00:05, 31.35it/s]Train epoch: 169 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010879\n","197it [00:05, 30.11it/s]Train epoch: 169 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010915\n","224it [00:06, 28.52it/s]Train epoch: 169 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.012654\n","247it [00:07, 26.57it/s]Train epoch: 169 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.014240\n","275it [00:08, 28.13it/s]Train epoch: 169 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.012399\n","299it [00:09, 28.18it/s]Train epoch: 169 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013878\n","324it [00:10, 27.02it/s]Train epoch: 169 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.012635\n","348it [00:11, 26.92it/s]Train epoch: 169 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011684\n","375it [00:12, 26.96it/s]Train epoch: 169 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012401\n","399it [00:13, 25.02it/s]Train epoch: 169 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011546\n","423it [00:14, 23.62it/s]Train epoch: 169 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.014706\n","450it [00:15, 22.24it/s]Train epoch: 169 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010741\n","474it [00:16, 21.59it/s]Train epoch: 169 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014034\n","500it [00:17, 19.34it/s]Train epoch: 169 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.014205\n","505it [00:18, 27.65it/s]\n","epoch loss: 0.012541730289282923\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 336.21it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3792, 0.5613, 0.4954, 0.5263, 0.8561\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4070, 0.6159, 0.5454, 0.5785, 0.8868\n","rec_at_5: 0.5432\n","prec_at_5: 0.5591\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 170\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 170 [batch #0, batch_size 16, seq length 212]\tLoss: 0.036587\n","24it [00:00, 34.90it/s]Train epoch: 170 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013193\n","48it [00:01, 35.45it/s]Train epoch: 170 [batch #50, batch_size 16, seq length 709]\tLoss: 0.012143\n","72it [00:02, 33.60it/s]Train epoch: 170 [batch #75, batch_size 16, seq length 806]\tLoss: 0.014291\n","100it [00:02, 31.22it/s]Train epoch: 170 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012624\n","124it [00:03, 31.77it/s]Train epoch: 170 [batch #125, batch_size 16, seq length 978]\tLoss: 0.015545\n","148it [00:04, 30.44it/s]Train epoch: 170 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.013801\n","172it [00:05, 30.85it/s]Train epoch: 170 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010974\n","200it [00:06, 30.34it/s]Train epoch: 170 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010682\n","224it [00:06, 28.71it/s]Train epoch: 170 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011752\n","250it [00:07, 28.98it/s]Train epoch: 170 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.013942\n","272it [00:08, 26.99it/s]Train epoch: 170 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.012534\n","300it [00:09, 27.08it/s]Train epoch: 170 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010256\n","325it [00:10, 25.75it/s]Train epoch: 170 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.011008\n","349it [00:11, 27.02it/s]Train epoch: 170 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011754\n","373it [00:12, 25.95it/s]Train epoch: 170 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.013083\n","400it [00:13, 24.39it/s]Train epoch: 170 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011285\n","424it [00:14, 23.93it/s]Train epoch: 170 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.012591\n","448it [00:15, 21.90it/s]Train epoch: 170 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011978\n","475it [00:16, 21.70it/s]Train epoch: 170 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011896\n","499it [00:18, 19.17it/s]Train epoch: 170 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.014787\n","505it [00:18, 27.48it/s]\n","epoch loss: 0.012314325911142761\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 337.17it/s]\n","Finish save rediction by checkpoint  170\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3774, 0.5614, 0.4923, 0.5246, 0.8562\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4054, 0.6166, 0.5421, 0.5769, 0.8869\n","rec_at_5: 0.5436\n","prec_at_5: 0.5580\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 171\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 171 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025571\n","22it [00:00, 36.58it/s]Train epoch: 171 [batch #25, batch_size 16, seq length 571]\tLoss: 0.013582\n","50it [00:01, 35.34it/s]Train epoch: 171 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010703\n","74it [00:02, 33.76it/s]Train epoch: 171 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015389\n","98it [00:02, 31.47it/s]Train epoch: 171 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012470\n","122it [00:03, 32.50it/s]Train epoch: 171 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011997\n","150it [00:04, 17.87it/s]Train epoch: 171 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.014631\n","174it [00:06, 18.93it/s]Train epoch: 171 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009753\n","199it [00:07, 19.44it/s]Train epoch: 171 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010069\n","225it [00:09, 17.59it/s]Train epoch: 171 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013543\n","248it [00:10, 27.00it/s]Train epoch: 171 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.013361\n","273it [00:10, 27.67it/s]Train epoch: 171 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009275\n","299it [00:11, 25.74it/s]Train epoch: 171 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.011754\n","325it [00:12, 27.36it/s]Train epoch: 171 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010608\n","349it [00:13, 26.91it/s]Train epoch: 171 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009448\n","373it [00:14, 26.66it/s]Train epoch: 171 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.012128\n","400it [00:15, 25.84it/s]Train epoch: 171 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.012893\n","424it [00:16, 25.05it/s]Train epoch: 171 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010519\n","448it [00:17, 23.80it/s]Train epoch: 171 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.011944\n","475it [00:18, 22.55it/s]Train epoch: 171 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.012890\n","499it [00:20, 20.23it/s]Train epoch: 171 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012680\n","505it [00:20, 24.71it/s]\n","epoch loss: 0.011967353871792997\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 338.33it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3781, 0.5622, 0.4928, 0.5252, 0.8563\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4068, 0.6177, 0.5437, 0.5783, 0.8870\n","rec_at_5: 0.5439\n","prec_at_5: 0.5601\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 172\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 172 [batch #0, batch_size 16, seq length 212]\tLoss: 0.039490\n","25it [00:00, 37.47it/s]Train epoch: 172 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008717\n","49it [00:01, 35.55it/s]Train epoch: 172 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011448\n","73it [00:02, 33.38it/s]Train epoch: 172 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013033\n","97it [00:02, 33.62it/s]Train epoch: 172 [batch #100, batch_size 16, seq length 892]\tLoss: 0.015980\n","125it [00:03, 33.34it/s]Train epoch: 172 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011654\n","149it [00:04, 30.15it/s]Train epoch: 172 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.015352\n","173it [00:05, 30.49it/s]Train epoch: 172 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.010846\n","200it [00:06, 29.63it/s]Train epoch: 172 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010738\n","223it [00:06, 28.86it/s]Train epoch: 172 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011107\n","250it [00:07, 29.25it/s]Train epoch: 172 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012089\n","273it [00:08, 28.33it/s]Train epoch: 172 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.011149\n","299it [00:09, 28.03it/s]Train epoch: 172 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.013002\n","323it [00:10, 27.88it/s]Train epoch: 172 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009883\n","350it [00:11, 26.21it/s]Train epoch: 172 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.011943\n","374it [00:12, 25.93it/s]Train epoch: 172 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.011435\n","398it [00:13, 24.07it/s]Train epoch: 172 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.011300\n","425it [00:14, 24.20it/s]Train epoch: 172 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.013071\n","449it [00:15, 23.57it/s]Train epoch: 172 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.012562\n","473it [00:16, 20.80it/s]Train epoch: 172 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.014160\n","500it [00:18, 19.53it/s]Train epoch: 172 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.013259\n","505it [00:18, 27.50it/s]\n","epoch loss: 0.011988149211502238\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 338.93it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3759, 0.5583, 0.4906, 0.5223, 0.8561\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4046, 0.6150, 0.5417, 0.5761, 0.8867\n","rec_at_5: 0.5439\n","prec_at_5: 0.5594\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 173\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 173 [batch #0, batch_size 16, seq length 212]\tLoss: 0.022949\n","24it [00:00, 36.96it/s]Train epoch: 173 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009965\n","48it [00:01, 34.39it/s]Train epoch: 173 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009538\n","72it [00:02, 32.92it/s]Train epoch: 173 [batch #75, batch_size 16, seq length 806]\tLoss: 0.014638\n","100it [00:02, 32.24it/s]Train epoch: 173 [batch #100, batch_size 16, seq length 892]\tLoss: 0.013415\n","124it [00:03, 31.69it/s]Train epoch: 173 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012013\n","148it [00:04, 31.43it/s]Train epoch: 173 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.012860\n","172it [00:05, 30.72it/s]Train epoch: 173 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008313\n","200it [00:06, 29.34it/s]Train epoch: 173 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009818\n","223it [00:06, 29.61it/s]Train epoch: 173 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011787\n","249it [00:07, 28.62it/s]Train epoch: 173 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.012513\n","275it [00:08, 27.92it/s]Train epoch: 173 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.012059\n","300it [00:09, 27.54it/s]Train epoch: 173 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.012127\n","325it [00:10, 25.91it/s]Train epoch: 173 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.012014\n","349it [00:11, 27.24it/s]Train epoch: 173 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009848\n","373it [00:12, 25.98it/s]Train epoch: 173 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010394\n","400it [00:13, 23.77it/s]Train epoch: 173 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.012307\n","424it [00:14, 23.60it/s]Train epoch: 173 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011775\n","448it [00:15, 23.25it/s]Train epoch: 173 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.012684\n","475it [00:16, 22.03it/s]Train epoch: 173 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.012207\n","499it [00:17, 19.84it/s]Train epoch: 173 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010670\n","505it [00:18, 27.77it/s]\n","epoch loss: 0.01128994146878324\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 341.78it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3762, 0.5589, 0.4911, 0.5228, 0.8560\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4062, 0.6171, 0.5430, 0.5777, 0.8867\n","rec_at_5: 0.5427\n","prec_at_5: 0.5587\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 174\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 174 [batch #0, batch_size 16, seq length 212]\tLoss: 0.028616\n","25it [00:00, 36.85it/s]Train epoch: 174 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011878\n","49it [00:01, 34.12it/s]Train epoch: 174 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010316\n","73it [00:02, 33.87it/s]Train epoch: 174 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015473\n","97it [00:02, 33.24it/s]Train epoch: 174 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012608\n","125it [00:03, 32.93it/s]Train epoch: 174 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011275\n","149it [00:04, 31.96it/s]Train epoch: 174 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009717\n","173it [00:05, 30.82it/s]Train epoch: 174 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008746\n","197it [00:05, 30.17it/s]Train epoch: 174 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.010423\n","223it [00:06, 29.77it/s]Train epoch: 174 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.013225\n","250it [00:07, 29.38it/s]Train epoch: 174 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010798\n","273it [00:08, 28.94it/s]Train epoch: 174 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009317\n","300it [00:09, 29.00it/s]Train epoch: 174 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010734\n","324it [00:10, 26.47it/s]Train epoch: 174 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.010694\n","348it [00:11, 27.48it/s]Train epoch: 174 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010522\n","375it [00:12, 26.01it/s]Train epoch: 174 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.011237\n","399it [00:13, 25.67it/s]Train epoch: 174 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010545\n","423it [00:14, 24.34it/s]Train epoch: 174 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011069\n","450it [00:15, 22.84it/s]Train epoch: 174 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010271\n","474it [00:16, 22.19it/s]Train epoch: 174 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011930\n","500it [00:17, 19.12it/s]Train epoch: 174 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010925\n","505it [00:18, 27.83it/s]\n","epoch loss: 0.010971245920691307\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:05, 280.32it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3790, 0.5607, 0.4951, 0.5259, 0.8562\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4084, 0.6181, 0.5462, 0.5799, 0.8869\n","rec_at_5: 0.5444\n","prec_at_5: 0.5597\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 175\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 175 [batch #0, batch_size 16, seq length 212]\tLoss: 0.023698\n","25it [00:00, 34.93it/s]Train epoch: 175 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011401\n","49it [00:01, 35.40it/s]Train epoch: 175 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008761\n","73it [00:02, 33.54it/s]Train epoch: 175 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013305\n","97it [00:02, 31.95it/s]Train epoch: 175 [batch #100, batch_size 16, seq length 892]\tLoss: 0.012662\n","125it [00:03, 31.73it/s]Train epoch: 175 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011995\n","149it [00:04, 30.14it/s]Train epoch: 175 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.012188\n","173it [00:05, 31.41it/s]Train epoch: 175 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008884\n","200it [00:06, 30.22it/s]Train epoch: 175 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008608\n","224it [00:06, 28.70it/s]Train epoch: 175 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011253\n","248it [00:07, 29.37it/s]Train epoch: 175 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.011896\n","273it [00:08, 28.14it/s]Train epoch: 175 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010200\n","298it [00:09, 27.57it/s]Train epoch: 175 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010242\n","325it [00:10, 27.21it/s]Train epoch: 175 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008910\n","349it [00:11, 26.36it/s]Train epoch: 175 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010352\n","373it [00:12, 25.79it/s]Train epoch: 175 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.011653\n","400it [00:13, 24.73it/s]Train epoch: 175 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010857\n","424it [00:14, 23.64it/s]Train epoch: 175 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.012940\n","448it [00:15, 23.02it/s]Train epoch: 175 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010454\n","475it [00:16, 22.93it/s]Train epoch: 175 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.011013\n","499it [00:17, 19.29it/s]Train epoch: 175 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.011168\n","505it [00:18, 27.74it/s]\n","epoch loss: 0.010902658136536155\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.30it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3766, 0.5587, 0.4921, 0.5233, 0.8560\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4067, 0.6168, 0.5441, 0.5782, 0.8867\n","rec_at_5: 0.5441\n","prec_at_5: 0.5594\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 176\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 176 [batch #0, batch_size 16, seq length 212]\tLoss: 0.039413\n","25it [00:00, 37.08it/s]Train epoch: 176 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011800\n","49it [00:01, 35.40it/s]Train epoch: 176 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010369\n","73it [00:02, 33.54it/s]Train epoch: 176 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012582\n","97it [00:02, 33.50it/s]Train epoch: 176 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010855\n","125it [00:03, 32.62it/s]Train epoch: 176 [batch #125, batch_size 16, seq length 978]\tLoss: 0.012716\n","149it [00:04, 32.71it/s]Train epoch: 176 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011023\n","173it [00:05, 29.77it/s]Train epoch: 176 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009480\n","199it [00:06, 28.53it/s]Train epoch: 176 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009597\n","225it [00:07, 28.13it/s]Train epoch: 176 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011353\n","248it [00:07, 28.58it/s]Train epoch: 176 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009259\n","275it [00:08, 28.02it/s]Train epoch: 176 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.010355\n","300it [00:09, 27.95it/s]Train epoch: 176 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010049\n","324it [00:10, 27.52it/s]Train epoch: 176 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008550\n","348it [00:11, 27.24it/s]Train epoch: 176 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008562\n","375it [00:12, 24.81it/s]Train epoch: 176 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009661\n","399it [00:13, 24.86it/s]Train epoch: 176 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009211\n","423it [00:14, 24.97it/s]Train epoch: 176 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011200\n","450it [00:15, 23.89it/s]Train epoch: 176 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008614\n","474it [00:16, 21.72it/s]Train epoch: 176 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010627\n","500it [00:17, 19.49it/s]Train epoch: 176 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.011324\n","505it [00:18, 27.70it/s]\n","epoch loss: 0.010664894208277455\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 339.54it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3757, 0.5574, 0.4916, 0.5224, 0.8554\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4052, 0.6147, 0.5431, 0.5767, 0.8863\n","rec_at_5: 0.5436\n","prec_at_5: 0.5588\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 177\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 177 [batch #0, batch_size 16, seq length 212]\tLoss: 0.042477\n","24it [00:00, 36.71it/s]Train epoch: 177 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009329\n","48it [00:01, 34.82it/s]Train epoch: 177 [batch #50, batch_size 16, seq length 709]\tLoss: 0.011022\n","72it [00:02, 34.55it/s]Train epoch: 177 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011261\n","100it [00:02, 33.39it/s]Train epoch: 177 [batch #100, batch_size 16, seq length 892]\tLoss: 0.011210\n","124it [00:03, 31.23it/s]Train epoch: 177 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011793\n","148it [00:04, 30.82it/s]Train epoch: 177 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011568\n","172it [00:05, 30.87it/s]Train epoch: 177 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.009540\n","200it [00:06, 30.27it/s]Train epoch: 177 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009209\n","225it [00:06, 29.22it/s]Train epoch: 177 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011973\n","247it [00:07, 27.84it/s]Train epoch: 177 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009912\n","275it [00:08, 28.56it/s]Train epoch: 177 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009232\n","300it [00:09, 26.82it/s]Train epoch: 177 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.008762\n","325it [00:10, 27.02it/s]Train epoch: 177 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008971\n","349it [00:11, 26.98it/s]Train epoch: 177 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009764\n","373it [00:12, 25.87it/s]Train epoch: 177 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010023\n","400it [00:13, 24.39it/s]Train epoch: 177 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010663\n","424it [00:14, 23.39it/s]Train epoch: 177 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010250\n","448it [00:15, 22.63it/s]Train epoch: 177 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009339\n","475it [00:16, 21.66it/s]Train epoch: 177 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010152\n","499it [00:18, 19.34it/s]Train epoch: 177 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.012298\n","505it [00:18, 27.59it/s]\n","epoch loss: 0.010325947761452655\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 340.87it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3772, 0.5585, 0.4940, 0.5243, 0.8554\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4058, 0.6137, 0.5450, 0.5773, 0.8864\n","rec_at_5: 0.5432\n","prec_at_5: 0.5589\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 178\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 178 [batch #0, batch_size 16, seq length 212]\tLoss: 0.018115\n","22it [00:00, 37.54it/s]Train epoch: 178 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010675\n","50it [00:01, 34.90it/s]Train epoch: 178 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009301\n","74it [00:02, 33.03it/s]Train epoch: 178 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011717\n","98it [00:02, 30.83it/s]Train epoch: 178 [batch #100, batch_size 16, seq length 892]\tLoss: 0.011725\n","122it [00:03, 31.93it/s]Train epoch: 178 [batch #125, batch_size 16, seq length 978]\tLoss: 0.011792\n","150it [00:04, 30.04it/s]Train epoch: 178 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009332\n","174it [00:05, 30.46it/s]Train epoch: 178 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008112\n","199it [00:06, 27.78it/s]Train epoch: 178 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008085\n","224it [00:07, 28.23it/s]Train epoch: 178 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.011200\n","247it [00:07, 27.88it/s]Train epoch: 178 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010856\n","274it [00:08, 28.03it/s]Train epoch: 178 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007517\n","299it [00:09, 26.73it/s]Train epoch: 178 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.009313\n","324it [00:10, 25.98it/s]Train epoch: 178 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009151\n","348it [00:11, 25.45it/s]Train epoch: 178 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009883\n","375it [00:12, 24.54it/s]Train epoch: 178 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010282\n","399it [00:13, 24.22it/s]Train epoch: 178 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.010577\n","423it [00:14, 23.51it/s]Train epoch: 178 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.010437\n","450it [00:15, 22.44it/s]Train epoch: 178 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009970\n","474it [00:16, 19.96it/s]Train epoch: 178 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009533\n","498it [00:18, 19.41it/s]Train epoch: 178 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009851\n","505it [00:18, 27.24it/s]\n","epoch loss: 0.01005742424651796\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 332.13it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3769, 0.5588, 0.4944, 0.5246, 0.8557\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4068, 0.6142, 0.5464, 0.5783, 0.8863\n","rec_at_5: 0.5427\n","prec_at_5: 0.5577\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 179\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 179 [batch #0, batch_size 16, seq length 212]\tLoss: 0.022524\n","24it [00:00, 35.91it/s]Train epoch: 179 [batch #25, batch_size 16, seq length 571]\tLoss: 0.011518\n","48it [00:01, 34.40it/s]Train epoch: 179 [batch #50, batch_size 16, seq length 709]\tLoss: 0.010866\n","72it [00:02, 34.14it/s]Train epoch: 179 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012331\n","100it [00:02, 33.60it/s]Train epoch: 179 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010088\n","124it [00:03, 31.66it/s]Train epoch: 179 [batch #125, batch_size 16, seq length 978]\tLoss: 0.010124\n","148it [00:04, 30.10it/s]Train epoch: 179 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011936\n","174it [00:05, 30.80it/s]Train epoch: 179 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.007727\n","200it [00:06, 28.33it/s]Train epoch: 179 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007131\n","224it [00:07, 28.91it/s]Train epoch: 179 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010495\n","250it [00:07, 29.20it/s]Train epoch: 179 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010216\n","273it [00:08, 27.81it/s]Train epoch: 179 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008335\n","298it [00:09, 27.05it/s]Train epoch: 179 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.010419\n","325it [00:10, 26.42it/s]Train epoch: 179 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009794\n","349it [00:11, 25.98it/s]Train epoch: 179 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.010708\n","373it [00:12, 24.67it/s]Train epoch: 179 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010668\n","400it [00:13, 25.02it/s]Train epoch: 179 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008354\n","424it [00:14, 23.31it/s]Train epoch: 179 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008859\n","448it [00:15, 22.20it/s]Train epoch: 179 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.010238\n","475it [00:17, 20.20it/s]Train epoch: 179 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010933\n","500it [00:18, 18.88it/s]Train epoch: 179 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010391\n","505it [00:18, 27.03it/s]\n","epoch loss: 0.009803145535585313\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 329.91it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3756, 0.5607, 0.4889, 0.5223, 0.8555\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4043, 0.6159, 0.5407, 0.5758, 0.8865\n","rec_at_5: 0.5437\n","prec_at_5: 0.5586\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 180\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 180 [batch #0, batch_size 16, seq length 212]\tLoss: 0.022762\n","25it [00:00, 24.73it/s]Train epoch: 180 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007658\n","49it [00:02, 21.68it/s]Train epoch: 180 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009970\n","74it [00:03, 20.21it/s]Train epoch: 180 [batch #75, batch_size 16, seq length 806]\tLoss: 0.012048\n","98it [00:04, 22.28it/s]Train epoch: 180 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010525\n","125it [00:05, 19.53it/s]Train epoch: 180 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009777\n","149it [00:06, 28.89it/s]Train epoch: 180 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009881\n","173it [00:07, 30.55it/s]Train epoch: 180 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008379\n","199it [00:08, 28.79it/s]Train epoch: 180 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008882\n","223it [00:09, 30.00it/s]Train epoch: 180 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009520\n","250it [00:10, 29.60it/s]Train epoch: 180 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010461\n","273it [00:10, 28.48it/s]Train epoch: 180 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007040\n","298it [00:11, 27.68it/s]Train epoch: 180 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.009476\n","325it [00:12, 26.74it/s]Train epoch: 180 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009026\n","349it [00:13, 26.55it/s]Train epoch: 180 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.009911\n","373it [00:14, 25.94it/s]Train epoch: 180 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009343\n","400it [00:15, 25.36it/s]Train epoch: 180 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008194\n","424it [00:16, 23.25it/s]Train epoch: 180 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009192\n","448it [00:17, 23.34it/s]Train epoch: 180 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006758\n","475it [00:18, 21.71it/s]Train epoch: 180 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.010261\n","499it [00:20, 19.66it/s]Train epoch: 180 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008910\n","505it [00:20, 24.66it/s]\n","epoch loss: 0.009316254953109188\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 333.27it/s]\n","Finish save rediction by checkpoint  180\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3757, 0.5555, 0.4927, 0.5222, 0.8553\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4036, 0.6114, 0.5429, 0.5751, 0.8859\n","rec_at_5: 0.5428\n","prec_at_5: 0.5574\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 181\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 181 [batch #0, batch_size 16, seq length 212]\tLoss: 0.035457\n","24it [00:00, 37.76it/s]Train epoch: 181 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007686\n","48it [00:01, 34.54it/s]Train epoch: 181 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007704\n","72it [00:02, 33.91it/s]Train epoch: 181 [batch #75, batch_size 16, seq length 806]\tLoss: 0.010726\n","100it [00:02, 33.17it/s]Train epoch: 181 [batch #100, batch_size 16, seq length 892]\tLoss: 0.011416\n","124it [00:03, 33.32it/s]Train epoch: 181 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009093\n","148it [00:04, 31.66it/s]Train epoch: 181 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011036\n","172it [00:05, 30.53it/s]Train epoch: 181 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.008641\n","200it [00:06, 30.78it/s]Train epoch: 181 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.006887\n","224it [00:06, 29.99it/s]Train epoch: 181 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010139\n","250it [00:07, 28.79it/s]Train epoch: 181 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010146\n","273it [00:08, 29.39it/s]Train epoch: 181 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009696\n","299it [00:09, 27.36it/s]Train epoch: 181 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.008650\n","323it [00:10, 26.99it/s]Train epoch: 181 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007373\n","350it [00:11, 25.30it/s]Train epoch: 181 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007519\n","374it [00:12, 26.18it/s]Train epoch: 181 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.010170\n","398it [00:13, 24.98it/s]Train epoch: 181 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008191\n","425it [00:14, 24.50it/s]Train epoch: 181 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.011160\n","449it [00:15, 22.99it/s]Train epoch: 181 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008421\n","473it [00:16, 21.67it/s]Train epoch: 181 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008296\n","500it [00:17, 19.05it/s]Train epoch: 181 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.010706\n","505it [00:18, 27.84it/s]\n","epoch loss: 0.009327306938968544\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 342.76it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3750, 0.5567, 0.4912, 0.5219, 0.8555\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4045, 0.6139, 0.5426, 0.5761, 0.8864\n","rec_at_5: 0.5438\n","prec_at_5: 0.5596\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 182\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 182 [batch #0, batch_size 16, seq length 212]\tLoss: 0.026368\n","22it [00:00, 36.89it/s]Train epoch: 182 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010047\n","50it [00:01, 34.87it/s]Train epoch: 182 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009343\n","74it [00:02, 35.07it/s]Train epoch: 182 [batch #75, batch_size 16, seq length 806]\tLoss: 0.013964\n","98it [00:02, 33.10it/s]Train epoch: 182 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009163\n","122it [00:03, 32.96it/s]Train epoch: 182 [batch #125, batch_size 16, seq length 978]\tLoss: 0.010619\n","150it [00:04, 31.47it/s]Train epoch: 182 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011577\n","174it [00:05, 31.82it/s]Train epoch: 182 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006820\n","198it [00:05, 29.54it/s]Train epoch: 182 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007450\n","223it [00:06, 28.78it/s]Train epoch: 182 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010986\n","250it [00:07, 28.79it/s]Train epoch: 182 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009202\n","273it [00:08, 27.45it/s]Train epoch: 182 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008080\n","300it [00:09, 27.50it/s]Train epoch: 182 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.009005\n","324it [00:10, 27.57it/s]Train epoch: 182 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.006655\n","349it [00:11, 27.33it/s]Train epoch: 182 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007962\n","373it [00:12, 25.58it/s]Train epoch: 182 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009899\n","400it [00:13, 24.87it/s]Train epoch: 182 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.007489\n","424it [00:14, 24.08it/s]Train epoch: 182 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008695\n","448it [00:15, 24.20it/s]Train epoch: 182 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008689\n","475it [00:16, 21.72it/s]Train epoch: 182 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009373\n","499it [00:17, 20.14it/s]Train epoch: 182 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008702\n","505it [00:18, 27.88it/s]\n","epoch loss: 0.009257669641871448\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.04it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3767, 0.5579, 0.4926, 0.5232, 0.8555\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4059, 0.6162, 0.5431, 0.5774, 0.8863\n","rec_at_5: 0.5431\n","prec_at_5: 0.5566\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 183\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 183 [batch #0, batch_size 16, seq length 212]\tLoss: 0.024756\n","25it [00:00, 36.02it/s]Train epoch: 183 [batch #25, batch_size 16, seq length 571]\tLoss: 0.010739\n","49it [00:01, 35.18it/s]Train epoch: 183 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008539\n","73it [00:02, 33.26it/s]Train epoch: 183 [batch #75, batch_size 16, seq length 806]\tLoss: 0.010387\n","97it [00:02, 31.72it/s]Train epoch: 183 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010530\n","125it [00:03, 31.95it/s]Train epoch: 183 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008953\n","149it [00:04, 31.45it/s]Train epoch: 183 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009590\n","173it [00:05, 29.83it/s]Train epoch: 183 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006787\n","197it [00:06, 28.41it/s]Train epoch: 183 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007602\n","223it [00:07, 27.48it/s]Train epoch: 183 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010618\n","247it [00:07, 28.33it/s]Train epoch: 183 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.010513\n","275it [00:08, 26.91it/s]Train epoch: 183 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008077\n","298it [00:09, 28.62it/s]Train epoch: 183 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.009745\n","324it [00:10, 27.32it/s]Train epoch: 183 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.009211\n","348it [00:11, 26.68it/s]Train epoch: 183 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007085\n","375it [00:12, 26.59it/s]Train epoch: 183 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008455\n","399it [00:13, 24.99it/s]Train epoch: 183 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009323\n","423it [00:14, 23.05it/s]Train epoch: 183 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008719\n","450it [00:15, 23.66it/s]Train epoch: 183 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.009044\n","474it [00:16, 22.23it/s]Train epoch: 183 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.009022\n","498it [00:17, 20.09it/s]Train epoch: 183 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008976\n","505it [00:18, 27.54it/s]\n","epoch loss: 0.00897610083056411\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 343.38it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3750, 0.5567, 0.4911, 0.5218, 0.8554\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4047, 0.6129, 0.5436, 0.5762, 0.8864\n","rec_at_5: 0.5418\n","prec_at_5: 0.5572\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 184\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 184 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020092\n","25it [00:00, 35.91it/s]Train epoch: 184 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008847\n","49it [00:01, 35.33it/s]Train epoch: 184 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007167\n","73it [00:02, 33.77it/s]Train epoch: 184 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011519\n","97it [00:02, 30.76it/s]Train epoch: 184 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009994\n","125it [00:03, 33.09it/s]Train epoch: 184 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009279\n","149it [00:04, 32.04it/s]Train epoch: 184 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011110\n","173it [00:05, 31.14it/s]Train epoch: 184 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006129\n","199it [00:06, 28.49it/s]Train epoch: 184 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.006589\n","225it [00:07, 27.66it/s]Train epoch: 184 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009387\n","250it [00:07, 29.53it/s]Train epoch: 184 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.008880\n","274it [00:08, 29.38it/s]Train epoch: 184 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007543\n","299it [00:09, 28.83it/s]Train epoch: 184 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.008285\n","325it [00:10, 27.92it/s]Train epoch: 184 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.006932\n","349it [00:11, 27.59it/s]Train epoch: 184 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007635\n","373it [00:12, 26.68it/s]Train epoch: 184 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008762\n","400it [00:13, 24.92it/s]Train epoch: 184 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008355\n","424it [00:14, 24.01it/s]Train epoch: 184 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009019\n","448it [00:15, 23.70it/s]Train epoch: 184 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008953\n","475it [00:16, 21.77it/s]Train epoch: 184 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008743\n","499it [00:17, 20.26it/s]Train epoch: 184 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009921\n","505it [00:18, 27.85it/s]\n","epoch loss: 0.008852685538194335\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 328.85it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3772, 0.5601, 0.4928, 0.5243, 0.8550\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4058, 0.6153, 0.5438, 0.5773, 0.8860\n","rec_at_5: 0.5432\n","prec_at_5: 0.5583\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 185\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 185 [batch #0, batch_size 16, seq length 212]\tLoss: 0.017484\n","22it [00:00, 38.18it/s]Train epoch: 185 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008381\n","50it [00:01, 34.59it/s]Train epoch: 185 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008879\n","74it [00:02, 34.92it/s]Train epoch: 185 [batch #75, batch_size 16, seq length 806]\tLoss: 0.015062\n","98it [00:02, 33.62it/s]Train epoch: 185 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009267\n","122it [00:03, 31.47it/s]Train epoch: 185 [batch #125, batch_size 16, seq length 978]\tLoss: 0.010643\n","150it [00:04, 31.55it/s]Train epoch: 185 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009392\n","174it [00:05, 29.55it/s]Train epoch: 185 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006495\n","198it [00:06, 29.41it/s]Train epoch: 185 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007724\n","222it [00:06, 28.64it/s]Train epoch: 185 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.009761\n","250it [00:07, 29.24it/s]Train epoch: 185 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.008948\n","274it [00:08, 28.48it/s]Train epoch: 185 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007820\n","298it [00:09, 26.53it/s]Train epoch: 185 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007712\n","324it [00:10, 27.85it/s]Train epoch: 185 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007914\n","348it [00:11, 27.00it/s]Train epoch: 185 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008192\n","375it [00:12, 24.09it/s]Train epoch: 185 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008522\n","399it [00:13, 24.51it/s]Train epoch: 185 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009591\n","423it [00:14, 24.54it/s]Train epoch: 185 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.007260\n","450it [00:15, 22.84it/s]Train epoch: 185 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007083\n","474it [00:16, 21.09it/s]Train epoch: 185 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008989\n","498it [00:17, 20.07it/s]Train epoch: 185 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008993\n","505it [00:18, 27.61it/s]\n","epoch loss: 0.008456407075846974\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 335.77it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3752, 0.5590, 0.4888, 0.5216, 0.8551\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4045, 0.6177, 0.5395, 0.5760, 0.8859\n","rec_at_5: 0.5428\n","prec_at_5: 0.5582\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 186\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 186 [batch #0, batch_size 16, seq length 212]\tLoss: 0.012343\n","24it [00:00, 36.41it/s]Train epoch: 186 [batch #25, batch_size 16, seq length 571]\tLoss: 0.006644\n","48it [00:01, 34.59it/s]Train epoch: 186 [batch #50, batch_size 16, seq length 709]\tLoss: 0.009384\n","72it [00:02, 33.49it/s]Train epoch: 186 [batch #75, batch_size 16, seq length 806]\tLoss: 0.011479\n","100it [00:02, 34.05it/s]Train epoch: 186 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008725\n","124it [00:03, 33.04it/s]Train epoch: 186 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009085\n","148it [00:04, 31.75it/s]Train epoch: 186 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.007954\n","172it [00:05, 31.07it/s]Train epoch: 186 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006996\n","200it [00:06, 30.39it/s]Train epoch: 186 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.005647\n","224it [00:06, 28.82it/s]Train epoch: 186 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.007903\n","248it [00:07, 27.81it/s]Train epoch: 186 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.009864\n","274it [00:08, 27.97it/s]Train epoch: 186 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008843\n","298it [00:09, 17.59it/s]Train epoch: 186 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.008734\n","324it [00:11, 18.43it/s]Train epoch: 186 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007680\n","350it [00:12, 25.10it/s]Train epoch: 186 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008440\n","374it [00:13, 26.86it/s]Train epoch: 186 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.009001\n","398it [00:14, 25.38it/s]Train epoch: 186 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.007349\n","425it [00:15, 23.77it/s]Train epoch: 186 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009351\n","449it [00:16, 23.18it/s]Train epoch: 186 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.007988\n","473it [00:17, 21.97it/s]Train epoch: 186 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.006706\n","500it [00:18, 20.16it/s]Train epoch: 186 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007066\n","505it [00:19, 26.35it/s]\n","epoch loss: 0.008156262365709252\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 344.48it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3771, 0.5590, 0.4930, 0.5240, 0.8550\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4061, 0.6151, 0.5444, 0.5776, 0.8861\n","rec_at_5: 0.5435\n","prec_at_5: 0.5588\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 187\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 187 [batch #0, batch_size 16, seq length 212]\tLoss: 0.024034\n","24it [00:00, 36.01it/s]Train epoch: 187 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008858\n","48it [00:01, 32.52it/s]Train epoch: 187 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008462\n","72it [00:02, 33.71it/s]Train epoch: 187 [batch #75, batch_size 16, seq length 806]\tLoss: 0.010308\n","100it [00:02, 31.76it/s]Train epoch: 187 [batch #100, batch_size 16, seq length 892]\tLoss: 0.010346\n","124it [00:03, 31.40it/s]Train epoch: 187 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008247\n","148it [00:04, 32.78it/s]Train epoch: 187 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.007988\n","172it [00:05, 30.50it/s]Train epoch: 187 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.007961\n","200it [00:06, 30.76it/s]Train epoch: 187 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.006789\n","223it [00:06, 29.18it/s]Train epoch: 187 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010215\n","250it [00:07, 28.68it/s]Train epoch: 187 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.007914\n","274it [00:08, 28.57it/s]Train epoch: 187 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007439\n","298it [00:09, 28.08it/s]Train epoch: 187 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007298\n","325it [00:10, 27.52it/s]Train epoch: 187 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.008413\n","349it [00:11, 26.43it/s]Train epoch: 187 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007828\n","373it [00:12, 24.33it/s]Train epoch: 187 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.007558\n","400it [00:13, 25.58it/s]Train epoch: 187 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009105\n","424it [00:14, 25.05it/s]Train epoch: 187 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008598\n","448it [00:15, 24.31it/s]Train epoch: 187 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006908\n","475it [00:16, 21.23it/s]Train epoch: 187 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008788\n","499it [00:17, 19.90it/s]Train epoch: 187 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.009716\n","505it [00:18, 27.74it/s]\n","epoch loss: 0.008287613817356933\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 339.99it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3772, 0.5579, 0.4929, 0.5234, 0.8552\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4059, 0.6154, 0.5439, 0.5775, 0.8864\n","rec_at_5: 0.5440\n","prec_at_5: 0.5594\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 188\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 188 [batch #0, batch_size 16, seq length 212]\tLoss: 0.025612\n","25it [00:00, 37.42it/s]Train epoch: 188 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009625\n","49it [00:01, 34.81it/s]Train epoch: 188 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007050\n","73it [00:02, 34.67it/s]Train epoch: 188 [batch #75, batch_size 16, seq length 806]\tLoss: 0.009170\n","97it [00:02, 33.56it/s]Train epoch: 188 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008848\n","125it [00:03, 31.99it/s]Train epoch: 188 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008387\n","149it [00:04, 31.34it/s]Train epoch: 188 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.008029\n","173it [00:05, 31.24it/s]Train epoch: 188 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.005435\n","197it [00:05, 30.21it/s]Train epoch: 188 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008160\n","225it [00:06, 30.35it/s]Train epoch: 188 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.007792\n","250it [00:07, 29.34it/s]Train epoch: 188 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.006808\n","275it [00:08, 29.41it/s]Train epoch: 188 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009279\n","298it [00:09, 27.49it/s]Train epoch: 188 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.006900\n","324it [00:10, 27.74it/s]Train epoch: 188 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007724\n","348it [00:11, 24.94it/s]Train epoch: 188 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.008396\n","375it [00:12, 26.57it/s]Train epoch: 188 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.007166\n","399it [00:13, 24.86it/s]Train epoch: 188 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009518\n","423it [00:14, 23.98it/s]Train epoch: 188 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.006274\n","450it [00:15, 22.51it/s]Train epoch: 188 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.008408\n","474it [00:16, 22.32it/s]Train epoch: 188 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008256\n","498it [00:17, 20.72it/s]Train epoch: 188 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008543\n","505it [00:18, 27.93it/s]\n","epoch loss: 0.00822112446048327\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:06, 243.75it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3756, 0.5559, 0.4923, 0.5221, 0.8550\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4045, 0.6126, 0.5435, 0.5760, 0.8861\n","rec_at_5: 0.5432\n","prec_at_5: 0.5598\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 189\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 189 [batch #0, batch_size 16, seq length 212]\tLoss: 0.018377\n","24it [00:00, 34.38it/s]Train epoch: 189 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008363\n","48it [00:01, 34.17it/s]Train epoch: 189 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007344\n","72it [00:02, 34.20it/s]Train epoch: 189 [batch #75, batch_size 16, seq length 806]\tLoss: 0.010230\n","100it [00:03, 33.41it/s]Train epoch: 189 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008576\n","124it [00:03, 32.73it/s]Train epoch: 189 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009677\n","148it [00:04, 31.52it/s]Train epoch: 189 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.011483\n","172it [00:05, 31.31it/s]Train epoch: 189 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006640\n","200it [00:06, 31.78it/s]Train epoch: 189 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.006722\n","224it [00:06, 29.18it/s]Train epoch: 189 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010538\n","248it [00:07, 30.89it/s]Train epoch: 189 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.008246\n","274it [00:08, 28.73it/s]Train epoch: 189 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.006552\n","299it [00:09, 28.51it/s]Train epoch: 189 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007690\n","325it [00:10, 26.37it/s]Train epoch: 189 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007499\n","349it [00:11, 27.93it/s]Train epoch: 189 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007769\n","373it [00:12, 25.54it/s]Train epoch: 189 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.006217\n","400it [00:13, 24.81it/s]Train epoch: 189 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.009795\n","424it [00:14, 24.39it/s]Train epoch: 189 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.008419\n","448it [00:15, 23.68it/s]Train epoch: 189 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006834\n","475it [00:16, 21.64it/s]Train epoch: 189 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.008756\n","499it [00:17, 19.42it/s]Train epoch: 189 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008056\n","505it [00:18, 27.77it/s]\n","epoch loss: 0.008110610814159966\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 337.27it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3763, 0.5576, 0.4923, 0.5229, 0.8551\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4052, 0.6146, 0.5433, 0.5767, 0.8862\n","rec_at_5: 0.5439\n","prec_at_5: 0.5591\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 190\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 190 [batch #0, batch_size 16, seq length 212]\tLoss: 0.032073\n","25it [00:00, 34.08it/s]Train epoch: 190 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007694\n","49it [00:01, 34.43it/s]Train epoch: 190 [batch #50, batch_size 16, seq length 709]\tLoss: 0.006240\n","73it [00:02, 33.31it/s]Train epoch: 190 [batch #75, batch_size 16, seq length 806]\tLoss: 0.008401\n","97it [00:02, 33.24it/s]Train epoch: 190 [batch #100, batch_size 16, seq length 892]\tLoss: 0.009900\n","125it [00:03, 31.73it/s]Train epoch: 190 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008368\n","149it [00:04, 32.48it/s]Train epoch: 190 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.007350\n","173it [00:05, 31.22it/s]Train epoch: 190 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006619\n","200it [00:06, 29.46it/s]Train epoch: 190 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.009006\n","224it [00:07, 28.80it/s]Train epoch: 190 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.007742\n","247it [00:07, 29.26it/s]Train epoch: 190 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.006941\n","273it [00:08, 28.61it/s]Train epoch: 190 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.008671\n","299it [00:09, 28.81it/s]Train epoch: 190 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007249\n","324it [00:10, 27.96it/s]Train epoch: 190 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007673\n","348it [00:11, 27.46it/s]Train epoch: 190 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007812\n","375it [00:12, 26.55it/s]Train epoch: 190 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008426\n","399it [00:13, 24.93it/s]Train epoch: 190 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.008266\n","423it [00:14, 24.68it/s]Train epoch: 190 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.007893\n","450it [00:15, 24.19it/s]Train epoch: 190 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006314\n","474it [00:16, 22.25it/s]Train epoch: 190 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.006788\n","500it [00:17, 18.76it/s]Train epoch: 190 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.008517\n","505it [00:18, 27.85it/s]\n","epoch loss: 0.007771716574535447\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 341.90it/s]\n","Finish save rediction by checkpoint  190\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3754, 0.5585, 0.4888, 0.5213, 0.8551\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4044, 0.6163, 0.5405, 0.5759, 0.8864\n","rec_at_5: 0.5431\n","prec_at_5: 0.5591\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 191\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 191 [batch #0, batch_size 16, seq length 212]\tLoss: 0.018277\n","24it [00:00, 37.49it/s]Train epoch: 191 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007053\n","48it [00:01, 34.41it/s]Train epoch: 191 [batch #50, batch_size 16, seq length 709]\tLoss: 0.006511\n","72it [00:02, 32.64it/s]Train epoch: 191 [batch #75, batch_size 16, seq length 806]\tLoss: 0.008202\n","100it [00:02, 32.35it/s]Train epoch: 191 [batch #100, batch_size 16, seq length 892]\tLoss: 0.007900\n","124it [00:03, 33.41it/s]Train epoch: 191 [batch #125, batch_size 16, seq length 978]\tLoss: 0.009674\n","148it [00:04, 32.02it/s]Train epoch: 191 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009432\n","172it [00:05, 30.48it/s]Train epoch: 191 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.005713\n","200it [00:06, 30.14it/s]Train epoch: 191 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008102\n","224it [00:06, 29.73it/s]Train epoch: 191 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.007184\n","248it [00:07, 29.29it/s]Train epoch: 191 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.005874\n","273it [00:08, 28.51it/s]Train epoch: 191 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.007419\n","300it [00:09, 27.75it/s]Train epoch: 191 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007042\n","325it [00:10, 26.67it/s]Train epoch: 191 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.006840\n","349it [00:11, 25.67it/s]Train epoch: 191 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.006544\n","373it [00:12, 26.03it/s]Train epoch: 191 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.007142\n","400it [00:13, 25.03it/s]Train epoch: 191 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.005640\n","424it [00:14, 25.08it/s]Train epoch: 191 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.009462\n","448it [00:15, 24.07it/s]Train epoch: 191 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006651\n","475it [00:16, 22.22it/s]Train epoch: 191 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.006843\n","499it [00:17, 19.60it/s]Train epoch: 191 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007686\n","505it [00:18, 28.05it/s]\n","epoch loss: 0.007390206154752269\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 343.82it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3765, 0.5591, 0.4908, 0.5227, 0.8550\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4048, 0.6166, 0.5410, 0.5763, 0.8863\n","rec_at_5: 0.5440\n","prec_at_5: 0.5593\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 192\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 192 [batch #0, batch_size 16, seq length 212]\tLoss: 0.021610\n","22it [00:00, 37.28it/s]Train epoch: 192 [batch #25, batch_size 16, seq length 571]\tLoss: 0.007428\n","50it [00:01, 35.25it/s]Train epoch: 192 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008310\n","74it [00:02, 35.03it/s]Train epoch: 192 [batch #75, batch_size 16, seq length 806]\tLoss: 0.008233\n","98it [00:02, 32.98it/s]Train epoch: 192 [batch #100, batch_size 16, seq length 892]\tLoss: 0.006785\n","122it [00:03, 32.74it/s]Train epoch: 192 [batch #125, batch_size 16, seq length 978]\tLoss: 0.007916\n","150it [00:04, 31.54it/s]Train epoch: 192 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.008118\n","174it [00:05, 31.80it/s]Train epoch: 192 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006500\n","198it [00:05, 30.44it/s]Train epoch: 192 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.008247\n","222it [00:06, 29.93it/s]Train epoch: 192 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.010950\n","249it [00:07, 28.84it/s]Train epoch: 192 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.007226\n","273it [00:08, 27.50it/s]Train epoch: 192 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.006975\n","297it [00:09, 27.76it/s]Train epoch: 192 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.006395\n","323it [00:10, 28.22it/s]Train epoch: 192 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.006240\n","350it [00:11, 27.04it/s]Train epoch: 192 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.005825\n","374it [00:12, 26.32it/s]Train epoch: 192 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.008111\n","398it [00:13, 25.28it/s]Train epoch: 192 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.006668\n","425it [00:14, 23.49it/s]Train epoch: 192 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.006499\n","449it [00:15, 23.18it/s]Train epoch: 192 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.005871\n","473it [00:16, 22.10it/s]Train epoch: 192 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.006003\n","499it [00:17, 19.46it/s]Train epoch: 192 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.006574\n","505it [00:18, 27.89it/s]\n","epoch loss: 0.0072237789581615095\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 342.15it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3753, 0.5575, 0.4894, 0.5213, 0.8550\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4038, 0.6153, 0.5402, 0.5753, 0.8862\n","rec_at_5: 0.5428\n","prec_at_5: 0.5580\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 193\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 193 [batch #0, batch_size 16, seq length 212]\tLoss: 0.019458\n","24it [00:00, 37.77it/s]Train epoch: 193 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008265\n","48it [00:01, 34.13it/s]Train epoch: 193 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007872\n","72it [00:02, 33.58it/s]Train epoch: 193 [batch #75, batch_size 16, seq length 806]\tLoss: 0.008097\n","100it [00:02, 33.48it/s]Train epoch: 193 [batch #100, batch_size 16, seq length 892]\tLoss: 0.007860\n","124it [00:03, 31.38it/s]Train epoch: 193 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008005\n","148it [00:04, 30.94it/s]Train epoch: 193 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.008044\n","172it [00:05, 32.04it/s]Train epoch: 193 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.006522\n","199it [00:06, 29.21it/s]Train epoch: 193 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.005456\n","223it [00:06, 30.03it/s]Train epoch: 193 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008179\n","248it [00:07, 29.06it/s]Train epoch: 193 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.006965\n","272it [00:08, 28.23it/s]Train epoch: 193 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.006679\n","299it [00:09, 27.65it/s]Train epoch: 193 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.006530\n","323it [00:10, 28.75it/s]Train epoch: 193 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.006905\n","348it [00:11, 25.95it/s]Train epoch: 193 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.006740\n","375it [00:12, 25.44it/s]Train epoch: 193 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.006305\n","399it [00:13, 24.99it/s]Train epoch: 193 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.007315\n","423it [00:14, 24.47it/s]Train epoch: 193 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.007410\n","450it [00:15, 22.32it/s]Train epoch: 193 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006741\n","474it [00:16, 21.36it/s]Train epoch: 193 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.006531\n","498it [00:17, 20.26it/s]Train epoch: 193 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007585\n","505it [00:18, 27.93it/s]\n","epoch loss: 0.007420407721858084\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 343.03it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3752, 0.5576, 0.4893, 0.5212, 0.8551\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4042, 0.6160, 0.5403, 0.5757, 0.8863\n","rec_at_5: 0.5447\n","prec_at_5: 0.5603\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 194\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 194 [batch #0, batch_size 16, seq length 212]\tLoss: 0.022828\n","22it [00:00, 37.79it/s]Train epoch: 194 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009395\n","50it [00:01, 35.71it/s]Train epoch: 194 [batch #50, batch_size 16, seq length 709]\tLoss: 0.006529\n","74it [00:02, 34.33it/s]Train epoch: 194 [batch #75, batch_size 16, seq length 806]\tLoss: 0.007809\n","98it [00:02, 32.74it/s]Train epoch: 194 [batch #100, batch_size 16, seq length 892]\tLoss: 0.007982\n","122it [00:03, 32.15it/s]Train epoch: 194 [batch #125, batch_size 16, seq length 978]\tLoss: 0.006608\n","150it [00:04, 32.22it/s]Train epoch: 194 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.009029\n","174it [00:05, 30.23it/s]Train epoch: 194 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.005229\n","198it [00:06, 29.76it/s]Train epoch: 194 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.006508\n","222it [00:06, 28.39it/s]Train epoch: 194 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.006407\n","250it [00:07, 28.51it/s]Train epoch: 194 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.006853\n","274it [00:08, 27.84it/s]Train epoch: 194 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.009251\n","298it [00:09, 28.18it/s]Train epoch: 194 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.006707\n","325it [00:10, 26.73it/s]Train epoch: 194 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.005506\n","349it [00:11, 27.34it/s]Train epoch: 194 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.006577\n","373it [00:12, 25.66it/s]Train epoch: 194 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.006482\n","400it [00:13, 25.13it/s]Train epoch: 194 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.006326\n","424it [00:14, 24.87it/s]Train epoch: 194 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.006603\n","448it [00:15, 23.49it/s]Train epoch: 194 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006182\n","475it [00:16, 22.65it/s]Train epoch: 194 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.007441\n","499it [00:17, 19.86it/s]Train epoch: 194 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007530\n","505it [00:18, 27.92it/s]\n","epoch loss: 0.007096656195912741\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 343.17it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3759, 0.5577, 0.4916, 0.5226, 0.8549\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4037, 0.6139, 0.5410, 0.5752, 0.8859\n","rec_at_5: 0.5411\n","prec_at_5: 0.5570\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 195\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 195 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020507\n","22it [00:00, 37.17it/s]Train epoch: 195 [batch #25, batch_size 16, seq length 571]\tLoss: 0.008039\n","50it [00:01, 36.71it/s]Train epoch: 195 [batch #50, batch_size 16, seq length 709]\tLoss: 0.007014\n","74it [00:02, 34.39it/s]Train epoch: 195 [batch #75, batch_size 16, seq length 806]\tLoss: 0.007680\n","98it [00:02, 33.15it/s]Train epoch: 195 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008839\n","122it [00:03, 32.45it/s]Train epoch: 195 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008131\n","150it [00:04, 32.36it/s]Train epoch: 195 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.005934\n","174it [00:05, 31.85it/s]Train epoch: 195 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.005894\n","198it [00:05, 29.22it/s]Train epoch: 195 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.006293\n","224it [00:06, 29.29it/s]Train epoch: 195 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.007319\n","250it [00:07, 27.09it/s]Train epoch: 195 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.006694\n","274it [00:08, 28.05it/s]Train epoch: 195 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.005814\n","300it [00:09, 25.93it/s]Train epoch: 195 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.006437\n","323it [00:10, 28.18it/s]Train epoch: 195 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.007371\n","350it [00:11, 25.67it/s]Train epoch: 195 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007683\n","374it [00:12, 24.35it/s]Train epoch: 195 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.007255\n","398it [00:13, 25.62it/s]Train epoch: 195 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.005641\n","425it [00:14, 24.54it/s]Train epoch: 195 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.007411\n","449it [00:15, 23.31it/s]Train epoch: 195 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006595\n","473it [00:16, 21.94it/s]Train epoch: 195 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.006748\n","500it [00:17, 19.99it/s]Train epoch: 195 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.006229\n","505it [00:18, 27.66it/s]\n","epoch loss: 0.006834246637486959\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 343.23it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3757, 0.5580, 0.4904, 0.5220, 0.8547\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4045, 0.6140, 0.5424, 0.5760, 0.8859\n","rec_at_5: 0.5411\n","prec_at_5: 0.5568\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 196\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 196 [batch #0, batch_size 16, seq length 212]\tLoss: 0.019612\n","24it [00:00, 36.65it/s]Train epoch: 196 [batch #25, batch_size 16, seq length 571]\tLoss: 0.009162\n","48it [00:01, 36.27it/s]Train epoch: 196 [batch #50, batch_size 16, seq length 709]\tLoss: 0.006363\n","72it [00:02, 33.55it/s]Train epoch: 196 [batch #75, batch_size 16, seq length 806]\tLoss: 0.009655\n","100it [00:02, 34.13it/s]Train epoch: 196 [batch #100, batch_size 16, seq length 892]\tLoss: 0.007817\n","124it [00:03, 33.19it/s]Train epoch: 196 [batch #125, batch_size 16, seq length 978]\tLoss: 0.008259\n","148it [00:04, 32.96it/s]Train epoch: 196 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.007116\n","172it [00:05, 30.50it/s]Train epoch: 196 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.005246\n","200it [00:06, 29.90it/s]Train epoch: 196 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.005265\n","225it [00:06, 29.53it/s]Train epoch: 196 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.007061\n","250it [00:07, 29.43it/s]Train epoch: 196 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.006298\n","274it [00:08, 28.84it/s]Train epoch: 196 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.005976\n","297it [00:09, 28.54it/s]Train epoch: 196 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.008076\n","323it [00:10, 27.41it/s]Train epoch: 196 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.006025\n","348it [00:11, 26.89it/s]Train epoch: 196 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.006888\n","375it [00:12, 25.49it/s]Train epoch: 196 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.005670\n","399it [00:13, 25.35it/s]Train epoch: 196 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.005684\n","423it [00:14, 23.81it/s]Train epoch: 196 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.006788\n","450it [00:15, 22.46it/s]Train epoch: 196 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.005020\n","474it [00:16, 20.58it/s]Train epoch: 196 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.006385\n","498it [00:17, 19.88it/s]Train epoch: 196 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.006498\n","505it [00:18, 27.92it/s]\n","epoch loss: 0.006760462304193407\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 334.51it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3756, 0.5596, 0.4892, 0.5221, 0.8547\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4047, 0.6151, 0.5420, 0.5762, 0.8861\n","rec_at_5: 0.5412\n","prec_at_5: 0.5570\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 197\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 197 [batch #0, batch_size 16, seq length 212]\tLoss: 0.017428\n","25it [00:00, 37.38it/s]Train epoch: 197 [batch #25, batch_size 16, seq length 571]\tLoss: 0.006196\n","49it [00:01, 35.81it/s]Train epoch: 197 [batch #50, batch_size 16, seq length 709]\tLoss: 0.004898\n","73it [00:02, 33.93it/s]Train epoch: 197 [batch #75, batch_size 16, seq length 806]\tLoss: 0.007287\n","97it [00:02, 33.51it/s]Train epoch: 197 [batch #100, batch_size 16, seq length 892]\tLoss: 0.008140\n","125it [00:03, 31.87it/s]Train epoch: 197 [batch #125, batch_size 16, seq length 978]\tLoss: 0.007792\n","149it [00:04, 32.28it/s]Train epoch: 197 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.006076\n","173it [00:05, 31.22it/s]Train epoch: 197 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.005301\n","197it [00:05, 30.58it/s]Train epoch: 197 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.005406\n","224it [00:06, 28.90it/s]Train epoch: 197 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.007996\n","248it [00:07, 28.69it/s]Train epoch: 197 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.007263\n","272it [00:08, 28.55it/s]Train epoch: 197 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.004646\n","298it [00:09, 27.54it/s]Train epoch: 197 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007412\n","323it [00:10, 27.87it/s]Train epoch: 197 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.004608\n","350it [00:11, 26.41it/s]Train epoch: 197 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.007365\n","374it [00:12, 25.24it/s]Train epoch: 197 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.004714\n","400it [00:14, 13.72it/s]Train epoch: 197 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.005319\n","424it [00:15, 12.88it/s]Train epoch: 197 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.005429\n","450it [00:17, 15.22it/s]Train epoch: 197 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.005792\n","475it [00:18, 20.42it/s]Train epoch: 197 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.007121\n","499it [00:20, 19.28it/s]Train epoch: 197 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.007543\n","505it [00:20, 24.68it/s]\n","epoch loss: 0.006493826940774429\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 344.93it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3747, 0.5565, 0.4897, 0.5210, 0.8548\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4041, 0.6158, 0.5403, 0.5756, 0.8859\n","rec_at_5: 0.5418\n","prec_at_5: 0.5587\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 198\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 198 [batch #0, batch_size 16, seq length 212]\tLoss: 0.024888\n","24it [00:00, 35.84it/s]Train epoch: 198 [batch #25, batch_size 16, seq length 571]\tLoss: 0.006206\n","48it [00:01, 35.27it/s]Train epoch: 198 [batch #50, batch_size 16, seq length 709]\tLoss: 0.008476\n","72it [00:02, 34.04it/s]Train epoch: 198 [batch #75, batch_size 16, seq length 806]\tLoss: 0.006296\n","100it [00:02, 31.84it/s]Train epoch: 198 [batch #100, batch_size 16, seq length 892]\tLoss: 0.006364\n","124it [00:03, 32.31it/s]Train epoch: 198 [batch #125, batch_size 16, seq length 978]\tLoss: 0.006478\n","148it [00:04, 32.03it/s]Train epoch: 198 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.005391\n","172it [00:05, 32.09it/s]Train epoch: 198 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.005552\n","200it [00:06, 30.50it/s]Train epoch: 198 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.004995\n","225it [00:06, 28.76it/s]Train epoch: 198 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008160\n","250it [00:07, 28.89it/s]Train epoch: 198 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.007655\n","275it [00:08, 29.64it/s]Train epoch: 198 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.005923\n","299it [00:09, 28.89it/s]Train epoch: 198 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.007788\n","323it [00:10, 27.48it/s]Train epoch: 198 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.005159\n","350it [00:11, 17.74it/s]Train epoch: 198 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.004911\n","374it [00:13, 16.24it/s]Train epoch: 198 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.006049\n","400it [00:14, 23.16it/s]Train epoch: 198 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.006626\n","424it [00:15, 23.96it/s]Train epoch: 198 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.005971\n","448it [00:16, 23.08it/s]Train epoch: 198 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006420\n","475it [00:17, 22.07it/s]Train epoch: 198 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.006205\n","499it [00:18, 19.79it/s]Train epoch: 198 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.005210\n","505it [00:19, 26.37it/s]\n","epoch loss: 0.006410963439018383\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 342.01it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3771, 0.5563, 0.4945, 0.5236, 0.8548\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4049, 0.6134, 0.5437, 0.5764, 0.8859\n","rec_at_5: 0.5426\n","prec_at_5: 0.5586\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","EPOCH 199\n","0it [00:00, ?it/s]/content/drive/My Drive/MIMIC/caml-mimic/datasets.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  np.array(self.descs)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 199 [batch #0, batch_size 16, seq length 212]\tLoss: 0.020233\n","22it [00:00, 37.29it/s]Train epoch: 199 [batch #25, batch_size 16, seq length 571]\tLoss: 0.006414\n","50it [00:01, 35.45it/s]Train epoch: 199 [batch #50, batch_size 16, seq length 709]\tLoss: 0.006132\n","74it [00:02, 33.90it/s]Train epoch: 199 [batch #75, batch_size 16, seq length 806]\tLoss: 0.008142\n","98it [00:02, 33.52it/s]Train epoch: 199 [batch #100, batch_size 16, seq length 892]\tLoss: 0.006160\n","122it [00:03, 31.89it/s]Train epoch: 199 [batch #125, batch_size 16, seq length 978]\tLoss: 0.005044\n","150it [00:04, 31.65it/s]Train epoch: 199 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.007417\n","174it [00:05, 30.67it/s]Train epoch: 199 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.004678\n","198it [00:05, 29.52it/s]Train epoch: 199 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.007293\n","224it [00:06, 30.71it/s]Train epoch: 199 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.008856\n","249it [00:07, 29.25it/s]Train epoch: 199 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.006337\n","273it [00:08, 29.25it/s]Train epoch: 199 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.004878\n","299it [00:09, 28.01it/s]Train epoch: 199 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.006328\n","323it [00:10, 27.56it/s]Train epoch: 199 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.005806\n","350it [00:11, 26.18it/s]Train epoch: 199 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.004298\n","374it [00:12, 26.46it/s]Train epoch: 199 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.005535\n","398it [00:13, 24.35it/s]Train epoch: 199 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.006412\n","425it [00:14, 24.64it/s]Train epoch: 199 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.006537\n","449it [00:15, 22.59it/s]Train epoch: 199 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.006870\n","473it [00:16, 22.18it/s]Train epoch: 199 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.007248\n","500it [00:17, 19.89it/s]Train epoch: 199 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.005441\n","505it [00:18, 27.83it/s]\n","epoch loss: 0.006406547937549116\n","last epoch: testing on test and train sets\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1573it [00:04, 341.95it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3767, 0.5582, 0.4913, 0.5227, 0.8548\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4059, 0.6163, 0.5431, 0.5774, 0.8863\n","rec_at_5: 0.5436\n","prec_at_5: 0.5593\n","\n","\n","\n","evaluating on test\n","file for evaluation: ./mimicdata/mimic3/test_50.csv\n","0it [00:00, ?it/s]./learn/training.py:248: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","1729it [00:05, 301.33it/s]\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.3765, 0.5619, 0.4925, 0.5249, 0.8526\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.4035, 0.6175, 0.5378, 0.5749, 0.8840\n","rec_at_5: 0.5297\n","prec_at_5: 0.5610\n","\n","saved metrics, params, model to directory /content/drive/My Drive/MIMIC/caml-mimic/saved_models/conv_attn_Aug_22_04:20:53\n","\n","TOTAL ELAPSED TIME FOR conv_attn MODEL AND 200 EPOCHS: 4754.259675\n"]}],"source":["# Train by DRCaml\n","! python ./learn/training.py ./mimicdata/mimic3/train_50.csv ./mimicdata/mimic3/vocab.csv 50 conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --lmbda 10 --embed-file ./mimicdata/mimic3/processed_full.embed --gpu\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21997,"status":"ok","timestamp":1661183032773,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"ezvKXcBvO-oW","outputId":"3c238220-6da2-492c-dabb-dfc29f79620b"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading lookups...\n","loading pretrained embeddings...\n","adding unk embedding\n","/content/drive/My Drive/MIMIC/caml-mimic/learn/models.py:135: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  xavier_uniform(self.conv.weight)\n","/content/drive/My Drive/MIMIC/caml-mimic/learn/models.py:139: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  xavier_uniform(self.U.weight)\n","/content/drive/My Drive/MIMIC/caml-mimic/learn/models.py:143: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  xavier_uniform(self.final.weight)\n","ConvAttnPool(\n","  (embed_drop): Dropout(p=0.2, inplace=False)\n","  (embed): Embedding(51919, 100, padding_idx=0)\n","  (conv): Conv1d(100, 50, kernel_size=(10,), stride=(1,), padding=(5,))\n","  (U): Linear(in_features=50, out_features=50, bias=True)\n","  (final): Linear(in_features=50, out_features=50, bias=True)\n",")\n","EPOCH 0\n","0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n","Train epoch: 0 [batch #0, batch_size 16, seq length 212]\tLoss: 0.687521\n","17it [00:01, 14.96it/s]Train epoch: 0 [batch #25, batch_size 16, seq length 571]\tLoss: 0.678974\n","42it [00:01, 40.37it/s]Train epoch: 0 [batch #50, batch_size 16, seq length 709]\tLoss: 0.668249\n","66it [00:01, 64.65it/s]Train epoch: 0 [batch #75, batch_size 16, seq length 806]\tLoss: 0.653184\n","100it [00:02, 85.18it/s]Train epoch: 0 [batch #100, batch_size 16, seq length 892]\tLoss: 0.618988\n","122it [00:02, 90.37it/s]Train epoch: 0 [batch #125, batch_size 16, seq length 978]\tLoss: 0.577690\n","142it [00:02, 89.22it/s]Train epoch: 0 [batch #150, batch_size 16, seq length 1063]\tLoss: 0.528528\n","172it [00:03, 87.75it/s]Train epoch: 0 [batch #175, batch_size 16, seq length 1137]\tLoss: 0.469366\n","199it [00:03, 83.66it/s]Train epoch: 0 [batch #200, batch_size 16, seq length 1217]\tLoss: 0.430781\n","225it [00:03, 79.07it/s]Train epoch: 0 [batch #225, batch_size 16, seq length 1299]\tLoss: 0.409238\n","249it [00:04, 73.95it/s]Train epoch: 0 [batch #250, batch_size 16, seq length 1383]\tLoss: 0.388907\n","272it [00:04, 67.51it/s]Train epoch: 0 [batch #275, batch_size 16, seq length 1470]\tLoss: 0.388667\n","300it [00:04, 64.34it/s]Train epoch: 0 [batch #300, batch_size 16, seq length 1563]\tLoss: 0.384206\n","321it [00:05, 64.73it/s]Train epoch: 0 [batch #325, batch_size 16, seq length 1660]\tLoss: 0.369148\n","349it [00:05, 58.47it/s]Train epoch: 0 [batch #350, batch_size 16, seq length 1774]\tLoss: 0.379453\n","373it [00:06, 55.56it/s]Train epoch: 0 [batch #375, batch_size 16, seq length 1893]\tLoss: 0.372787\n","397it [00:06, 51.38it/s]Train epoch: 0 [batch #400, batch_size 16, seq length 2032]\tLoss: 0.376491\n","421it [00:07, 51.01it/s]Train epoch: 0 [batch #425, batch_size 16, seq length 2231]\tLoss: 0.376187\n","448it [00:07, 46.85it/s]Train epoch: 0 [batch #450, batch_size 16, seq length 2491]\tLoss: 0.383955\n","473it [00:08, 43.16it/s]Train epoch: 0 [batch #475, batch_size 16, seq length 2500]\tLoss: 0.402156\n","500it [00:09, 35.02it/s]Train epoch: 0 [batch #500, batch_size 16, seq length 2500]\tLoss: 0.404198\n","505it [00:09, 54.70it/s]\n","epoch loss: 0.4686170492431905\n","file for evaluation: ./mimicdata/mimic3/dev_50.csv\n","0it [00:00, ?it/s]./learn/training.py:249: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(torch.LongTensor(data), volatile=True), Variable(torch.FloatTensor(target))\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","hadm: [111912]\n","targets: [array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","        0., 1.]], dtype=float32)]\n","1573it [00:03, 480.05it/s]\n","before [array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","        0., 1.]], dtype=float32), array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","        1., 0.]], dtype=float32)]\n","after [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n","  1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n","  0. 1.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n","  1. 0.]]\n","/content/drive/My Drive/MIMIC/caml-mimic/evaluation.py:155: RuntimeWarning: invalid value encountered in double_scalars\n","  return intersect_size(yhatmic, ymic, 0) / yhatmic.sum(axis=0)\n","\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.0000, 0.0000, 0.0000, 0.0000, 0.5487\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.0000, nan, 0.0000, nan, 0.6297\n","rec_at_5: 0.2353\n","prec_at_5: 0.2659\n","\n","TOTAL ELAPSED TIME FOR conv_attn MODEL AND 1 EPOCHS: 19.817455\n"]}],"source":["!python ./learn/training.py ./mimicdata/mimic3/train_50.csv ./mimicdata/mimic3/vocab.csv 50 conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_8 --lr 0.0001 --embed-file ./mimicdata/mimic3/processed_full.embed --gpu\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3812,"status":"ok","timestamp":1660986612000,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"3wpkp6HaZ0LL","outputId":"aaaffc22-ddfc-4bce-ea13-41316168c4ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading lookups...\n","/content/drive/My Drive/MIMIC/caml-mimic/learn/models.py:135: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  xavier_uniform(self.conv.weight)\n","/content/drive/My Drive/MIMIC/caml-mimic/learn/models.py:139: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  xavier_uniform(self.U.weight)\n","/content/drive/My Drive/MIMIC/caml-mimic/learn/models.py:143: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","  xavier_uniform(self.final.weight)\n","Traceback (most recent call last):\n","  File \"./learn/training.py\", line 363, in <module>\n","    main(args)\n","  File \"./learn/training.py\", line 30, in main\n","    args, model, optimizer, params, dicts = init(args)\n","  File \"./learn/training.py\", line 46, in init\n","    model = tools.pick_model(args, dicts)\n","  File \"/content/drive/My Drive/MIMIC/caml-mimic/learn/tools.py\", line 38, in pick_model\n","    model.load_state_dict(sd)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1605, in load_state_dict\n","    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n","RuntimeError: Error(s) in loading state_dict for ConvAttnPool:\n","\tUnexpected key(s) in state_dict: \"desc_embedding.weight\", \"label_conv.weight\", \"label_conv.bias\", \"label_fc1.weight\", \"label_fc1.bias\". \n","\tsize mismatch for U.weight: copying a param with shape torch.Size([8921, 50]) from checkpoint, the shape in current model is torch.Size([50, 50]).\n","\tsize mismatch for U.bias: copying a param with shape torch.Size([8921]) from checkpoint, the shape in current model is torch.Size([50]).\n","\tsize mismatch for final.weight: copying a param with shape torch.Size([8921, 50]) from checkpoint, the shape in current model is torch.Size([50, 50]).\n","\tsize mismatch for final.bias: copying a param with shape torch.Size([8921]) from checkpoint, the shape in current model is torch.Size([50]).\n"]}],"source":["# Test\n","#!python ./learn/training.py ./mimicdata/mimic3/train_full.csv ./mimicdata/mimic3/vocab.csv 50 conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --lr 0.0001 --public-model --test-model ./saved_models/conv_attn_Jul_26_09:47:49/model_best_prec_at_8.pth --gpu\n","#! python ./learn/training.py ./mimicdata/mimic3/train_50.csv ./mimicdata/mimic3/vocab.csv 50 conv_attn 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --lr 0.0001 --public-model --test-model ./predictions/DRCAML_mimic3_50/model.pth --gpu\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30127,"status":"ok","timestamp":1660963572120,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"lYkpZHhuIvkn","outputId":"1d088128-2afc-4869-d56d-7f28bf298eda"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading predictions\n","loading ground truth\n","reformatting predictions\n","3372it [00:08, 392.18it/s]\n","evaluating code-type metrics\n","3367it [00:03, 942.01it/s]\n","3367it [00:00, 3419.13it/s]\n","[BY CODE TYPE] f1-diag f1-proc\n","0.5239 0.6087\n","evaluating all other metrics\n","\n","[MACRO] accuracy, precision, recall, f-measure\n","0.0608, 0.0913, 0.0856, 0.0884\n","[MICRO] accuracy, precision, recall, f-measure\n","0.3686, 0.6070, 0.4842, 0.5387\n","\n"]}],"source":["# ! python get_metrics_for_saved_predictions.py predictions/CAML_mimic3_full"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":577,"status":"ok","timestamp":1660970282426,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"ytNsfH_iIvua","outputId":"8d432ab6-47b1-4e1d-f6ee-a9494a061d5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.4160341e-08 1.2085459e-07 3.0057940e-08 ... 3.5258786e-08\n","  6.8668072e-07 4.1092875e-08]\n"," [1.7712557e-07 4.2215268e-07 3.8929187e-08 ... 2.0615595e-08\n","  1.0496751e-06 2.6602395e-08]\n"," [1.0357180e-08 1.4444414e-06 1.8080565e-08 ... 8.3759417e-09\n","  8.8142690e-07 5.5539513e-08]\n"," ...\n"," [1.6645089e-07 7.1244263e-05 2.3126906e-06 ... 2.6773739e-07\n","  2.0262627e-05 1.6298941e-07]\n"," [3.3105960e-07 1.3189461e-05 1.6705425e-06 ... 3.9362990e-06\n","  9.5020132e-06 1.3684920e-07]\n"," [4.1516237e-06 6.9593749e-05 2.2052951e-05 ... 3.6465002e-07\n","  1.7429253e-05 1.6612769e-07]]\n","(3372, 8921)\n"]}],"source":["import numpy as np\n","path = \"/content/drive/My Drive/MIMIC/caml-mimic/predictions/CAML_mimic3_50/\"\n","\n","with open(path + 'test_full_outputs.npy', 'rb') as f:  \n","  outputs = np.load(f)\n","  print(outputs)\n","  print(outputs.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":542,"status":"ok","timestamp":1660916595125,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"P5CDW0MRZHhT","outputId":"90524357-0303-44d9-95eb-aefd70bb0c1d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/content/drive/MyDrive/MIMIC/caml-mimic/evaluation.py:125: RuntimeWarning: invalid value encountered in double_scalars\n","  vals.append(num_true_in_top_k / float(denom))\n"]}],"source":["import sys\n","import numpy as np\n","sys.path.append(\"/content/drive/My Drive/MIMIC/caml-mimic\")\n","import evaluation\n","yhat = np.array([[1.0,1.0,0.0],\n","                 [0.0,0.0,1.0],\n","                 [1.0,1.0,1.0]])\n","y = np.array([[1.0,1.0,1.0],\n","              [0.0,0.0,0.0],\n","              [0.0,1.0,1.0]])\n","yhat_raw= np.array([[0.5,0.5,0.1],\n","                    [0.09,0.08,0.9],\n","                    [0.9,0.8,0.7]])\n","metrics = evaluation.all_metrics(yhat, y, k=5, yhat_raw=yhat_raw)\n","#evaluation.print_metrics(metrics)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1660916607691,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"hTjnsRN2UeHD","outputId":"af5c6cdf-113a-4e01-8f2a-eb56261db1fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","[MACRO] accuracy, precision, recall, f-measure, AUC\n","0.6111, 0.6667, 0.8333, 0.7407, 0.5000\n","[MICRO] accuracy, precision, recall, f-measure, AUC\n","0.5714, 0.6667, 0.8000, 0.7273, 0.5000\n","rec_at_5: 0.6667\n","prec_at_5: 0.5556\n","\n"]}],"source":["evaluation.print_metrics(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wP3JeGgUeJY"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAAGJMMXUeL3"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"MwYOI0aGZIC5"},"source":["# Explore data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67rDX4pzHZIO"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-oGoLtK06OT"},"outputs":[],"source":["train_file_path = './mimicdata/mimic3/train_full.csv'\n","train_file = pd.read_csv(train_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"elapsed":477,"status":"ok","timestamp":1659014234188,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"ln9cj2n9KKpb","outputId":"d9c134fd-6249-4c9a-9b73-9dba1d4df7ce"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-57c352bb-967a-4425-916a-4e4cbde1f97c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>SUBJECT_ID</th>\n","      <th>HADM_ID</th>\n","      <th>TEXT</th>\n","      <th>LABELS</th>\n","      <th>length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>158</td>\n","      <td>169433</td>\n","      <td>admission date discharge date date of birth se...</td>\n","      <td>532.40;493.20;V45.81;412;401.9;44.43</td>\n","      <td>51</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2896</td>\n","      <td>178124</td>\n","      <td>name known lastname known firstname unit no nu...</td>\n","      <td>211.3;427.31;578.9;560.1;496;584.9;428.0;276.5...</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6495</td>\n","      <td>139808</td>\n","      <td>admission date discharge date date of birth se...</td>\n","      <td>998.59;998.32;905.4;E929.0;041.85;86.22;86.69;...</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3564</td>\n","      <td>117638</td>\n","      <td>admission date discharge date service doctor l...</td>\n","      <td>038.49;041.6;785.59;518.81;507.0;592.1;591;276...</td>\n","      <td>68</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7995</td>\n","      <td>190945</td>\n","      <td>admission date discharge date date of birth se...</td>\n","      <td>440.22;492.8;401.9;714.0;39.29;88.48</td>\n","      <td>74</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57c352bb-967a-4425-916a-4e4cbde1f97c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-57c352bb-967a-4425-916a-4e4cbde1f97c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-57c352bb-967a-4425-916a-4e4cbde1f97c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   SUBJECT_ID  HADM_ID                                               TEXT  \\\n","0         158   169433  admission date discharge date date of birth se...   \n","1        2896   178124  name known lastname known firstname unit no nu...   \n","2        6495   139808  admission date discharge date date of birth se...   \n","3        3564   117638  admission date discharge date service doctor l...   \n","4        7995   190945  admission date discharge date date of birth se...   \n","\n","                                              LABELS  length  \n","0               532.40;493.20;V45.81;412;401.9;44.43      51  \n","1  211.3;427.31;578.9;560.1;496;584.9;428.0;276.5...      55  \n","2  998.59;998.32;905.4;E929.0;041.85;86.22;86.69;...      60  \n","3  038.49;041.6;785.59;518.81;507.0;592.1;591;276...      68  \n","4               440.22;492.8;401.9;714.0;39.29;88.48      74  "]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["train_file[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112311,"status":"ok","timestamp":1659001734777,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"qZP8Zu0Mhtf-","outputId":"4024125f-27b7-4e16-e83e-1c737e40833b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]}],"source":["noteevents_file_path = './mimicdata/mimic3/NOTEEVENTS.csv'\n","noteevents_file = pd.read_csv(noteevents_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":824},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1659001734778,"user":{"displayName":"CUI TENGFEI","userId":"12310385898916684610"},"user_tz":-480},"id":"gD0Ht-iTJ5O0","outputId":"266eb8e3-01c6-4676-ef5e-61f367a65d90"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-69fb8f75-5ba9-4604-b37d-bc70458c647d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ROW_ID</th>\n","      <th>SUBJECT_ID</th>\n","      <th>HADM_ID</th>\n","      <th>CHARTDATE</th>\n","      <th>CHARTTIME</th>\n","      <th>STORETIME</th>\n","      <th>CATEGORY</th>\n","      <th>DESCRIPTION</th>\n","      <th>CGID</th>\n","      <th>ISERROR</th>\n","      <th>TEXT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2083165</th>\n","      <td>2079745</td>\n","      <td>32242</td>\n","      <td>163974.0</td>\n","      <td>2121-11-14</td>\n","      <td>2121-11-14 15:31:00</td>\n","      <td>2121-11-14 15:32:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>19412.0</td>\n","      <td>NaN</td>\n","      <td>Respiratory Care\\nPt cont on prong CPAP. Weane...</td>\n","    </tr>\n","    <tr>\n","      <th>2083166</th>\n","      <td>2079746</td>\n","      <td>32242</td>\n","      <td>163974.0</td>\n","      <td>2121-11-14</td>\n","      <td>2121-11-14 20:21:00</td>\n","      <td>2121-11-14 20:28:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>18706.0</td>\n","      <td>NaN</td>\n","      <td>Neonatology NNP note\\nProcedure note: PICC lin...</td>\n","    </tr>\n","    <tr>\n","      <th>2083167</th>\n","      <td>2075879</td>\n","      <td>31668</td>\n","      <td>134163.0</td>\n","      <td>2122-09-10</td>\n","      <td>2122-09-10 13:25:00</td>\n","      <td>2122-09-10 13:41:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>21232.0</td>\n","      <td>NaN</td>\n","      <td>NPN 7A-7P\\n\\n\\n#2 After discussion with team a...</td>\n","    </tr>\n","    <tr>\n","      <th>2083168</th>\n","      <td>2075880</td>\n","      <td>31668</td>\n","      <td>134163.0</td>\n","      <td>2122-09-11</td>\n","      <td>2122-09-11 00:13:00</td>\n","      <td>2122-09-11 00:18:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>15938.0</td>\n","      <td>NaN</td>\n","      <td>[** 63**] PHysical Exam\\nPE: pink, AFOF, nasal...</td>\n","    </tr>\n","    <tr>\n","      <th>2083169</th>\n","      <td>2075881</td>\n","      <td>31668</td>\n","      <td>134163.0</td>\n","      <td>2122-09-11</td>\n","      <td>2122-09-11 03:33:00</td>\n","      <td>2122-09-11 03:42:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>15484.0</td>\n","      <td>NaN</td>\n","      <td>NPN\\n\\n\\n#2F/N O-Infant remains on demand feed...</td>\n","    </tr>\n","    <tr>\n","      <th>2083170</th>\n","      <td>2075882</td>\n","      <td>31668</td>\n","      <td>134163.0</td>\n","      <td>2122-09-11</td>\n","      <td>2122-09-11 12:47:00</td>\n","      <td>2122-09-11 12:49:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>20836.0</td>\n","      <td>NaN</td>\n","      <td>SOCIAL WORK\\nContinuing to follow this family ...</td>\n","    </tr>\n","    <tr>\n","      <th>2083171</th>\n","      <td>2075883</td>\n","      <td>31668</td>\n","      <td>134163.0</td>\n","      <td>2122-09-11</td>\n","      <td>2122-09-11 12:47:00</td>\n","      <td>2122-09-11 12:51:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>21222.0</td>\n","      <td>NaN</td>\n","      <td>Attending Note\\nDay of life 76 PMA 39 [**12-7*...</td>\n","    </tr>\n","    <tr>\n","      <th>2083172</th>\n","      <td>2077115</td>\n","      <td>31767</td>\n","      <td>105243.0</td>\n","      <td>2141-01-05</td>\n","      <td>2141-01-05 14:24:00</td>\n","      <td>2141-01-05 15:01:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>20700.0</td>\n","      <td>NaN</td>\n","      <td>NICU Attending Admission Note\\nPt is a 34 [**6...</td>\n","    </tr>\n","    <tr>\n","      <th>2083173</th>\n","      <td>2077116</td>\n","      <td>31767</td>\n","      <td>105243.0</td>\n","      <td>2141-01-05</td>\n","      <td>2141-01-05 16:28:00</td>\n","      <td>2141-01-05 16:54:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>20634.0</td>\n","      <td>NaN</td>\n","      <td>Admission Note\\n\\n\\nBaby boy [**Known lastname...</td>\n","    </tr>\n","    <tr>\n","      <th>2083174</th>\n","      <td>2070656</td>\n","      <td>31097</td>\n","      <td>115637.0</td>\n","      <td>2132-01-20</td>\n","      <td>2132-01-20 14:00:00</td>\n","      <td>2132-01-20 14:13:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>20634.0</td>\n","      <td>NaN</td>\n","      <td>0700-[**2054**] NPN\\n\\n\\nRESP: Infant is in RA...</td>\n","    </tr>\n","    <tr>\n","      <th>2083175</th>\n","      <td>2070657</td>\n","      <td>31097</td>\n","      <td>115637.0</td>\n","      <td>2132-01-21</td>\n","      <td>2132-01-21 03:27:00</td>\n","      <td>2132-01-21 03:38:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>17581.0</td>\n","      <td>NaN</td>\n","      <td>NPN\\n\\n\\n#1  Infant remains in RA with O2 sats...</td>\n","    </tr>\n","    <tr>\n","      <th>2083176</th>\n","      <td>2070658</td>\n","      <td>31097</td>\n","      <td>115637.0</td>\n","      <td>2132-01-21</td>\n","      <td>2132-01-21 09:50:00</td>\n","      <td>2132-01-21 09:53:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>19211.0</td>\n","      <td>NaN</td>\n","      <td>Neonatology\\nDOL #5, CGA 36 weeks.\\n\\nCVR: Con...</td>\n","    </tr>\n","    <tr>\n","      <th>2083177</th>\n","      <td>2070659</td>\n","      <td>31097</td>\n","      <td>115637.0</td>\n","      <td>2132-01-21</td>\n","      <td>2132-01-21 16:42:00</td>\n","      <td>2132-01-21 16:44:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>20104.0</td>\n","      <td>NaN</td>\n","      <td>Family Meeting Note\\nFamily meeting held with ...</td>\n","    </tr>\n","    <tr>\n","      <th>2083178</th>\n","      <td>2070660</td>\n","      <td>31097</td>\n","      <td>115637.0</td>\n","      <td>2132-01-21</td>\n","      <td>2132-01-21 18:05:00</td>\n","      <td>2132-01-21 18:16:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>16023.0</td>\n","      <td>NaN</td>\n","      <td>NPN 1800\\n\\n\\n#1 Resp: [**Known lastname 2243*...</td>\n","    </tr>\n","    <tr>\n","      <th>2083179</th>\n","      <td>2070661</td>\n","      <td>31097</td>\n","      <td>115637.0</td>\n","      <td>2132-01-21</td>\n","      <td>2132-01-21 18:05:00</td>\n","      <td>2132-01-21 18:31:00</td>\n","      <td>Nursing/other</td>\n","      <td>Report</td>\n","      <td>16023.0</td>\n","      <td>NaN</td>\n","      <td>NPN 1800\\nNursing Addendum:\\n[**Known lastname...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69fb8f75-5ba9-4604-b37d-bc70458c647d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-69fb8f75-5ba9-4604-b37d-bc70458c647d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-69fb8f75-5ba9-4604-b37d-bc70458c647d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n","2083165  2079745       32242  163974.0  2121-11-14  2121-11-14 15:31:00   \n","2083166  2079746       32242  163974.0  2121-11-14  2121-11-14 20:21:00   \n","2083167  2075879       31668  134163.0  2122-09-10  2122-09-10 13:25:00   \n","2083168  2075880       31668  134163.0  2122-09-11  2122-09-11 00:13:00   \n","2083169  2075881       31668  134163.0  2122-09-11  2122-09-11 03:33:00   \n","2083170  2075882       31668  134163.0  2122-09-11  2122-09-11 12:47:00   \n","2083171  2075883       31668  134163.0  2122-09-11  2122-09-11 12:47:00   \n","2083172  2077115       31767  105243.0  2141-01-05  2141-01-05 14:24:00   \n","2083173  2077116       31767  105243.0  2141-01-05  2141-01-05 16:28:00   \n","2083174  2070656       31097  115637.0  2132-01-20  2132-01-20 14:00:00   \n","2083175  2070657       31097  115637.0  2132-01-21  2132-01-21 03:27:00   \n","2083176  2070658       31097  115637.0  2132-01-21  2132-01-21 09:50:00   \n","2083177  2070659       31097  115637.0  2132-01-21  2132-01-21 16:42:00   \n","2083178  2070660       31097  115637.0  2132-01-21  2132-01-21 18:05:00   \n","2083179  2070661       31097  115637.0  2132-01-21  2132-01-21 18:05:00   \n","\n","                   STORETIME       CATEGORY DESCRIPTION     CGID  ISERROR  \\\n","2083165  2121-11-14 15:32:00  Nursing/other      Report  19412.0      NaN   \n","2083166  2121-11-14 20:28:00  Nursing/other      Report  18706.0      NaN   \n","2083167  2122-09-10 13:41:00  Nursing/other      Report  21232.0      NaN   \n","2083168  2122-09-11 00:18:00  Nursing/other      Report  15938.0      NaN   \n","2083169  2122-09-11 03:42:00  Nursing/other      Report  15484.0      NaN   \n","2083170  2122-09-11 12:49:00  Nursing/other      Report  20836.0      NaN   \n","2083171  2122-09-11 12:51:00  Nursing/other      Report  21222.0      NaN   \n","2083172  2141-01-05 15:01:00  Nursing/other      Report  20700.0      NaN   \n","2083173  2141-01-05 16:54:00  Nursing/other      Report  20634.0      NaN   \n","2083174  2132-01-20 14:13:00  Nursing/other      Report  20634.0      NaN   \n","2083175  2132-01-21 03:38:00  Nursing/other      Report  17581.0      NaN   \n","2083176  2132-01-21 09:53:00  Nursing/other      Report  19211.0      NaN   \n","2083177  2132-01-21 16:44:00  Nursing/other      Report  20104.0      NaN   \n","2083178  2132-01-21 18:16:00  Nursing/other      Report  16023.0      NaN   \n","2083179  2132-01-21 18:31:00  Nursing/other      Report  16023.0      NaN   \n","\n","                                                      TEXT  \n","2083165  Respiratory Care\\nPt cont on prong CPAP. Weane...  \n","2083166  Neonatology NNP note\\nProcedure note: PICC lin...  \n","2083167  NPN 7A-7P\\n\\n\\n#2 After discussion with team a...  \n","2083168  [** 63**] PHysical Exam\\nPE: pink, AFOF, nasal...  \n","2083169  NPN\\n\\n\\n#2F/N O-Infant remains on demand feed...  \n","2083170  SOCIAL WORK\\nContinuing to follow this family ...  \n","2083171  Attending Note\\nDay of life 76 PMA 39 [**12-7*...  \n","2083172  NICU Attending Admission Note\\nPt is a 34 [**6...  \n","2083173  Admission Note\\n\\n\\nBaby boy [**Known lastname...  \n","2083174  0700-[**2054**] NPN\\n\\n\\nRESP: Infant is in RA...  \n","2083175  NPN\\n\\n\\n#1  Infant remains in RA with O2 sats...  \n","2083176  Neonatology\\nDOL #5, CGA 36 weeks.\\n\\nCVR: Con...  \n","2083177  Family Meeting Note\\nFamily meeting held with ...  \n","2083178  NPN 1800\\n\\n\\n#1 Resp: [**Known lastname 2243*...  \n","2083179  NPN 1800\\nNursing Addendum:\\n[**Known lastname...  "]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["noteevents_file[-15:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IccwEtaLhkP4"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-UgXqXDhr91"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsDklxeihsTw"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZ2E7enKhsWu"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKCLaMJOMyK7"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQqurAGoMyNz"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4WGKw3vMyQ4"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O098cIYuMyTM"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6N4fDQk4hsZg"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"name":"CALM_RUN.ipynb","provenance":[],"authorship_tag":"ABX9TyML1R8CPpbDqHndvvyrM5RR"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}